{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SST_Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJlfppr4gPPpWzmJbkFj8m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyajitghana/TSAI-DeepNLP-END2.0/blob/main/07_Seq2Seq/SST_Redo/SST_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw4KShucNlIR"
      },
      "source": [
        "! pip install pytorch-lightning --quiet\n",
        "! pip install nlpaug --quiet\n",
        "! pip install gdown==3.13.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNwTHtsDKavA"
      },
      "source": [
        "# Stanford Sentiment TreeBank Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQIjbRupMoy5"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMz6g0sbMqS_",
        "outputId": "2c360c86-4465-4838-cbf3-344cf11a1d1c"
      },
      "source": [
        "! gdown https://drive.google.com/uc?id=1urNi0Rtp9XkvkxxeKytjl1WoYNYUEoPI"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1urNi0Rtp9XkvkxxeKytjl1WoYNYUEoPI\n",
            "To: /content/sst_dataset.zip\n",
            "5.04MB [00:00, 19.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1RcVU7fM-XY",
        "outputId": "4c41438b-b32a-4b34-8de2-6fb824c9a9a4"
      },
      "source": [
        "! unzip sst_dataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  sst_dataset.zip\n",
            "   creating: sst_dataset/\n",
            "  inflating: sst_dataset/sst_dataset_augmented.csv  \n",
            "  inflating: sst_dataset/sst_dataset_cleaned.csv  \n",
            "  inflating: sst_dataset/sst_dataset_synonym.csv  \n",
            "  inflating: sst_dataset/sst_dataset_translated.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6BgOkuFKg9M"
      },
      "source": [
        "## PyTorch `Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYAwBM0Yub9v"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.experimental.functional import sequential_transforms, ngrams_func, totensor, vocab_func\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "import torchtext.experimental.functional as text_f\n",
        "\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "import nlpaug.flow as nafc\n",
        "\n",
        "from nlpaug.util import Action\n",
        "\n",
        "import random\n",
        "import gdown\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from typing import Optional, Tuple, Any, Dict, List"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "XzRIq254NgwJ",
        "outputId": "4dc53b61-2751-4b85-a510-64f6507e112b"
      },
      "source": [
        "url = 'https://drive.google.com/uc?id=1urNi0Rtp9XkvkxxeKytjl1WoYNYUEoPI'\n",
        "output = 'sst_dataset.zip'\n",
        "\n",
        "gdown.cached_download(url, output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File exists: sst_dataset.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sst_dataset.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1TRxoOAOUwH"
      },
      "source": [
        "with ZipFile('sst_dataset.zip') as datasetzip:\n",
        "    with datasetzip.open('sst_dataset/sst_dataset_augmented.csv') as f:\n",
        "        dataset = pd.read_csv(f, index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "QARAnYWBO24W",
        "outputId": "048049fd-d2fe-46a6-eae7-20084a1ff1c2"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "      <th>phrase</th>\n",
              "      <th>phrase_ids</th>\n",
              "      <th>splitset_label</th>\n",
              "      <th>sentiment_values</th>\n",
              "      <th>phrase_cleaned</th>\n",
              "      <th>synonym_sentences</th>\n",
              "      <th>backtranslated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>226166</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>The Rock is destined to be the 21st Century's ...</td>\n",
              "      <td>The Rock is destine to be the twenty first Cen...</td>\n",
              "      <td>Rock is set to be the 21st century's new `` Co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>226300</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>The gorgeously elaborate continuation of ` ` T...</td>\n",
              "      <td>The gorgeously elaborate continue to `` The Lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>13995</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>Effective but too - lukewarm biopic</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>14123</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>If you sometimes like to go to the motion pict...</td>\n",
              "      <td>If you sometimes want to go to the movies to p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>13999</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>Emerges as something rare, an effect movie tha...</td>\n",
              "      <td>One of the rare 'and therefore does not feel h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index  ...                                     backtranslated\n",
              "0               1  ...  Rock is set to be the 21st century's new `` Co...\n",
              "1               2  ...  The gorgeously elaborate continue to `` The Lo...\n",
              "2               3  ...                     Effective but too-tepid biopic\n",
              "3               4  ...  If you sometimes want to go to the movies to p...\n",
              "4               5  ...  One of the rare 'and therefore does not feel h...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpZGqPF0Tz1j"
      },
      "source": [
        "dataset_test = dataset[dataset['splitset_label'].isin([2])][['phrase_cleaned', 'sentiment_values']].rename(columns={\"phrase_cleaned\": 'phrase'}).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeQ7a5AkQyNU"
      },
      "source": [
        "dataset_train_raw = dataset[dataset['splitset_label'].isin([1, 3])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isUls0b7SMJT"
      },
      "source": [
        "phrase_cleaned = dataset_train_raw[['phrase_cleaned', 'sentiment_values']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVLRuz5zRgoT"
      },
      "source": [
        "dataset_train = pd.concat([\n",
        "           dataset_train_raw[['phrase_cleaned', 'sentiment_values']].rename(columns={\"phrase_cleaned\": 'phrase'}),\n",
        "           dataset_train_raw[['synonym_sentences', 'sentiment_values']].rename(columns={\"synonym_sentences\": 'phrase'}),\n",
        "           dataset_train_raw[['backtranslated', 'sentiment_values']].rename(columns={\"backtranslated\": 'phrase'}),\n",
        "], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "WUslbtS0RQec",
        "outputId": "e7f3b063-bc2e-4f9a-f91f-da27f37dbcf8"
      },
      "source": [
        "dataset_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase</th>\n",
              "      <th>sentiment_values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Rock is destined to be the 21st Century's ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You'd think by now America would have had enou...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yet the act is still charming here .</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              phrase  sentiment_values\n",
              "0  The Rock is destined to be the 21st Century's ...                 3\n",
              "1  The gorgeously elaborate continuation of `` Th...                 4\n",
              "2  Singer\\/composer Bryan Adams contributes a sle...                 3\n",
              "3  You'd think by now America would have had enou...                 2\n",
              "4               Yet the act is still charming here .                 3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHdT6KVsTNrM",
        "outputId": "d5d4869b-eee7-42a8-def9-e7d51dae9a9a"
      },
      "source": [
        "dataset_train.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27483 entries, 0 to 27482\n",
            "Data columns (total 2 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   phrase            27483 non-null  object\n",
            " 1   sentiment_values  27483 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 429.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "YurIQB2YT9FG",
        "outputId": "a35cb8bf-c641-44bb-dde6-9d9a55db143c"
      },
      "source": [
        "dataset_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phrase</th>\n",
              "      <th>sentiment_values</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The film provides some great insight into the ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Offers that rare combination of entertainment ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              phrase  sentiment_values\n",
              "0                     Effective but too-tepid biopic                 2\n",
              "1  If you sometimes like to go to the movies to h...                 3\n",
              "2  Emerges as something rare , an issue movie tha...                 4\n",
              "3  The film provides some great insight into the ...                 2\n",
              "4  Offers that rare combination of entertainment ...                 4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiT7aoy0Ua4w",
        "outputId": "aa4c30d4-92bf-4586-d040-699b7c06b527"
      },
      "source": [
        "dataset_test.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2125 entries, 0 to 2124\n",
            "Data columns (total 2 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   phrase            2125 non-null   object\n",
            " 1   sentiment_values  2125 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 33.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GbTAYWTnUc0J",
        "outputId": "57029b73-757f-4f07-e680-dcfd52fe1dbf"
      },
      "source": [
        "f'Train Data Size: {len(dataset_train)}, Test Data Size: {len(dataset_test)}'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Train Data Size: 27483, Test Data Size: 2125'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9awE-ErDBMHd"
      },
      "source": [
        "class StanfordSentimentTreeBank(Dataset):\n",
        "    \"\"\"The Standford Sentiment Tree Bank Dataset\n",
        "    Stanford Sentiment Treebank V1.0\n",
        "\n",
        "    This is the dataset of the paper:\n",
        "\n",
        "    Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\n",
        "    Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng and Christopher Potts\n",
        "    Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)\n",
        "\n",
        "    If you use this dataset in your research, please cite the above paper.\n",
        "\n",
        "    @incollection{SocherEtAl2013:RNTN,\n",
        "    title = {{Parsing With Compositional Vector Grammars}},\n",
        "    author = {Richard Socher and Alex Perelygin and Jean Wu and Jason Chuang and Christopher Manning and Andrew Ng and Christopher Potts},\n",
        "    booktitle = {{EMNLP}},\n",
        "    year = {2013}\n",
        "    }\n",
        "\n",
        "    This file includes:\n",
        "    1. original_rt_snippets.txt contains 10,605 processed snippets from the original pool of Rotten Tomatoes HTML files. Please note that some snippet may contain multiple sentences.\n",
        "\n",
        "    2. dictionary.txt contains all phrases and their IDs, separated by a vertical line |\n",
        "\n",
        "    3. sentiment_labels.txt contains all phrase ids and the corresponding sentiment labels, separated by a vertical line.\n",
        "    Note that you can recover the 5 classes by mapping the positivity probability using the following cut-offs:\n",
        "    [0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0]\n",
        "    for very negative, negative, neutral, positive, very positive, respectively.\n",
        "    Please note that phrase ids and sentence ids are not the same.\n",
        "\n",
        "    4. SOStr.txt and STree.txt encode the structure of the parse trees. \n",
        "    STree encodes the trees in a parent pointer format. Each line corresponds to each sentence in the datasetSentences.txt file. The Matlab code of this paper will show you how to read this format if you are not familiar with it.\n",
        "\n",
        "    5. datasetSentences.txt contains the sentence index, followed by the sentence string separated by a tab. These are the sentences of the train/dev/test sets.\n",
        "\n",
        "    6. datasetSplit.txt contains the sentence index (corresponding to the index in datasetSentences.txt file) followed by the set label separated by a comma:\n",
        "        1 = train\n",
        "        2 = test\n",
        "        3 = dev\n",
        "\n",
        "    Please note that the datasetSentences.txt file has more sentences/lines than the original_rt_snippet.txt. \n",
        "    Each row in the latter represents a snippet as shown on RT, whereas the former is each sub sentence as determined by the Stanford parser.\n",
        "\n",
        "    For comparing research and training models, please use the provided train/dev/test splits.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ORIG_URL = \"http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\"\n",
        "    DATASET_NAME = \"StanfordSentimentTreeBank\"\n",
        "    URL = 'https://drive.google.com/uc?id=1urNi0Rtp9XkvkxxeKytjl1WoYNYUEoPI'\n",
        "    OUTPUT = 'sst_dataset.zip'\n",
        " \n",
        "\n",
        "    def __init__(self, root, vocab=None, text_transforms=None, label_transforms=None, split='train', ngrams=1, use_transformed_dataset=True):\n",
        "        \"\"\"Initiate text-classification dataset.\n",
        "        Args:\n",
        "            data: a list of label and text tring tuple. label is an integer.\n",
        "                [(label1, text1), (label2, text2), (label2, text3)]\n",
        "            vocab: Vocabulary object used for dataset.\n",
        "            transforms: a tuple of label and text string transforms.\n",
        "        \"\"\"\n",
        "\n",
        "        super(self.__class__, self).__init__()\n",
        "\n",
        "        if split not in ['train', 'test']:\n",
        "            raise ValueError(f'split must be either [\"train\", \"test\"] unknown split {split}')\n",
        "\n",
        "        self.vocab = vocab\n",
        "\n",
        "        gdown.cached_download(self.URL, Path(root) / self.OUTPUT)\n",
        "\n",
        "        self.generate_sst_dataset(split, Path(root) / self.OUTPUT)\n",
        "\n",
        "        tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "        # the text transform can only work at the sentence level\n",
        "        # the rest of tokenization and vocab is done by this class\n",
        "        self.text_transform = sequential_transforms(tokenizer, text_f.ngrams_func(ngrams))\n",
        "\n",
        "        def build_vocab(data, transforms):\n",
        "            def apply_transforms(data):\n",
        "                for line in data:\n",
        "                    yield transforms(line)\n",
        "            return build_vocab_from_iterator(apply_transforms(data), len(data))\n",
        "\n",
        "        if self.vocab is None:\n",
        "            # vocab is always built on the train dataset\n",
        "            self.vocab = build_vocab(self.dataset_train[\"phrase\"], self.text_transform)\n",
        "\n",
        "\n",
        "        if text_transforms is not None:\n",
        "            self.text_transform = sequential_transforms(\n",
        "                self.text_transform, text_transforms, text_f.vocab_func(self.vocab), text_f.totensor(dtype=torch.long)\n",
        "            )\n",
        "        else:\n",
        "            self.text_transform = sequential_transforms(\n",
        "                self.text_transform, text_f.vocab_func(self.vocab), text_f.totensor(dtype=torch.long)\n",
        "            )\n",
        "\n",
        "        self.label_transform = sequential_transforms(text_f.totensor(dtype=torch.long))\n",
        "\n",
        "    def generate_sst_dataset(self, split, dataset_file):\n",
        "\n",
        "        with ZipFile(dataset_file) as datasetzip:\n",
        "            with datasetzip.open('sst_dataset/sst_dataset_augmented.csv') as f:\n",
        "                dataset = pd.read_csv(f, index_col=0)\n",
        "\n",
        "        self.dataset_orig = dataset.copy()\n",
        "\n",
        "        dataset_train_raw = dataset[dataset['splitset_label'].isin([1, 3])]\n",
        "        self.dataset_train = pd.concat([\n",
        "                dataset_train_raw[['phrase_cleaned', 'sentiment_values']].rename(columns={\"phrase_cleaned\": 'phrase'}),\n",
        "                dataset_train_raw[['synonym_sentences', 'sentiment_values']].rename(columns={\"synonym_sentences\": 'phrase'}),\n",
        "                dataset_train_raw[['backtranslated', 'sentiment_values']].rename(columns={\"backtranslated\": 'phrase'}),\n",
        "        ], ignore_index=True)\n",
        "\n",
        "        if split == 'train':\n",
        "            self.dataset = self.dataset_train.copy()\n",
        "        else:\n",
        "            self.dataset = dataset[dataset['splitset_label'].isin([2])] \\\n",
        "                                    [['phrase_cleaned', 'sentiment_values']] \\\n",
        "                                    .rename(columns={\"phrase_cleaned\": 'phrase'}) \\\n",
        "                                    .reset_index(drop=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def discretize_label(label):\n",
        "        if label <= 0.2: return 0\n",
        "        if label <= 0.4: return 1\n",
        "        if label <= 0.6: return 2\n",
        "        if label <= 0.8: return 3\n",
        "        return 4\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(f'text: {self.dataset[\"sentence\"].iloc[idx]}, label: {self.dataset[\"sentiment_values\"].iloc[idx]}')\n",
        "        text = self.text_transform(self.dataset['phrase'].iloc[idx])\n",
        "        label = self.label_transform(self.dataset['sentiment_values'].iloc[idx])\n",
        "        # print(f't_text: {text} {text.shape}, t_label: {label}')\n",
        "        return label, text \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_labels():\n",
        "        return ['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return self.vocab\n",
        "\n",
        "    @property\n",
        "    def collator_fn(self):\n",
        "        def collate_fn(batch):\n",
        "            pad_idx = self.get_vocab()['<pad>']\n",
        "            \n",
        "            labels, sequences = zip(*batch)\n",
        "\n",
        "            labels = torch.stack(labels)\n",
        "\n",
        "            lengths = torch.LongTensor([len(sequence) for sequence in sequences])\n",
        "\n",
        "            # print('before padding: ', sequences[40])\n",
        "            \n",
        "            sequences = torch.nn.utils.rnn.pad_sequence(sequences, \n",
        "                                                        padding_value = pad_idx,\n",
        "                                                        batch_first=True\n",
        "                                                        )\n",
        "            # print('after padding: ', sequences[40])\n",
        "                    \n",
        "            return labels, sequences, lengths\n",
        "        \n",
        "        return collate_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0rgVuvIFqvy",
        "outputId": "7b9ca0af-0344-45b6-f2aa-924b997e58d7"
      },
      "source": [
        "dataset = StanfordSentimentTreeBank(root='.', split='train')\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    collate_fn=dataset.collator_fn\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/27483 [00:00<?, ?lines/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "File exists: sst_dataset.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 27483/27483 [00:00<00:00, 44853.31lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e2bje7wW82B",
        "outputId": "295e400a-849c-40a8-deea-f213398012eb"
      },
      "source": [
        "len(loader) * 32"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27488"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbLngLdCGrZr"
      },
      "source": [
        "batch = next(iter(loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIrswf-W2zl_"
      },
      "source": [
        "labels, text, lengths = batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15hXFO07yRvq",
        "outputId": "e0d63676-d16f-4d22-f71e-9370c4b160ea"
      },
      "source": [
        "labels.shape, text.shape, lengths.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32]), torch.Size([32, 45]), torch.Size([32]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHXmD3IfXSxu"
      },
      "source": [
        "def random_deletion(words, p=0.1): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0, 1) > p, words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining\n",
        "\n",
        "def random_swap(sentence, n=3, p=0.1): \n",
        "    length = range(len(sentence))\n",
        "    n = min(n, len(sentence))\n",
        "    for _ in range(n):\n",
        "        if random.uniform(0, 1) > p:\n",
        "            idx1, idx2 = random.choices(length, k=2)\n",
        "            sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqdcwSQoKkCf"
      },
      "source": [
        "## PyTorch Lightning `LightningDataModule`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LSze6AtJEEL"
      },
      "source": [
        "class SSTDataModule(pl.LightningDataModule):\n",
        "    \"\"\"\n",
        "    DataModule for SST, train, val, test splits and transforms\n",
        "    \"\"\"\n",
        "\n",
        "    name = \"stanford_sentiment_treebank\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir: str = '.',\n",
        "        val_split: int = 1000,\n",
        "        num_workers: int = 2,\n",
        "        batch_size: int = 64,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir: where to save/load the data\n",
        "            val_split: how many of the training images to use for the validation split\n",
        "            num_workers: how many workers to use for loading data\n",
        "            normalize: If true applies image normalize\n",
        "            batch_size: desired batch size.\n",
        "        \"\"\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.data_dir = data_dir\n",
        "        self.val_split = val_split\n",
        "        self.num_workers = num_workers\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.dataset_train = ...\n",
        "        self.dataset_val = ...\n",
        "        self.dataset_test = ...\n",
        "\n",
        "        self.SST = StanfordSentimentTreeBank\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Saves IMDB files to `data_dir`\"\"\"\n",
        "        self.SST(self.data_dir)\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None):\n",
        "        \"\"\"Split the train and valid dataset\"\"\"\n",
        "\n",
        "        train_trans, test_trans = self.default_transforms\n",
        "\n",
        "        train_dataset = self.SST(self.data_dir, split='train', **train_trans)\n",
        "        test_dataset = self.SST(self.data_dir, split='test', **test_trans)\n",
        "\n",
        "        train_length = len(train_dataset)\n",
        "\n",
        "        self.raw_dataset_train = train_dataset\n",
        "        self.raw_dataset_test = test_dataset\n",
        "\n",
        "        # self.dataset_train, self.dataset_val = random_split(train_dataset, [train_length - self.val_split, self.val_split])\n",
        "        self.dataset_train = train_dataset\n",
        "        self.dataset_test = test_dataset\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"IMDB train set removes a subset to use for validation\"\"\"\n",
        "        loader = DataLoader(\n",
        "            self.dataset_train,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True,\n",
        "            collate_fn=self.collator_fn\n",
        "        )\n",
        "        return loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\"IMDB val set uses a subset of the training set for validation\"\"\"\n",
        "        loader = DataLoader(\n",
        "            self.dataset_test,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True,\n",
        "            collate_fn=self.collator_fn\n",
        "        )\n",
        "        return loader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        \"\"\"IMDB test set uses the test split\"\"\"\n",
        "        loader = DataLoader(\n",
        "            self.dataset_test,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True,\n",
        "            collate_fn=self.collator_fn\n",
        "        )\n",
        "        return loader\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return self.raw_dataset_train.get_vocab()\n",
        "\n",
        "    @property\n",
        "    def default_transforms(self):\n",
        "        train_transforms = {\n",
        "            'text_transforms': text_f.sequential_transforms(\n",
        "                random_deletion,\n",
        "                random_swap\n",
        "            ),\n",
        "            'label_transforms': None\n",
        "        }\n",
        "        test_transforms = {\n",
        "            'text_transforms': None,\n",
        "            'label_transforms': None\n",
        "        }\n",
        "\n",
        "        return train_transforms, test_transforms\n",
        "\n",
        "    @property\n",
        "    def collator_fn(self):\n",
        "        return self.raw_dataset_train.collator_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_oyifcOylx5",
        "outputId": "9d2e8ff1-b16b-41c2-d6b1-ded31618da84"
      },
      "source": [
        "datamodule = SSTDataModule()\n",
        "datamodule.setup()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/27483 [00:00<?, ?lines/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "File exists: sst_dataset.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 27483/27483 [00:00<00:00, 44661.64lines/s]\n",
            "  0%|          | 0/27483 [00:00<?, ?lines/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "File exists: sst_dataset.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 27483/27483 [00:00<00:00, 44941.58lines/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APbKfC-sKy1Q"
      },
      "source": [
        "train_loader = datamodule.train_dataloader()\n",
        "val_loader = datamodule.val_dataloader()\n",
        "test_loader = datamodule.test_dataloader()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZbxstfZ1TD9",
        "outputId": "8e6a6383-57c7-4f6c-811a-fd980d45d3ec"
      },
      "source": [
        "len(train_loader), len(val_loader), len(test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(430, 34, 34)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQWnJSxYLjbE"
      },
      "source": [
        "a, b, c = next(iter(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmpfBuNBYFad"
      },
      "source": [
        "vocab = datamodule.get_vocab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvIr004ZYUue"
      },
      "source": [
        "Example Sample Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bt5xRizYgki",
        "outputId": "7aa7309b-86b6-4e4d-fb6f-2129e764a44f"
      },
      "source": [
        "b[30].numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  826,    74,  2099,     8,    12, 17011,    15,  9373,    18,\n",
              "          53,    58,     2,    21,     3, 22737,   180,     4,   100,\n",
              "        1149,    68,  2035, 11625,  1347,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "           1,     1,     1,     1,     1,     1,     1,     1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "bB1rdlW2LsFE",
        "outputId": "37c53532-7b3b-470d-86bd-d976e510d559"
      },
      "source": [
        "' '.join(vocab.itos[x] for x in b[30].numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"unfortunately he carvey ' s rubber - grimace with up no . for the savorless script , get crafted make harris goldberg match <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoy9wNgsYWsV"
      },
      "source": [
        "Label of above text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFR3xRVDYeLb",
        "outputId": "7734d3d9-fde7-4d7b-d3fd-4893783080c6"
      },
      "source": [
        "a[30].numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4wmIFslzYjn8",
        "outputId": "f1dcaf80-6b0e-43d4-85bd-308fa29e7912"
      },
      "source": [
        "datamodule.dataset_train.get_labels()[a[30].numpy()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "j-QoTnbZ0G8P",
        "outputId": "c82289ed-c932-456b-e602-15f478560e0c"
      },
      "source": [
        "text = datamodule.raw_dataset_train.dataset['phrase'].iloc[0]\n",
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The Rock is destined to be the 21st Century's new `` Conan '' and that he's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuqDHPi3-0mn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}