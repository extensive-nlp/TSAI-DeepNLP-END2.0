<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>11_Attention_Advanced</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="attention-advanced">11 Attention Advanced</h1>
<h2 id="assignment">Assignment</h2>
<ol>
<li>Follow the similar strategy as we did in our  <a href="https://colab.research.google.com/drive/1IlorkvXhZgmd_sayOVx4bC_I5Qpdzxk_?usp=sharing">baby-steps-code (Links to an external site.)</a>, but replace GRU with LSTM. In your code you must:
<ol>
<li>Perform 1 full feed forward step for the encoder <strong>manually</strong></li>
<li>Perform 1 full feed forward step for the decoder <strong>manually</strong>.</li>
<li>You can use any of the 3 attention mechanisms that we discussed.</li>
</ol>
</li>
<li>Explain your steps in the readme file and</li>
<li>Submit the assignment asking for these things:
<ol>
<li>Link to the readme file that must explain Encoder/Decoder Feed-forward manual steps  <strong>and the attention mechanism that you have used</strong> - 500 pts</li>
<li>Copy-paste (don’t redirect to github), the Encoder Feed Forward steps for 2 words - 250 pts</li>
<li>Copy-paste (don’t redirect to github), the Decoder Feed Forward steps for 2 words - 250 pts</li>
</ol>
</li>
</ol>
<h2 id="solution">Solution</h2>

<table>
<thead>
<tr>
<th></th>
<th>NBViewer</th>
<th>Google Colab</th>
<th>Tensorboard Logs</th>
</tr>
</thead>
<tbody>
<tr>
<td>Attention Advanced - <strong>Solution</strong></td>
<td><a href="https://nbviewer.jupyter.org/github/satyajitghana/TSAI-DeepNLP-END2.0/blob/main/11_Attention_Advanced/Attention_Advanced.ipynb"><img alt="Open In NBViewer" src="https://img.shields.io/badge/render-nbviewer-orange?logo=Jupyter"></a></td>
<td><a href="https://githubtocolab.com/satyajitghana/TSAI-DeepNLP-END2.0/blob/main/11_Attention_Advanced/Attention_Advanced.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
<td><a href="https://tensorboard.dev/experiment/kwH5WKoQTOaJLhOc9oYy6Q/"><img src="https://img.shields.io/badge/logs-tensorboard-orange?logo=Tensorflow"></a></td>
</tr>
<tr>
<td>Seq2Seq-Attention (Reference)</td>
<td><a href="https://nbviewer.jupyter.org/github/satyajitghana/TSAI-DeepNLP-END2.0/blob/main/11_Attention_Advanced/seq2seq-translation.ipynb"><img alt="Open In NBViewer" src="https://img.shields.io/badge/render-nbviewer-orange?logo=Jupyter"></a></td>
<td><a href="https://githubtocolab.com/satyajitghana/TSAI-DeepNLP-END2.0/blob/main/11_Attention_Advanced/seq2seq-translation.ipynb"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"></a></td>
<td></td>
</tr>
</tbody>
</table><p>If someday PyTorch decides to remove the <code>data.zip</code> file, I’ve added it to <a href="https://github.com/satyajitghana/TSAI-DeepNLP-END2.0/blob/main/10_Seq2Seq_Attention/data.zip">this repository</a>.</p>
<h3 id="encoder-feed-forward-steps">Encoder Feed Forward Steps</h3>
<pre class=" language-python"><code class="prism  language-python">enc_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>
    hparams<span class="token punctuation">.</span>input_dim<span class="token punctuation">,</span>
    hparams<span class="token punctuation">.</span>hidden_size
<span class="token punctuation">)</span>
enc_embedding
</code></pre>
<pre><code>&gt;&gt; Embedding(4347, 64)
</code></pre>
<pre class=" language-python"><code class="prism  language-python">enc_lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>
    hparams<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
    hparams<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
    num_layers<span class="token operator">=</span><span class="token number">1</span>
<span class="token punctuation">)</span>
enc_lstm
</code></pre>
<pre><code>&gt;&gt; LSTM(64, 64)
</code></pre>
<pre class=" language-python"><code class="prism  language-python">f<span class="token string">'The encoder will take the input sentence {src_text[0]} = {" ".join(input_lang_itos[x] for x in src_text[0])}'</span>
</code></pre>
<pre><code>&gt;&gt; 'The encoder will take the input sentence tensor([ 13,  16, 463,   4,   3]) = tu es impatiente . &lt;eos&gt;'
</code></pre>
<pre class=" language-python"><code class="prism  language-python">enc_encoder_hidden <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> hparams<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> hparams<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>
enc_encoder_hidden<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">,</span> enc_encoder_hidden<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape
</code></pre>
<pre><code>&gt;&gt; (torch.Size([1, 1, 64]), torch.Size([1, 1, 64]))
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token comment"># forward pass</span>
seq_len <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>src_text<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
embedded <span class="token operator">=</span> enc_embedding<span class="token punctuation">(</span>src_text<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
enc_output<span class="token punctuation">,</span> enc_hidden <span class="token operator">=</span> enc_lstm<span class="token punctuation">(</span>embedded<span class="token punctuation">,</span> enc_encoder_hidden<span class="token punctuation">)</span>
</code></pre>
<p>Note <code>enc_output</code> and <code>enc_hidden</code> will later be used by the decoder !</p>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'since our sentence has {len(src_text[0])} words, the number of tensors in enc_output is {enc_output.shape[0]}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>&gt;&gt; since our sentence has 5 words, the number of tensors in enc_output is 5
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'embedding: {embedded.shape}\nenc_output: {enc_output.shape}\nenc_hidden: {enc_hidden[0].shape, enc_hidden[1].shape}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>&gt;&gt; embedding: torch.Size([5, 1, 64])
&gt;&gt; enc_output: torch.Size([5, 1, 64])
&gt;&gt; enc_hidden: (torch.Size([1, 1, 64]), torch.Size([1, 1, 64]))
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'encoding for the word \'{input_lang_itos[src_text[0, 0]]}\' =&gt;\n\n{enc_output[0]}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>encoding for the word 'tu' =&gt;

tensor([[-0.0543,  0.0638, -0.1975, -0.1677, -0.0539, -0.0799, -0.0503,  0.0007,
         -0.1526,  0.0269,  0.1492,  0.2136,  0.0022, -0.0716,  0.0493,  0.0884,
          0.2390, -0.1747,  0.0222,  0.1018,  0.0792, -0.1830,  0.2660, -0.1601,
         -0.0031,  0.2112,  0.1274, -0.2266,  0.1665, -0.0918,  0.1431, -0.1941,
          0.1174, -0.1755,  0.2341, -0.1604, -0.0336, -0.0107, -0.0823,  0.2096,
         -0.1492,  0.0024, -0.2048, -0.2197, -0.0225, -0.0126,  0.1423, -0.0376,
          0.0351, -0.0735,  0.1298,  0.0437,  0.1812, -0.1990, -0.0230, -0.1988,
         -0.0519, -0.0607, -0.0144,  0.0720, -0.2157, -0.0570,  0.0637, -0.0687]],
       grad_fn=&lt;SelectBackward&gt;)
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'encoding for the word \'{input_lang_itos[src_text[0, 1]]}\' =&gt;\n\n{enc_output[1]}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>encoding for the word 'es' =&gt;

tensor([[ 0.0052, -0.1731, -0.2374, -0.0742, -0.1043, -0.0528, -0.0789,  0.0948,
         -0.1553, -0.1581,  0.3061,  0.0233, -0.0263, -0.1190,  0.2288,  0.3443,
          0.3691, -0.0884, -0.1495,  0.0013, -0.0716,  0.0549,  0.2131, -0.0852,
         -0.1066,  0.2260,  0.0553, -0.0925,  0.2297, -0.0972,  0.2397, -0.0222,
          0.0623, -0.3111,  0.2283, -0.1766,  0.0787, -0.0744, -0.0616,  0.0231,
         -0.0838,  0.0849, -0.4161,  0.0202, -0.1292, -0.0138,  0.0784,  0.0334,
         -0.1377, -0.0678,  0.0150,  0.1796, -0.0396,  0.1356,  0.0032, -0.0631,
          0.0989,  0.0350, -0.2848, -0.0165, -0.1672, -0.0179,  0.1378,  0.0915]],
       grad_fn=&lt;SelectBackward&gt;)
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'encoding for the word \'{input_lang_itos[src_text[0, 2]]}\' =&gt;\n\n{enc_output[2]}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>encoding for the word 'impatiente' =&gt;

tensor([[ 0.0732, -0.1036, -0.1111, -0.0589, -0.0526, -0.0549, -0.1882,  0.2312,
          0.0089, -0.1297, -0.0507, -0.1168, -0.0353,  0.0075,  0.0738,  0.1092,
          0.0566,  0.0912, -0.2511, -0.0793, -0.0123,  0.1230,  0.1665, -0.0725,
         -0.0762,  0.1660, -0.1273,  0.0871,  0.2491,  0.0638,  0.2093,  0.0893,
          0.1419, -0.2067,  0.0119, -0.1068,  0.1178,  0.0655, -0.0208, -0.0647,
         -0.1147, -0.0500, -0.0150, -0.0616, -0.2934, -0.1099, -0.2117,  0.1308,
         -0.1164, -0.0382,  0.1587,  0.1247, -0.1988,  0.1069,  0.0867, -0.0014,
          0.0079, -0.0263, -0.1672,  0.0169, -0.0829,  0.0871,  0.0611,  0.0820]],
       grad_fn=&lt;SelectBackward&gt;)
</code></pre>
<p>and so on every word is now encoded</p>
<h3 id="decoder-feed-forward-steps">Decoder Feed Forward Steps</h3>
<p><strong>A complete step by step Decoder Feed Forward</strong></p>
<p>The Decoder parameters</p>
<pre class=" language-python"><code class="prism  language-python">dec_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>
    hparams<span class="token punctuation">.</span>output_dim<span class="token punctuation">,</span>
    hparams<span class="token punctuation">.</span>hidden_size
<span class="token punctuation">)</span>
dec_embedding
</code></pre>
<pre><code>&gt;&gt; Embedding(2805, 64)
</code></pre>
<pre class=" language-python"><code class="prism  language-python">dec_lstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>
    hparams<span class="token punctuation">.</span>hidden_size <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span>
    hparams<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
dec_lstm
</code></pre>
<pre><code>&gt;&gt; LSTM(128, 64)
</code></pre>
<pre class=" language-python"><code class="prism  language-python">dec_out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
    hparams<span class="token punctuation">.</span>hidden_size <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> hparams<span class="token punctuation">.</span>output_dim
<span class="token punctuation">)</span>
dec_out
</code></pre>
<pre><code>&gt;&gt; Linear(in_features=128, out_features=2805, bias=True)
</code></pre>
<p>This is the attention part</p>
<pre class=" language-python"><code class="prism  language-python">luong_attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>
    hparams<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> hparams<span class="token punctuation">.</span>hidden_size
<span class="token punctuation">)</span>
luong_attn
</code></pre>
<pre><code>&gt;&gt; Linear(in_features=64, out_features=64, bias=True)
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">from</span> ttctext<span class="token punctuation">.</span>datamodules<span class="token punctuation">.</span>torch_translate <span class="token keyword">import</span> SOS_token<span class="token punctuation">,</span> EOS_token<span class="token punctuation">,</span> PAD_token
</code></pre>
<pre class=" language-python"><code class="prism  language-python">decoder_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>SOS_token<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># SOS is the first word to the decoder</span>
decoder_context <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> hparams<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>
decoder_hidden <span class="token operator">=</span> enc_encoder_hidden <span class="token comment"># Use last hidden state from encoder to start decoder</span>
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'decoder_input:\t\t{decoder_input.shape}\ndecoder_context:\t{decoder_context.shape}\ndecoder_hidden:\t\t{decoder_hidden[0].shape, decoder_hidden[1].shape}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>&gt;&gt; decoder_input:		torch.Size([1, 1])
&gt;&gt; decoder_context:	torch.Size([1, 64])
&gt;&gt; decoder_hidden:		(torch.Size([1, 1, 64]), torch.Size([1, 1, 64]))
</code></pre>
<pre class=" language-python"><code class="prism  language-python">dec_word_input <span class="token operator">=</span> decoder_input

<span class="token comment"># Get the embedding of the current input word (last output word)</span>
dec_word_embedded <span class="token operator">=</span> dec_embedding<span class="token punctuation">(</span>dec_word_input<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># S=1 x B x N</span>
f<span class="token string">'dec_word_embedded: {dec_word_embedded.shape}'</span>
</code></pre>
<pre><code>&gt;&gt; 'dec_word_embedded: torch.Size([1, 1, 64])'
</code></pre>
<pre class=" language-python"><code class="prism  language-python">last_context <span class="token operator">=</span> decoder_context
last_hidden <span class="token operator">=</span> decoder_hidden

<span class="token comment"># Combine embedded input word and last context, run through RNN</span>
dec_rnn_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>dec_word_embedded<span class="token punctuation">,</span> last_context<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
dec_rnn_output<span class="token punctuation">,</span> dec_rnn_hidden <span class="token operator">=</span> dec_lstm<span class="token punctuation">(</span>dec_rnn_input<span class="token punctuation">,</span> last_hidden<span class="token punctuation">)</span>
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'dec_rnn_input:\t\t{dec_rnn_input.shape}\ndec_rnn_output:\t\t{dec_rnn_output.shape}\ndec_rnn_hidden:\t\t{dec_rnn_hidden[0].shape, dec_rnn_hidden[0].shape}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>&gt;&gt; dec_rnn_input:		torch.Size([1, 1, 128])
&gt;&gt; dec_rnn_output:		torch.Size([1, 1, 64])
&gt;&gt; dec_rnn_hidden:		(torch.Size([1, 1, 64]), torch.Size([1, 1, 64]))
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'dec_rnn_output: {dec_rnn_output.shape} value =&gt;\n\n{dec_rnn_output}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>dec_rnn_output: torch.Size([1, 1, 64]) value =&gt;

tensor([[[ 2.4784e-02,  1.3236e-01,  2.8387e-02,  3.1887e-03,  1.3423e-01,
          -8.7927e-02,  1.5511e-01,  2.8784e-02,  1.0580e-01, -1.4575e-01,
          -4.3383e-03, -5.2812e-02,  1.7754e-01, -3.1593e-02,  6.7075e-02,
           7.7494e-02,  4.9320e-02,  1.7713e-01, -2.0790e-01, -5.0475e-02,
          -5.8649e-02,  4.6692e-02, -3.1964e-02, -1.3329e-01, -9.9950e-02,
          -9.4949e-02,  2.1983e-02,  1.2766e-01,  3.3407e-02,  1.2375e-02,
          -9.7627e-02,  8.2564e-05,  4.0589e-02, -5.0377e-02, -6.5772e-02,
           2.8655e-02, -1.1418e-01,  5.5525e-02,  1.6390e-01,  3.2977e-02,
           1.3898e-02,  4.4744e-02, -5.5388e-02, -5.9081e-02,  2.2567e-02,
          -1.6297e-01,  9.2167e-02, -1.4468e-01,  5.4815e-02, -8.2351e-02,
          -4.3525e-02,  6.9047e-02,  5.4019e-02, -1.1496e-01, -1.9732e-01,
           5.2014e-02,  2.2706e-01,  6.1765e-02,  1.0778e-01,  7.5064e-02,
           1.1164e-01, -4.1908e-02, -7.5117e-02,  1.3715e-02]]],
       grad_fn=&lt;StackBackward&gt;)
</code></pre>
<p>Calculate attention from current RNN state</p>
<pre class=" language-python"><code class="prism  language-python">seq_len <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>enc_output<span class="token punctuation">)</span>

<span class="token comment"># Create variable to store attention energies</span>
attn_energies <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>seq_len<span class="token punctuation">)</span> <span class="token comment"># B x 1 x S</span>

attn_hidden <span class="token operator">=</span> dec_rnn_output<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token comment"># Calculate energies for each encoder output</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>seq_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
    energy <span class="token operator">=</span> luong_attn<span class="token punctuation">(</span>enc_output<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
    energy <span class="token operator">=</span> attn_hidden<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>energy<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'energy for {i}th word: {energy}'</span><span class="token punctuation">)</span>

    attn_energies<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> energy
</code></pre>
<pre><code>&gt;&gt; energy for 0th word: 0.013496480882167816
&gt;&gt; energy for 1th word: -0.019721370190382004
&gt;&gt; energy for 2th word: 0.04026033729314804
&gt;&gt; energy for 3th word: 0.11657365411520004
&gt;&gt; energy for 4th word: 0.05412563309073448
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'attention energies: {attn_energies.shape} values =&gt;\n\n{attn_energies}'</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>attention energies: torch.Size([5]) values =&gt;

tensor([ 0.0135, -0.0197,  0.0403,  0.1166,  0.0541], grad_fn=&lt;CopySlices&gt;)
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token comment"># Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len</span>
dec_attn_weights <span class="token operator">=</span>  F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn_energies<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre>
<pre class=" language-python"><code class="prism  language-python">dec_attn_weights<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<pre><code>&gt;&gt; tensor(1., grad_fn=&lt;SumBackward0&gt;)
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'attention weights: {dec_attn_weights.shape} values =&gt;\n\n{dec_attn_weights}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>attention weights: torch.Size([1, 1, 5]) values =&gt;

&gt;&gt; tensor([[[0.1944, 0.1880, 0.1997, 0.2155, 0.2024]]],
       grad_fn=&lt;UnsqueezeBackward0&gt;)
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token comment"># apply to encoder outputs</span>
dec_context_new <span class="token operator">=</span> dec_attn_weights<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>enc_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># B x 1 x N</span>
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'dec_context_new: {dec_context_new.shape} values =&gt;\n\n{dec_context_new}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>dec_context_new: torch.Size([1, 1, 64]) values =&gt;

tensor([[[ 0.0873, -0.0190, -0.0559, -0.0386, -0.0353,  0.0400, -0.0116,
           0.1641, -0.0525, -0.0474,  0.0651,  0.0297, -0.0100, -0.1076,
           0.0932,  0.1184,  0.1446, -0.0358, -0.2018,  0.0410, -0.0010,
          -0.0116,  0.1718, -0.0905, -0.0948,  0.2211, -0.1088, -0.0856,
           0.0962, -0.0698,  0.0966, -0.0301,  0.1399, -0.1744,  0.0716,
          -0.1258,  0.0235, -0.0409, -0.0293,  0.0687, -0.0779,  0.0336,
          -0.1106,  0.0323, -0.1402, -0.0324, -0.0821,  0.1166, -0.1192,
           0.0371,  0.1582,  0.0321, -0.1342,  0.1206,  0.0797, -0.0553,
          -0.0261,  0.0474, -0.0550,  0.0025, -0.0787, -0.0014, -0.0070,
           0.0421]]], grad_fn=&lt;BmmBackward0&gt;)
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token comment"># Final output layer (next word prediction) using the RNN hidden state and context vector</span>
dec_rnn_output_new <span class="token operator">=</span> dec_rnn_output<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># S=1 x B x N -&gt; B x N</span>
dec_context_new <span class="token operator">=</span> dec_context_new<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>       <span class="token comment"># B x S=1 x N -&gt; B x N</span>
dec_output_final <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>
    dec_out<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>dec_rnn_output_new<span class="token punctuation">,</span> dec_context_new<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>
<span class="token punctuation">)</span>
</code></pre>
<pre class=" language-python"><code class="prism  language-python">torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>
    dec_out<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>dec_rnn_output_new<span class="token punctuation">,</span> dec_context_new<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>
<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<pre><code>&gt;&gt; tensor(-22277.1465, grad_fn=&lt;SumBackward0&gt;)
</code></pre>
<pre class=" language-python"><code class="prism  language-python">dec_output_final<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
<pre><code>&gt;&gt; tensor(-22277.1465, grad_fn=&lt;SumBackward0&gt;)
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'size after concatenating dec_context and dec_rnn_output: {torch.cat((dec_rnn_output_new, dec_context_new), dim=1).shape}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>size after concatenating dec_context and dec_rnn_output: torch.Size([1, 128])
</code></pre>
<p>After applying the final FC layer of decoder output</p>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'dec_rnn_output_new: {dec_rnn_output_new.shape} values =&gt;\n\n{dec_rnn_output_new}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>dec_rnn_output_new: torch.Size([1, 64]) values =&gt;

tensor([[ 2.4784e-02,  1.3236e-01,  2.8387e-02,  3.1887e-03,  1.3423e-01,
         -8.7927e-02,  1.5511e-01,  2.8784e-02,  1.0580e-01, -1.4575e-01,
         -4.3383e-03, -5.2812e-02,  1.7754e-01, -3.1593e-02,  6.7075e-02,
          7.7494e-02,  4.9320e-02,  1.7713e-01, -2.0790e-01, -5.0475e-02,
         -5.8649e-02,  4.6692e-02, -3.1964e-02, -1.3329e-01, -9.9950e-02,
         -9.4949e-02,  2.1983e-02,  1.2766e-01,  3.3407e-02,  1.2375e-02,
         -9.7627e-02,  8.2564e-05,  4.0589e-02, -5.0377e-02, -6.5772e-02,
          2.8655e-02, -1.1418e-01,  5.5525e-02,  1.6390e-01,  3.2977e-02,
          1.3898e-02,  4.4744e-02, -5.5388e-02, -5.9081e-02,  2.2567e-02,
         -1.6297e-01,  9.2167e-02, -1.4468e-01,  5.4815e-02, -8.2351e-02,
         -4.3525e-02,  6.9047e-02,  5.4019e-02, -1.1496e-01, -1.9732e-01,
          5.2014e-02,  2.2706e-01,  6.1765e-02,  1.0778e-01,  7.5064e-02,
          1.1164e-01, -4.1908e-02, -7.5117e-02,  1.3715e-02]],
       grad_fn=&lt;SqueezeBackward1&gt;)
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'dec_context_new: {dec_context_new.shape} values =&gt;\n\n{dec_context_new}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>dec_context_new: torch.Size([1, 64]) values =&gt;

tensor([[ 0.0873, -0.0190, -0.0559, -0.0386, -0.0353,  0.0400, -0.0116,  0.1641,
         -0.0525, -0.0474,  0.0651,  0.0297, -0.0100, -0.1076,  0.0932,  0.1184,
          0.1446, -0.0358, -0.2018,  0.0410, -0.0010, -0.0116,  0.1718, -0.0905,
         -0.0948,  0.2211, -0.1088, -0.0856,  0.0962, -0.0698,  0.0966, -0.0301,
          0.1399, -0.1744,  0.0716, -0.1258,  0.0235, -0.0409, -0.0293,  0.0687,
         -0.0779,  0.0336, -0.1106,  0.0323, -0.1402, -0.0324, -0.0821,  0.1166,
         -0.1192,  0.0371,  0.1582,  0.0321, -0.1342,  0.1206,  0.0797, -0.0553,
         -0.0261,  0.0474, -0.0550,  0.0025, -0.0787, -0.0014, -0.0070,  0.0421]],
       grad_fn=&lt;SqueezeBackward1&gt;)
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'dec_output_final: {dec_output_final.shape} values =&gt;\n\n{dec_output_final}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>dec_output_final: torch.Size([1, 2805]) values =&gt;

&gt;&gt; tensor([[-7.8912, -7.8641, -7.9288,  ..., -7.8772, -7.9572, -8.1186]],
       grad_fn=&lt;LogSoftmaxBackward&gt;)
</code></pre>
<pre class=" language-python"><code class="prism  language-python">dec_topv<span class="token punctuation">,</span> dec_topi <span class="token operator">=</span> dec_output_final<span class="token punctuation">.</span>data<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
ni <span class="token operator">=</span> dec_topi<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
</code></pre>
<pre class=" language-python"><code class="prism  language-python">dec_topv<span class="token punctuation">,</span> dec_topi
</code></pre>
<pre><code>&gt;&gt; (tensor([[-7.6851]]), tensor([[2579]]))
</code></pre>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'predicted word: {target_lang_itos[ni]}'</span>
<span class="token punctuation">)</span>
</code></pre>
<pre><code>&gt;&gt; predicted word: samples
</code></pre>
<p><strong>Now we run it for two inputs</strong></p>
<pre class=" language-python"><code class="prism  language-python">decoder_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>SOS_token<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># SOS is the first word to the decoder</span>
decoder_context <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> hparams<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>
decoder_hidden_test <span class="token operator">=</span> enc_encoder_hidden <span class="token comment"># Use last hidden state from encoder to start decoder</span>
</code></pre>
<pre class=" language-python"><code class="prism  language-python">last_context <span class="token operator">=</span> decoder_context
last_hidden <span class="token operator">=</span> decoder_hidden
</code></pre>
<pre class=" language-python"><code class="prism  language-python">i <span class="token operator">=</span> <span class="token number">0</span>

dec_word_input <span class="token operator">=</span> decoder_input

<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'decoder word input: {dec_word_input} value: {target_lang_itos[dec_word_input[0, 0]]}'</span><span class="token punctuation">)</span>

dec_word_embedded <span class="token operator">=</span> dec_embedding<span class="token punctuation">(</span>dec_word_input<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

dec_rnn_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>dec_word_embedded<span class="token punctuation">,</span> last_context<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
dec_rnn_output<span class="token punctuation">,</span> dec_rnn_hidden_new <span class="token operator">=</span> dec_lstm<span class="token punctuation">(</span>dec_rnn_input<span class="token punctuation">,</span> last_hidden<span class="token punctuation">)</span>

<span class="token comment"># --- attn</span>
seq_len <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>enc_output<span class="token punctuation">)</span>
attn_energies <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>seq_len<span class="token punctuation">)</span>
attn_hidden <span class="token operator">=</span> dec_rnn_output<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>seq_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
    energy <span class="token operator">=</span> luong_attn<span class="token punctuation">(</span>enc_output<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
    energy <span class="token operator">=</span> attn_hidden<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>energy<span class="token punctuation">)</span>
    attn_energies<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> energy
<span class="token comment"># --- attn</span>

dec_attn_weights <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn_energies<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'attentions: {dec_attn_weights[0, 0].detach().numpy()}'</span><span class="token punctuation">)</span>

dec_context_new <span class="token operator">=</span> dec_attn_weights<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>enc_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

dec_rnn_output_new <span class="token operator">=</span> dec_rnn_output<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># S=1 x B x N -&gt; B x N</span>
dec_context_new <span class="token operator">=</span> dec_context_new<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>       <span class="token comment"># B x S=1 x N -&gt; B x N</span>
dec_output_final <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>
    dec_out<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>dec_rnn_output_new<span class="token punctuation">,</span> dec_context_new<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>
<span class="token punctuation">)</span>

dec_topv<span class="token punctuation">,</span> dec_topi <span class="token operator">=</span> dec_output_final<span class="token punctuation">.</span>data<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
ni <span class="token operator">=</span> dec_topi<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'predicted word: {target_lang_itos[ni]}'</span>
<span class="token punctuation">)</span>

decoder_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

last_context <span class="token operator">=</span> dec_context_new
last_hidden <span class="token operator">=</span> dec_rnn_hidden_new

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>
</code></pre>
<pre><code>&gt;&gt; decoder word input: tensor([[2]]) value: &lt;sos&gt;
&gt;&gt; attentions: [0.1943825  0.18803161 0.19965519 0.21548797 0.20244274]
&gt;&gt; predicted word: samples
</code></pre>
<pre class=" language-python"><code class="prism  language-python">last_context <span class="token operator">=</span> decoder_context
last_hidden <span class="token operator">=</span> decoder_hidden
</code></pre>
<pre class=" language-python"><code class="prism  language-python">i <span class="token operator">=</span> <span class="token number">1</span>

dec_word_input <span class="token operator">=</span> decoder_input

<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'decoder word input: {dec_word_input} value: {target_lang_itos[dec_word_input[0, 0]]}'</span><span class="token punctuation">)</span>

dec_word_embedded <span class="token operator">=</span> dec_embedding<span class="token punctuation">(</span>dec_word_input<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

dec_rnn_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>dec_word_embedded<span class="token punctuation">,</span> last_context<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
dec_rnn_output<span class="token punctuation">,</span> dec_rnn_hidden_new <span class="token operator">=</span> dec_lstm<span class="token punctuation">(</span>dec_rnn_input<span class="token punctuation">,</span> last_hidden<span class="token punctuation">)</span>

<span class="token comment"># --- attn</span>
seq_len <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>enc_output<span class="token punctuation">)</span>
attn_energies <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>seq_len<span class="token punctuation">)</span>
attn_hidden <span class="token operator">=</span> dec_rnn_output<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>seq_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
    energy <span class="token operator">=</span> luong_attn<span class="token punctuation">(</span>enc_output<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
    energy <span class="token operator">=</span> attn_hidden<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>energy<span class="token punctuation">)</span>
    attn_energies<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> energy
<span class="token comment"># --- attn</span>

dec_attn_weights <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn_energies<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'attentions: {dec_attn_weights[0, 0].detach().numpy()}'</span><span class="token punctuation">)</span>

dec_context_new <span class="token operator">=</span> dec_attn_weights<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>enc_output<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

dec_rnn_output_new <span class="token operator">=</span> dec_rnn_output<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># S=1 x B x N -&gt; B x N</span>
dec_context_new <span class="token operator">=</span> dec_context_new<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>       <span class="token comment"># B x S=1 x N -&gt; B x N</span>
dec_output_final <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>
    dec_out<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>dec_rnn_output_new<span class="token punctuation">,</span> dec_context_new<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span>
<span class="token punctuation">)</span>

dec_topv<span class="token punctuation">,</span> dec_topi <span class="token operator">=</span> dec_output_final<span class="token punctuation">.</span>data<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
ni <span class="token operator">=</span> dec_topi<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>
    f<span class="token string">'predicted word: {target_lang_itos[ni]}'</span>
<span class="token punctuation">)</span>

decoder_input <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>ni<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

last_context <span class="token operator">=</span> dec_context_new
last_hidden <span class="token operator">=</span> dec_rnn_hidden_new

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>
</code></pre>
<pre><code>&gt;&gt; decoder word input: tensor([[2579]]) value: samples
&gt;&gt; attentions: [0.21555178 0.19511788 0.20333609 0.19306885 0.19292536]
&gt;&gt; predicted word: shot
</code></pre>
<p>For even more verbosity please look into the notebook file for this assignment. (links are to the top)</p>
<h2 id="evaluation">Evaluation</h2>
<pre class=" language-text"><code class="prism  language-text">[KEY: &gt; input, = target, &lt; output]

&gt; elle est de mauvaise humeur .
= she is in a bad mood .
&lt; she is in a mood . &lt;EOS&gt;

&gt; je suis dure a cuire .
= i m tough .
&lt; i m tough . &lt;EOS&gt;

&gt; j etudie l economie a l universite .
= i m studying economics at university .
&lt; i m studying the college . &lt;EOS&gt;

&gt; je n en suis pas trop convaincu .
= i m not too convinced .
&lt; i m not too too . &lt;EOS&gt;

&gt; je suis ravie de t aider .
= i am glad to help you .
&lt; i m glad to help you . &lt;EOS&gt;

&gt; elle a tres peur des chiens .
= she s very afraid of dogs .
&lt; she is very afraid of dogs . &lt;EOS&gt;

&gt; il est fier d etre musicien .
= he is proud of being a musician .
&lt; he s proud of a of . &lt;EOS&gt;

&gt; c est le portrait crache de son pere .
= he is the image of his father .
&lt; he is the for father father . &lt;EOS&gt;

&gt; je suis juste paresseuse .
= i m just lazy .
&lt; i m just . . &lt;EOS&gt;

&gt; nous sommes en train de nous en charger .
= we re handling it .
&lt; we re back . &lt;EOS&gt;
</code></pre>
</div>
</body>

</html>
