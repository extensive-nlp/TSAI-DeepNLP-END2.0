{"27745": {"question": "Conda install gives 1.0.0 with CUDA 9.0", "answer": "@LLNLanLeN upgrade your NVIDIA driver to latest, and that will likely fix the issue. cudatoolkit 10.1 requires much newer driver than 10.0"}, "27749": {"question": "[Feature Request]: Batchwise torch.lstsq", "answer": "If you are only looking to compute the `solution` output from [`torch.lstsq`](https://pytorch.org/docs/stable/generated/torch.lstsq.html), but batch-wise, you could make use of [`torch.pinverse`](https://pytorch.org/docs/stable/generated/torch.pinverse.html) which is implemented to support batch-wise computation.\r\n\r\n> More about the relationship between the Moore-Penrose pseudo-inverse and ordinary least-squares can be found here: https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Linear_least-squares\r\n\r\nSo that, if the take the example from the [`torch.lstsq`](https://pytorch.org/docs/stable/generated/torch.lstsq.html) documentation:\r\n\r\n# No batch support\r\n\r\n```python\r\nimport torch\r\n\r\nA = torch.tensor([[1., 1, 1],\r\n                  [2, 3, 4],\r\n                  [3, 5, 2],\r\n                  [4, 2, 5],\r\n                  [5, 4, 3]])\r\n\r\nB = torch.tensor([[-10., -3],\r\n                  [ 12, 14],\r\n                  [ 14, 12],\r\n                  [ 16, 16],\r\n                  [ 18, 16]])\r\n\r\nX, _ = torch.lstsq(B, A)\r\n\r\nX[:A.shape[1]]  # According to the doc: \"The first n rows of X contains the solution.\"\r\n\r\n# tensor([[2.0000, 1.0000],\r\n#         [1.0000, 1.0000],\r\n#         [1.0000, 2.0000]])\r\n```\r\n\r\n# With batch support\r\n\r\n```python\r\ndef batch_lstsq(\r\n    input: torch.Tensor,  # matrix B of shape (batch * m * k) \r\n    A: torch.Tensor  # matrix A of shape (batch * m * n) \r\n):\r\n   \r\n    X = torch.bmm(\r\n        torch.pinverse(A),\r\n        input\r\n    )\r\n \r\n    return X\r\n```\r\n\r\n## Tests\r\n\r\n- with the same tensors as the ones defined above, but repeated\r\n\r\n```python\r\nn_batch = 3\r\n\r\nA_batch = torch.cat(n_batch * [A.unsqueeze(0)])\r\nB_batch = torch.cat(n_batch * [B.unsqueeze(0)])\r\n\r\nsolution = batch_lstsq(B_batch, A_batch)\r\n\r\nfor i in range(len(solution)):    \r\n    \r\n    X, _ = torch.lstsq(B_batch[i], A_batch[i])\r\n    X = X[:A_batch[i].shape[1]]\r\n    \r\n    assert torch.allclose(solution[i], X[:A.shape[1]])\r\n```\r\n\r\n- with random tensors, works for both (m >= n) and (m < n)\r\n\r\n```python\r\nn_batch = 3\r\nm = 10  # 2\r\nn = 3\r\nk = 2\r\n\r\nA_batch = torch.rand(n_batch, m, n)\r\nB_batch = torch.rand(n_batch, m, k)\r\n\r\nsolution = batch_lstsq(B_batch, A_batch)\r\n\r\nfor i in range(len(solution)):    \r\n    \r\n    X, _ = torch.lstsq(B_batch[i], A_batch[i])\r\n    X = X[:A_batch[i].shape[1]]\r\n    \r\n    assert torch.allclose(solution[i], X[:A.shape[1]])\r\n```\r\n\r\n## Wall clock performance test\r\n\r\n- Setup\r\n```\r\nn_batch = 1000\r\n\r\nA_batch = torch.cat(n_batch * [A.unsqueeze(0)])\r\nB_batch = torch.cat(n_batch * [B.unsqueeze(0)])\r\n\r\nm = A_batch.shape[2]\r\n```\r\n\r\n- with a for loop as suggested by @Electric-Turtle \r\n\r\n```\r\n%%timeit\r\nX = torch.stack([torch.lstsq(B_batch[i], A_batch[i])[0][:m] for i in range(n_batch)],dim=0)\r\n\r\n# on my machine\r\n# 16.7 ms \u00b1 320 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\n- when using batch-wise `torch.pinverse`\r\n```\r\n%%timeit\r\nX = batch_lstsq(B_batch, A_batch)\r\n\r\n# on my machine\r\n# 3.02 ms \u00b1 60.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\n\r\n\r\n\r\n\r\n"}, "27726": {"question": "[Android] Unknown builtin op: aten::mul", "answer": "> Oh man, I went through the bother of reproducing the error by creating a simple app, then added `torch::autograd::AutoGradMode guard(false);` and that fixed everything (also in my real app)!\r\n\r\nGlad to know it helps. Did it fix the \"_adaptive_avg_pool2d_backward\" error or not?\r\n\r\nIn case you don't know, we also created a sample android app that you can play with: https://github.com/pytorch/android-demo-app\r\n\r\nMore information can be found at: https://pytorch.org/mobile/home/"}, "27729": {"question": "quantizationed model cannot inference with cuda?", "answer": "That is correct, we don't support cuda currently."}, "27738": {"question": "Can not use .cuda() function to load the model into GPU using Pytorch 1.3", "answer": "@sayakpaul as the error message says, upgrade your CUDA driver, you installed the CUDA 10.1 compatible pytorch package which is the default."}, "27739": {"question": "Try torch.nn.quantized.functional.conv2d failed", "answer": "Finally, it works and thanks for your help!\r\n```\r\nqF.conv2d(q_inputs, q_filters, bias)\r\n```\r\nWhat's more, the data type of convolutional kernals seems to be quint8, and data type of intput tensor is qint8."}, "27743": {"question": "Support dictionary outputs in TorchScript tracer", "answer": "@mohammedayub44 this is a feature after 1.5, you have to install master or nightly builds to get this feature, or wait for 1.6 :)"}, "12160": {"question": "[feature request] Batched (n-1)-D to n-D matrix_diag", "answer": "@chrelli check my comment in https://github.com/pytorch/pytorch/issues/5198#issuecomment-425069863\r\nHere is a proposed implementation\r\n```python\r\ndef matrix_diag(diagonal):\r\n    N = diagonal.shape[-1]\r\n    shape = diagonal.shape[:-1] + (N, N)\r\n    device, dtype = diagonal.device, diagonal.dtype\r\n    result = torch.zeros(shape, dtype=dtype, device=device)\r\n    indices = torch.arange(result.numel(), device=device).reshape(shape)\r\n    indices = indices.diagonal(dim1=-2, dim2=-1)\r\n    result.view(-1)[indices] = diagonal\r\n    return result\r\n```"}, "12161": {"question": "[caffe2] VS2017 compiler error for ATen", "answer": "I hit the same problem and updating to VS 2017 15.9.0 also fixed it for me. I'm not trying to build with CUDA, but have CUDA 10.0 installed."}, "12166": {"question": "GPU hangs after killing the program using DistributedDataparallel Model", "answer": "hi, i  meet the same issue, my solution is :\r\ntry\r\n`ps aux|grep python` \r\nand then \r\n`kill -9 PID`\r\nbe sure that kill process **in order**"}, "12169": {"question": "Error building a custom PyTorch CUDA extension with 1.0 on macOS High Sierra", "answer": "We deprecated and removed FFI two days ago. Please try to build our new C++ extension API: https://pytorch.org/tutorials/advanced/cpp_extension.html"}, "12174": {"question": "get_cudnn_version in collect_env is flaky", "answer": "That's because LD_LIBRARY_PATH is not the only path used for lookup."}, "12190": {"question": "Slowdown in distributions log_prob methods", "answer": "Yes some of my benchmarks showed that the main slowdown in `log_prob`was in `pow` but that slowdown doesn't scale with input shapes, which might be related to some tensor overhead we added after 0.4. \r\nSorry didn't have time to get back to this last week, will do this week. "}, "12198": {"question": "[Feature Request]Synchronized batch norm", "answer": "If anyone happens to be struggling to get the `DistributedDataParallel` to use `SyncBatchnorm`, take a look at this small step-by-step a colleague of mine wrote: [github/dougsouza/pytorch-sync-batchnorm-example](https://github.com/dougsouza/pytorch-sync-batchnorm-example). Hope it helps. :)"}, "12207": {"question": "Non Deterministic Behaviour even after cudnn.deterministic = True and cudnn.benchmark=False ", "answer": "I don't think there is much to do, and FGSA is intentionally discontinuous (due to the sign). What you could do is determine the digit where it is unstable empirically and round that away before taking the sign.\r\n\r\nThat said, the reason is likely that the backward uses the cuda function `atomicAdd`, which is non-deterministic. For a factor of two, it would seem that\r\n```python\r\nclass MyUpsample2(nn.Module):\r\n    def forward(self, x):\r\n        return x[:, :, :, None, :, None].expand(-1, -1, -1, 2, -1, 2).reshape(x.size(0), x.size(1), x.size(2)*2, x.size(3)*2)\r\n```\r\nmakes this deterministic.\r\n\r\nI'll probably add a note to https://github.com/pytorch/pytorch/blob/master/docs/source/notes/randomness.rst about atomic add (Users include: index_add, scatter_add, bincount, lossctc in forward, embedding/embedding_bag, all sorts of pooling, padding, sampling in backward, possibly also sparse to dense or coalescing, but I didn't check).\r\n"}, "12210": {"question": "PackagesNotFoundError: unable to install torchvision in anaconda prompt on windows 10", "answer": "Use `pip install torchvision` instead. It is not yet available in Anaconda Cloud."}, "12218": {"question": "Weight decay modifies grad for SGD but not for Adam", "answer": "no adam modifies it in place see:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py#L90\r\n"}, "12230": {"question": "Bug in masked_fill_ for non contiguous tensors", "answer": "> `expanded_tensor * scalar` now copies it into a non contiguous tensor on linux\r\n\r\nGood to know! We're relying on it triggering a `.contiguous()` in a number of places in Pyro."}, "12256": {"question": "[Conv3D][cudnn] CUDNN_STATUS_NOT_INITIALIZED error when using batch size > 1 with conv layer", "answer": "@zou3519  Thank you for the reply. Unfortunately due to a non disclosure agreement, I am not allowed to upload code or parts of the code. But I found the solution. I was using a custom collate function for dataloader and one of the variables was initialized wrong. I was copying a tensor (index wise) to another, based on the indices in the wrongly initialized variable right before the conv3d layer. So it was more like an \"out of range\" error or something and I was confused by the CUDNN_STATUS_NOT_INITIALIZED error statement. "}, "12285": {"question": "Documentation not clear on torch.expand() alternatives when performing torch.autograd.gradcheck", "answer": "The note only talks about input though. It should have nothing to do with how your function is implemented. All you need to make sure is that the input used in gradcheck don't have overlapping storage."}, "21834": {"question": "Build failed when linking bin/test_parallel with multiple openmp functions defined", "answer": "I managed to complete the build+install process with a quick and dirty workaround:\r\n\r\n> find . -type f -name 'build.make' -exec sed -i 's:^.*\\: /usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \\;\r\n> find . -type f -name 'link.txt' -exec sed -i 's:/usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \\;\r\n\r\nI think I've run these commands mainly in `build/caffe2/`, `build/test_api/` and `build/test_jit/` one can simply run them in `build` and everything should build fine.\r\n"}, "21859": {"question": "CTCLoss CUDA backward throws setStorage: storage size mismatch error", "answer": "The canonical way is to build torchvision from source, it's really easy (much more so than PyTorch itself).\r\n"}, "21875": {"question": "In-place operation on differentiable view leaks memory", "answer": "This is a ref-cycle in cpp that goes as `x -> CopySlices -> AccumulateGrad -> x`.\r\nThis only happens because `x` here is a leaf Tensor.\r\nNote that after doing such thing, trying to run backward will fail with \"leaf variable has been moved into the graph interior\" so we should fail earlier before the cycle is created.\r\nThis can be done by making [this check](https://github.com/pytorch/pytorch/blob/6249d7302b7277864ed0ade93f58d88ee0cd3aa8/torch/csrc/autograd/VariableTypeUtils.h#L44) detect views of leafs in addition to only leafs."}, "21882": {"question": "Converting from IValue to double gives INTERNAL ASSERT failure", "answer": "The `.to...` methods don't do any conversion, you'll have to cast the IValue to a tensor `.toTensor()` then get the value out. The `toDouble()` method only works if the `IValue` contained is a C++ `double` type"}, "21893": {"question": "cmake error build failed", "answer": "Two options available: \r\n1. unset `CMAKE_GENERATOR_TOOLSET_VERSION` \r\n```cmd\r\nset CMAKE_GENERATOR_TOOLSET_VERSION=\r\n```\r\n2. activate the env.\r\n```cmd\r\nset DISTUTILS_USE_SDK=1\r\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,16^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\r\n```\r\nIt is clearly written in the doc through the previous link. If it is still broken, then please check your VS installation."}, "21900": {"question": "Multiprocessing and tensor problem", "answer": "To be more precise it still happens if I use the no grad function, hence no backprop involved. So, the problem is returning a pytorch tensor, I think. If I return the pytorch tensor as a list using the related tolist() method everything is fine. I don't know if this is expected or not."}, "21926": {"question": "Bug in saving indexed torch tensors makes it much slower than numpy.", "answer": "this is not a bug, it's actually by design.\r\n\r\nwhen you index individual elements into a Tensor, then you are holding a view into the original Tensor.\r\n\r\n```\r\na = torch.randn(10, 20)\r\nb = a[0]\r\nb.add_(10) # changes `a`\r\n```\r\n\r\nIf you actually do `torch.save([a, b], 'foo.pt')` then PyTorch makes sure that this \"view\" property is actually preserved across serialization / deserialization.\r\nSo, \r\n\r\n```\r\na = torch.randn(10, 20)\r\nb = a[0]\r\nb.add_(10) # changes `a`\r\n\r\ntorch.save((a, b), 'foo.pt')\r\na, b = torch.save('foo.pt')\r\n\r\nb.add_(10) # still changes `a`\r\n```\r\n\r\nSo, what you are seeing is that even when you are indexing, the whole original Storage is saved.\r\n\r\nYou can make pytorch only save the particular element with `torch.save(a[0].clone()`"}, "21935": {"question": "Unexpected output size for Maxpool", "answer": "\r\nThe implementation should allow for the output size to be 0 if the kernel is larger than input.\r\n\r\nFor example, if ** i=2, p=0, d=1, s=2, k=3**, the kernel is 1 pixel larger than the input.\r\n\r\nThe formula says output size =  **(2 - 3 )/ 2 + 1** =  **-1/2 + 1**\r\n\r\nHowever, in  the C++ implementation above, because of truncation (and not flooring) we get output size = 1. In Python we get 0.\r\n\r\nIf k=4, then the C++ behavior becomes correct again, and we get output size = 0. It seems like a corner case in the C++ implementation. The  floor function seems correct to use, versus rounding towards zero.\r\n"}, "21965": {"question": "base_lrs in torch.optim.lr_scheduler.CyclicLR gets overriden by parent class if parameter groups have 'initial_lr' set", "answer": "~~Schedulers are not currently chainable #13022, and so switching from one to another overrides the past. This should be considered for #24352.~~"}, "21971": {"question": "Implement covariance_matrix for Independent distributions", "answer": "@fehiepsi showed me this trick for batch-compatible `torch.diag()` (I think that's what you're asking for, @justindomke )\r\n```py\r\ndef batch_diag(batched_variance):\r\n    batch_shape = batched_variance.shape[:-1]\r\n    event_size = batched_variance.size(-1)\r\n    cov = batched_variance.new_zeros(batch_shape + (event_size * event_size,))\r\n    cov[..., ::1 + event_size] = batched_variance\r\n    return cov.reshape(batch_shape + (event_size, event_size))\r\n```"}, "21976": {"question": "About CMAKE_PREFIX_PATH", "answer": "OK I fixed that.\r\n\r\n`CMAKE_PREFIX_PATH` should be a folder containing `TorchConfig.cmake` which is `torch/share/cmake/Torch` \r\n\r\nmahmood@m2000:build$ cmake -DCMAKE_PREFIX_PATH=/home/mahmood/pytorch/torch/share/cmake/Torch/ ..\r\n-- The C compiler identification is GNU 7.4.0\r\n-- The CXX compiler identification is GNU 7.4.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Looking for pthread_create\r\n-- Looking for pthread_create - not found\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE\r\n-- Found CUDA: /usr/local/cuda-10.0 (found version \"10.0\")\r\n-- Caffe2: CUDA detected: 10.0\r\n-- Caffe2: CUDA nvcc is: /usr/local/cuda-10.0/bin/nvcc\r\n-- Caffe2: CUDA toolkit directory: /usr/local/cuda-10.0\r\n-- Caffe2: Header version is: 10.0\r\n-- Found CUDNN: /usr/local/cuda-10.0/include\r\n-- Found cuDNN: v7.4.2  (include: /usr/local/cuda-10.0/include, library: /usr/local/cuda-10.0/lib64/libcudnn.so)\r\n-- Autodetected CUDA architecture(s):  5.2\r\n-- Added CUDA NVCC flags for: -gencode;arch=compute_52,code=sm_52\r\n-- Found torch: /home/mahmood/pytorch/torch/lib/libtorch.so\r\n-- Downloading MNIST dataset\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/mahmood/pytorch/examples/cpp/dcgan/build\r\n```"}, "21981": {"question": "ReduceLROnPlateau parent class is not _LRScheduler", "answer": "`ReduceLROnPlateau` overrides every `_LRScheduler` function. We never had need to have `_LRScheduler` as parent. \r\n\r\nPlus, as already mentioned, usage of any internal API names is a terrible idea. \r\n\r\nI will close the issue as non-bug. However feel free to send us PR which will introduce good method of checking if object is Scheduler"}, "15770": {"question": "\"Trying to resize storage that is not resizable\" when calling pin_memory() on some zero-dimensional tensors", "answer": "@qbx2 This bug seems to be fixed on master, could you try installing PyTorch via `pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html` and then run the code again?"}, "15773": {"question": "Need GPU implementation of dirichlet_grad (originally: Reparameterized gradient on GPU for beta / Dirichlet)", "answer": "I find it surprising that there is no implementation of the Dirichlet gradient because there is already an implementation of the Gamma gradient. The standard way to generate a Beta variable is to generate two Gamma variables (see for instance [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Generating_beta-distributed_random_variates)) and compute their fractional ratio. For instance, the following fix on the code of @vmoens seems to work:\r\n\r\n    a,b = torch.ones(3,4,1,5,requires_grad=True,device='cuda'),torch.ones(3,4,1,5,requires_grad=True,device='cuda')\r\n    s1 = torch.distributions.Gamma(a,1).rsample(torch.Size((10,)))\r\n    s2 = torch.distributions.Gamma(b,1).rsample(torch.Size((10,)))\r\n    s = s1 / (s1+s2)\r\n    torch.sum(s).backward()\r\n    a.grad\r\n\r\nSame goes for sampling Dirichlet variables, one actually samples independent Gamma variables. So, right now users should be able to use this fix, but it seems to me that it should not be difficult to implement native support for Beta gradients with CUDA because all the functions are already there.\r\n\r\n@ezyang and @vishwakftw I posted a similar comment on related issue https://github.com/pytorch/pytorch/issues/11030."}, "15774": {"question": "Seems there's memory issue in pytorch 1.0.0", "answer": "I don't know what exactly would be the issue, but I experienced something somewhat similar, which I actually managed to pinpoint. The reason for my memory increase was using DataLoader with custom Dataset (subclassing from torch.utils.data.Dataset) where I was storing loading data in a dict instead of a simple list to accommodate shuffling. However, the indices going to method `__getitem__` (generated by DataLoader) were not simple integers as I anticipated (and as the doc would suggest, https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) but 0-D dimensional integer tensors. And since these don't have the `__hash__` depending on a value, rather on memory location, the dict would grow and grow and grow, because it didn't know there was something in it already. Hope I made it somewhat clear. To illustrate it more:\r\n\r\n    class LiDARDataset(torch.utils.data.Dataset):\r\n        def __init__(self, folder, keep_ram=False):\r\n            self.names = sorted(glob.glob(os.path.join(folder, '*.npy')))\r\n            self.keep_ram = keep_ram\r\n            if self.keep_ram:\r\n                self.loaded = dict()\r\n            else:\r\n                self.loaded = weakref.WeakValueDictionary()\r\n\r\n        def __len__(self):\r\n            return len(self.names)\r\n\r\n        def __getitem__(self, key):\r\n            if not isinstance(key, int): # without these two lines, it would grow\r\n                key = key.item() \r\n            result = self.loaded.get(key) # because the result would always be None\r\n            if result is None:\r\n                result = self.transform(np.load(self.names[key]), key)\r\n                self.loaded[key] = result\r\n            return result\r\n\r\nSo, check you don't have any dictionary, which is indexed by torch.tensor, because that is what is changed since 0.4.1. Or you don't rely on hashing of torch.tensor by its value rather than the memory location"}, "15777": {"question": "torch::save causes serialization error on mnist example", "answer": "You need to wrap the torch::nn::Module object (i.e., model) with torch::nn::ModuleHolder"}, "15778": {"question": "Implementation of ISTFT", "answer": "@SsnL @vincentqb Seems that ISTFT has been recently contributed to torchaudio: https://pytorch.org/audio/functional.html#istft\r\n\r\nIt would be great if it was relocated to the core"}, "15786": {"question": "Dropout on integer tensor types dies with SIGFPE", "answer": "@mruberry Think this issue should be closed since the signal is no longer raised.\r\n\r\n~~Instead we get this errors:~~\r\n\r\n~~```RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.```~~\r\n\r\n~~in both cases (the one with Dropout and the second one from @colesbury).~~ \r\n\r\nRan the code with the nightly build.\r\n\r\nFor the first example this error is raised:\r\n`RuntimeError: result type Float can't be cast to the desired output type Long`\r\n\r\nand for the second one (the one from @colesbury) there is no error, instead we get: \r\n`tensor([inf, nan, nan, nan, inf])`\r\n\r\nI think now it's more clear to the user what's the problem."}, "15797": {"question": "THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=405 error=11 : invalid argument", "answer": "I have successfully solved this problem by updating the cuda8.0 to cuda10.0. "}, "15799": {"question": "Memory leak during backprop() in PyTorch 1.0.0", "answer": "tl;dr After investigation I determined that the reinforcement learning tutorial doesn't leak memory on pytorch 1.2. However, we are committed to making sure there are no more memory leaks so please create new issues with individual new examples of memory leaks and we will investigate with high priority.\r\n\r\nI tried to repro this on my mac. I don't have a linux box with a monitor (which is a requirement to run the tutorial) but I suspect that the result should be the same.\r\n\r\nMy conclusion was that our reinforcement learning tutorial does NOT leak memory. However, it does grow in memory consumption. The reason why it grows in memory consumption is due to the model storing replay memory: https://github.com/pytorch/tutorials/blob/94cb6a3635b9b5ccb8a59c5346addba6901cd15d/intermediate_source/reinforcement_q_learning.py#L336\r\n\r\nOnce the replay memory limit is hit, the RAM consumption stays constant. This can be verified by setting the replay number to something low (i.e. 10) and watching RAM.\r\n\r\nI know that this wasn't completely satisfactory because I'm sure that some of you have encountered memory leaks while training in PyTorch, but we are committed to fixing them so please submit additional bug reports about memory leaks for your individual examples and we can investigate them separately. \r\n\r\n"}, "15803": {"question": "Debugging feature for \"modified by an inplace operation\" errors", "answer": "> which points directly to `b.exp_()`, and indeed, if you change that line to be `b.exp()`, it all works fine.\r\n\r\nTo clarify for other readers, the anomaly detection will *not* necessarily point you at the inplace operation that caused the failure. Instead, it will point you at the operation that could not compute its gradient in the backward pass. The inplace operation to blame may occur anywhere after that, modifying one of the tensors that participated in the line found by the anomaly detection.\r\n\r\nExample:\r\n```\r\nx = torch.rand(10, 20, requires_grad=True)\r\ny = torch.rand(10)\r\nz = (x / y[:, np.newaxis])  # anomaly detection will point here\r\nc = y.abs_()  # but the problem is here\r\nz.sum().backward()\r\n```\r\nThe last line will cause a `RuntimeError`. With anomaly detection enabled, it will point at the line performing the division, but the inplace operation came later."}, "15805": {"question": "Keyboard interrupt and saving the last state of a model", "answer": "This is all you need to implement this. I don't see why would this have to be part of the library.\r\n```python\r\ntry:\r\n    # training code here\r\nexcept KeyboardInterrupt:\r\n    # save model here\r\n```"}, "15808": {"question": "DataLoader freezes randomly when num_workers > 0 (Multiple threads train models on different GPUs in separate threads)", "answer": "I have solved the problem by using processes instead of threads. So in my main python file where I generate all possible hyperparameter combinations and then train each combination on a different GPU, I do not create a new thread and train a model in each thread. Instead, I fork new processes (one for each GPU) and create a `multiprocessing.Queue` that is passed to each subprocess. This queue contains hyperparameters to be trained. Each process is working on the same queue and training models on a dedicated GPU. \r\n\r\nIn my tests, this works even with num_workers=32. There was no freezing for 3 hours now. Usually it freezed much earlier (after a few minutes), so I hope that it is really fixed.\r\n\r\nSince it is working with `Process` and not with `Thread`, I assume that there is a bug in PyTorch's `DataLoader` class that causes the freezes when multiple `DataLoader`s are used in different threads. This problem does not occur with processes, because they have their private memory."}, "15815": {"question": "[JIT] Support Meta programming on If self.training to conditionally NOT compile training only code", "answer": "Hi @sidazhang, thanks for reporting this. Actually `self.training` is already supported in JIT, training and inference subgraph should be both exists in the generated IR, and will depend on the actual value of self.training to execute the corresponding if branch. So we don't need to meta program it. \r\n\r\nThe problem or workaround on your example is that, you should also annotate the function `training_code_only` with `@torch.jit.script_method`, because although we can run it without making it a script_method, we cannot serialize a python function call if you want to save the graph. \r\n\r\nEdited: if `training_code_only` is a TorchScript incompatible logic and cannot be rewrite to make it compatible, can you describe what is actually not compatible so that we could fix it instead? A simple for loop like the above can be simply rewrite to make it compatible. We don't want to add additional meta programming conditions as those logics are kinda hacks to get around some special syntax, also in many cases we could not know the actual value of `self.training` until runtime, so meta programming it might not be a generic solution. "}, "15817": {"question": "[QUESTION] Should _dummy_type use name instead of storage_name?", "answer": "Are you sure the 50% failure rate has anything to do with your change? That's a Caffe2 test and it shouldn't be exercising `torch/cuda/__init__.py` at all"}, "15827": {"question": "Non-monotonic execution times for increasing kernel sizes", "answer": "Maybe cudnn is probably selecting different algorithms.\r\nCould you try setting `torch.backends.cudnn.benchmark = True` in the beginning of the script and compare the values again?"}, "15848": {"question": "How to load a model trained on GPU 0 (cuda: 0) to GPU 1 (cuda:1) for inference?", "answer": "In all likelihood your `device` value is nonsense. Print it out and see what the problem is."}, "15857": {"question": "Feature request : Profiler ", "answer": "MXnet\u2019s profiler is almost certainly based on (or at least inspired by) the PyTorch profiler https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile"}, "15858": {"question": "LibTorch Windows binaries appear to not be built with CuDNN", "answer": "Okay, I know the reason. The PATHs of cuDNN is not passed through arguments like in setup.py when building libtorch."}, "15867": {"question": "RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'", "answer": "Note that the error: `RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'` originated from this line. https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/CheckGenerator.h#L15. Before my PR, the generated dispatch code utilized this function: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L327, and put it on all random distribution function dispatch code. For CUDA side,`THGenerator*` is set to null: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L1247. Hence, with the THGenerator* being null, and the `check_generator` function resolving to `check_generator<CUDAGenerator>(null, &globalContext().defaultGenerator(at::CUDA))`, we were getting that error. In my PR, `check_generator` (now called `check_generator_with_default`) is removed from `function_wrapper.py` and is currently only being called for the CPU functions and hence \"solves\" the bug. \r\n\r\nThe torch.manual_seed returning a CPU Generator is still there and I agree with @ezyang that it's weird. IMO, torch.manual_seed should return nothing, like how it's in torch.cuda.manual_seed. That is, [Py_RETURN_NONE](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/cuda/Module.cpp#L165) vs [(PyObject*)self](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/Generator.cpp#L104)"}, "5544": {"question": ".size() vs .shape, which one should be used?", "answer": ".size() method returns total elements in a dataframe , for eg shape of a tensor might be (10,3) , here total elements in tensor would be returned by .size() = 10X3 = 30 elements!!"}, "5551": {"question": "[feature request] ATen Documentation and Tutorial", "answer": "See https://pytorch.org/cppdocs/\r\nSee https://pytorch.org/cppdocs/notes/tensor_creation.html"}, "5552": {"question": "RuntimeError: dimension specified as 0 but tensor has no dimensions", "answer": "Did you try `z = z.unsqueeze(0)`? \r\n\r\nThe reference is [here](https://github.com/hunkim/PyTorchZeroToAll/issues/24).\r\n"}, "5553": {"question": "NCCL Error 1 when using torch.nn.DataParallel", "answer": "I can help -  @aleksod \r\nLooking at their official build guide: https://github.com/NVIDIA/nccl\r\n\r\nyou need to clone a different repo for the tests:\r\n`git clone https://github.com/NVIDIA/nccl-tests.git`\r\nThen, in order to build the tests, enter that repo and run:\r\nCUDA_HOME=[path to your cuda main install dir] NCCL_HOME=[path to your nccl build dir] make\r\n\r\nfor example\r\nCUDA_HOME=/usr/local/cuda-10.0 NCCL_HOME=/path/to/nccl/build make\r\n\r\nthen, an example test from their official doc is:\r\n`./build/all_reduce_perf -b 8 -e 256M -f 2 -g <ngpus>`\r\n\r\nif you get missing .so libraries errors, you can use LD_LIBRARY_PATH to run the test. for example:\r\n\r\n`LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/some/path/to/nccl/nccl/build/lib:$LD_LIBRARY_PATH ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 2`\r\n\r\n\r\n\r\n\r\n"}, "5554": {"question": "dimension out of range (expected to be in range of [-1, 0], but got 1)", "answer": "ok, i figured it out. So the key point which i didn't understand from the documentation was that the TARGET should be just one entry saying which class it belongs to Eg:[2] instead of a one hot vector like [0,0,1]. I mean, frankly, I  imagined the input and target being of similar shape (which is more intuitive). i.e Now that the input is a vector of [0,0,1] i imagined the TARGET also should be in same shape. Anyway, am glad it worked out. I would love to have a better worded documentation though, imho. \r\n\r\n Below is the code/shapes without dimension error.\r\n\r\n```\r\nlen(pred_y):\r\ntorch.Size([1, 3])\r\ntensor([[ 0.0000,  0.0000,  0.1527]])\r\nlen(x):\r\ntorch.Size([1])\r\ntensor([ 2])\r\nloss_training = loss_fn(pred_y, x)\r\n\r\n\r\n```"}, "5559": {"question": "Incorrect error message for advanced indexing cuda tensor", "answer": "This has been fixed in #5583. Now one can index tensors with both CPU and CUDA tensors so this particular error message doesn't apply anymore :)"}, "5560": {"question": "\"Reduce Failed to Synchronise\" in F.binary_cross_entropy ", "answer": "> See also: #2209\r\n> \r\n> BCELoss accepts only inputs that have all elements in range [0; 1] but this condition doesn't hold in your case\r\n\r\nI got [the same error](https://discuss.pytorch.org/t/cuda-out-of-memory-when-optimizer-step/55942?u=shirui-japina)\r\n\r\nand I tried to use `nn.BCELoss()` like:\r\n\r\n```\r\noptimizer = optim.SGD(model.parameters(), lr=0.0001)\r\ncriterion = nn.BCELoss()\r\n```\r\n\r\n_loop epoch train part:_\r\n\r\n```\r\nprediction = model(batch_input)\r\nloss = criterion(torch.sigmoid(prediction), label)\r\n\r\noptimizer.zero_grad()\r\nloss.backward()\r\noptimizer.step()\r\n```\r\n\r\nThen I solved the problem. Thank you for your comment!\r\n(But I don't know why optim.Adam() can't work well. It still errors: CUDA out of memory.)\r\n"}, "5561": {"question": "Device-side Assert in `THCReduceAll.cuh:339`", "answer": "You have out-of-bound indices when you create the one-hot vector. The error will be raised on the responsible line if you use `CUDA_LAUNCH_BLOCKING=1`"}, "5563": {"question": "Import Error : no module  named torch", "answer": "Add the path by: export PATH=~/anaconda3/bin:$PATH  \r\nbefore opening the python."}, "5569": {"question": "torch.autograd.Function memory leak", "answer": "Yes. The leaks happen only if you hold on to outputs via a mechanism different than `save_for_backward`"}, "5571": {"question": "Missing torch.* docs", "answer": "#5443 addresses all but `torch.default_generator`. :)"}, "5572": {"question": "Add Scale Factor To SGD", "answer": "1. because it's so much more common than rescaling gradients by a constant that doesn't depend on the iteration\r\n2. it needs to be performed on weights and not gradients, and this requires an extra `no_grad` block\r\n3. ideally it wouldn't be part of the optimizer, but we're stuck with it for now because of backward compatibility"}, "5577": {"question": "[bug] Expected GLOO_USE_IBVERBS to be defined", "answer": "Looks like the reproduced failure was due to a staled gloo library.  After starting everything from fresh, everything works fine. So as I said, @alexholdenmiller, could you try building from scratch by git clone --recursive the latest PyTorch master. \r\n\r\nOr if you don't want to clone a new pytorch, at least, please make sure torch/lib/gloo folder's git status is clean.  Also in Pytorch folder, please do git submodule update to get gloo lib updated.\r\n\r\nIf this works, please close this issue."}, "5587": {"question": "[bug]  assert len(modules) == len(inputs) when use torch.distributed to compute last batch", "answer": "As a quick fix you can pass `drop_last=True` when creating the data loader, but it's a bug that we'll need to fix"}, "5602": {"question": "[bug?] Problem with load_state_dict after installing latest pytorch by source", "answer": "The `running_*` are disabled by default for InstanceNorm* layers after #4922 . You can add `track_running_stats=True` to the InstanceNorm* layer constructors."}, "5606": {"question": " when pytorch should add Windows support.", "answer": "Starting from the next release. Should be out within weeks"}, "5609": {"question": "In IPython torch.dot always returns 0", "answer": "please do:\r\n\r\n```\r\npip uninstall -y numpy\r\nconda install -y numpy \r\n```\r\n\r\nThe pip version of numpy is linked against OpenBLAS, which has a `dot` that conflicts in ABI with MKL's `dot`"}, "5613": {"question": "Feature request: named dimensions", "answer": "See `xarray` for prior work https://github.com/pydata/xarray"}, "5616": {"question": "[feature request] torch.argmax / torch.argmin", "answer": "@thecortex note that you can currently obtain the argmax as the second return value of `torch.max` when a dimension is specified. Same for argmin"}, "5625": {"question": "Pytorch 0.4.0 documentation typo for batchnorm's momentum parameter", "answer": "closed via https://github.com/pytorch/pytorch/pull/5450/"}, "5627": {"question": "Can not restart the training to obtain the same results", "answer": "Many things can matter, e.g. random number generator, cudnn non-derministic algorithms, dataloader worker task scheduling, optimizer state, etc."}, "5648": {"question": "[feature request] Add C++ test framework for pure C++ autograd+jit tests", "answer": "The Catch2 devs recommend to just vendor Catch2 in repositories\r\nhttps://github.com/catchorg/Catch2/blob/master/docs/build-systems.md#cmake\r\n\r\nThe alternative, also mentioned on that page, is to use CMake's ability to git clone dependencies\u00a0using the ExternalProject machinery, but I honestly wouldn't go there."}, "5650": {"question": "Handle None gradients in nn.utils.clip_grad_norm", "answer": "What this says is `p.grad` is None. It's possible that `p` (whatever it is) wasn't used in the gradient computation, or there was no backwards pass applied."}, "4623": {"question": "Variables are behaving strangely for indexing", "answer": "One problem is the slice parsing code didn't properly handle the error return. That caused the weird error messages.\r\n\r\nThe other problem is that `Variable` doesn't implement `__index__` so you can't use Variables as slice indices."}, "4628": {"question": "Softmax using multiple threads inhibiting parallel execution of forward passes (?)", "answer": "Using `torch.set_num_threads(1)` solves the problem.\r\n\r\nI figure the problem was a deadlock arising as a result of creating a huge number of threads:\r\n\r\nI create a number of threads where each thread should execute a series of forward passes. However, when the thread encounters the softmax, it spawns a number of child threads. As every thread did this, a huge number of threads was being spawned in total, which caused a deadlock. \r\n\r\nIt is funny though that this occurs only on Linux and not Mac OS."}, "4629": {"question": "0.3.0 availability on conda", "answer": "conda install pytorch=0.3.0 torchvision -c pytorch\r\n\r\nI test this command is right to install pytorch3.0.0 in MacOS!"}, "4631": {"question": "Only nn.Parameters defined directly within nn.Module are listed in module.parameters()", "answer": "There are good reason why we don't want to traverse arbitrary Python data structures, with performance being the most important consideration. That's why we have `nn.ParameterList` and `nn.ModuleList`. `nn.Module()` can also be used as a dictionary-like object for both parameters and submodules."}, "4634": {"question": "Issues about symeig and svd on GPU", "answer": "Do you mean svd? We use magma bindings to many linalg functions. Magma doesn't implement everything in GPU so it sometimes still makes some lapack calls."}, "4650": {"question": "ModuleNotFoundError: No module named 'torch.version'", "answer": "It looks like you are. Change the directory and you should be good."}, "4652": {"question": " the derivative for 'svd' is not implemented", "answer": "the derivative for torch.svd is implemented in the master branch. It hasn't yet been incorporated into a release yet. If you would like to use this feature immediately, you can install pytorch from source: https://github.com/pytorch/pytorch#from-source"}, "4661": {"question": "backward(create_graph=True) should raise a warning for potential memory leak", "answer": "My 2 cents. I've done higher-order differentiation many times. I've always used `autograd.grad` instead of `.backward` or `autograd.backward` because \r\n1. `autograd.grad` doesn't accumulate gradient with previous values, which can lead to obscure errors.\r\n2. it is almost always clearer in code than accessing `.grad` of leaves.\r\n3. it avoids ref cycle.\r\n\r\nIn fact, most DL uses of higher-order gradients I've seen use `autograd.grad`. So could we add a warning (either in doc or in code) to `backward(create_graph)` and advocate `autograd.grad` instead?"}, "4664": {"question": "Bug in inplace operation after expansion", "answer": "Yes, and that's the expected behavior. Don't do in-place on expanded tensors."}, "4669": {"question": "[Feature request] Gradient of cholesky_inverse, cholesky_solve", "answer": "I use the following workarounds for lack of batching in `cholesky_inverse` and lack of gradients in `cholesky_solve`. I'm guessing it should be easy to move this logic into PyTorch.\r\n```py\r\ndef cholesky_solve(b, u):\r\n    \"Like :func:`torch.cholesky_solve` but supports gradients.\"\r\n    if not b.requires_grad and not u.requires_grad:\r\n        return b.cholesky_solve(u)\r\n    x = b.triangular_solve(u, upper=False).solution\r\n    return x.triangular_solve(u, upper=False, transpose=True).solution\r\n\r\ndef cholesky_inverse(u):\r\n    \"Like :func:`torch.cholesky_inverse` but supports batching and gradients.\"\r\n    if u.dim() == 2 and not u.requires_grad:\r\n        return u.cholesky_inverse()\r\n    return cholesky_solve(torch.eye(u.size(-1)).expand(u.size()), u)\r\n```"}, "4697": {"question": "No module named torchvision ", "answer": "If you followed the instructions here: http://pytorch.org/ and install with conda it will install both torch and torchvision together."}, "4698": {"question": "Failed to load model", "answer": "@apaszke I solved it by using the recommended way instead, it's my fault : )\r\n```python\r\ntorch.save(the_model.state_dict(), PATH)\r\n\r\nthe_model = TheModelClass(*args, **kwargs)\r\nthe_model.load_state_dict(torch.load(PATH))\r\n```"}, "19742": {"question": "Serialization of tensors with pickle.dumps seems to be inconsistent, leading to inconsistent redis cache hit/miss", "answer": "Can you provide a minimum example that reproduces this behavior and that indicate that this is a PyTorch problem?\r\n\r\nFrom your description, it might imply that `pickle.dumps(tensor)` is not always the same. But I couldn't reproduce this locally.\r\n\r\nIt could also be that the algorithm that Redis uses to cache large objects is not perfect and is subject to missing it a few times. Hard to say without a repro."}, "19778": {"question": "Grad is None after using view", "answer": "So this isn't a bug per se, but it is definitely a source of confusion. The issue with the above code is that the gradient information is attached to the initial tensor before the `view`, but not the viewed tensor. Performing the initialization and view operation before assigning the tensor to the variable results in losing the access to the gradient information. Splitting out the view works fine. It would be useful to call this out in the docs (maybe I missed this).\r\n```\r\nX0 = torch.tensor([0.25, 0.75], requires_grad=True,)\r\nX_view = X0.view(2, 1, 1)\r\nprint(f\"X_view.shape: {X_view.shape}\")\r\nX_view.sum().backward()\r\nprint(f\"X_view.grad: {X_view.grad}\")\r\nprint(f\"X_view.grad is None: {X_view.grad is None}\")\r\nprint(f\"X0.grad: {X0.grad}\")\r\n```\r\nOutput:\r\n```\r\nX_view.shape: torch.Size([2, 1, 1])\r\nX_view.grad: None\r\nX_view.grad is None: True\r\nX0.grad: tensor([1., 1.])\r\n```\r\n"}, "19786": {"question": "Could Pytorch C++ API load tensor from gpu memory directly?", "answer": "Example code that converts a cv::cuda::GpuMat of type 32F to a torch::Tensor:\r\n\r\n```C++\r\n    void deleter(void *arg){};\r\n    torch::Tensor matToTensor(const cv::cuda::GpuMat &image)\r\n    {\r\n        std::vector<int64_t> dims = {image.rows, image.cols, image.channels()};\r\n        long long step = image.step / sizeof(float);\r\n        std::vector<int64_t> strides = {step, image.channels(), 1};\r\n        return torch::from_blob(image.data, dims, strides, deleter, torch::kCUDA)\r\n    }\r\n```\r\n\r\nIf you want to send in bytes, you would update step size and add torch::kByte/kChar to options.\r\n\r\nOnly tricky part for your GL image may be step size otherwise the code should look almost identical."}, "19798": {"question": "[Caffe2] Compile error: onnxTensorDescriptorV1 has no member named quantizationParams", "answer": "Should be fixed by https://github.com/pytorch/pytorch/pull/19793"}, "19805": {"question": "AverageUnpooling layer for PyTorch (Proposal)", "answer": "Isn't `AverageUnpooling` very similar to `F.interpolate`?"}, "19808": {"question": "Why nn.Sequential can't handle multiple input?", "answer": "@soumith Hi, I changed `nn.Sequential` to this\r\n```python\r\nclass mySequential(nn.Sequential):\r\n    def forward(self, *input):\r\n        for module in self._modules.values():\r\n            input = module(*input)\r\n        return input\r\n```\r\nAnd it could handle multiple inputs/outputs only need the number of outputs from the previous layer equals the number of inputs from the next layer.\r\n```python\r\nclass n_to_n(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\r\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\r\n\r\n    def forward(self, x1, x2):\r\n        y1 = self.conv1(x1)\r\n        y2 = self.conv2(x2)\r\n        return y1, y2\r\n\r\n\r\nclass n_to_one(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\r\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\r\n\r\n    def forward(self, x1, x2):\r\n        y1 = self.conv1(x1)\r\n        y2 = self.conv2(x2)\r\n        return y1 + y2\r\n\r\n\r\nclass one_to_n(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\r\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\r\n\r\n    def forward(self, x):\r\n        y1 = self.conv1(x)\r\n        y2 = self.conv2(x)\r\n        return y1, y2\r\n\r\nseq = mySequential(one_to_n(), n_to_n(), n_to_one()).cuda()\r\ntd = torch.rand(1, 3, 32, 32).cuda()\r\n\r\nout = seq(td)\r\nprint(out.size())\r\n```\r\n```shell\r\ntorch.Size([1, 3, 32, 32])\r\n```\r\nWhat do you think?"}, "19838": {"question": "CTCLoss reduction=\u2019none\u2018 and calculate average in hand is not equal to 'mean'", "answer": "@carry-xz this is expected, see the implementation of `CTCLoss` in https://github.com/pytorch/pytorch/blob/39b885cbbfc8c115069d49f5a6d27ea622bd05dc/aten/src/ATen/native/LossCTC.cpp#L366-L371\r\n\r\nFrom [the documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.CTCLoss)\r\n> \u2018mean\u2019: the output losses will be divided by the target lengths and then the mean over the batch is taken"}, "19840": {"question": "Using DistributedDataParallel through NCCL throws RuntimeError", "answer": "@Ze-Yang Pytorch DDP has a serious flaw which is not documented: the computational graphs for all the nodes must be the same or you will get this error or training sometimes just hangs at 100% CPU (even when training on GPU) on backward() call. You can debug your problem by visualising the computational graphs on each node and see if they are the same. If not, make them (or fix the pytorch source).\r\n\r\nAn example: if your loss function contains an IF condition with additional computation, and one node goes into IF and other does not, this will be enough to crash DDP training. Instead of IF try to solve the same thing some other way and/or trick pytorch to make the graph look the same on each node. Of course, there might be other causes for different autograd graph too.\r\n\r\nMy case was this. Buggy code (crashed):\r\n```\r\nif positive_class.shape[0] > 0:\r\n   loss += torch.somefunction(positive_class)\r\n```\r\nFix:\r\n```\r\n# concat with tensor that has no effect on loss calculation, but positive_class always has content\r\npositive_class = torch.cat(positive_class, dummy)\r\nloss += torch.somefunction(positive_class)\r\n```\r\n\r\nI don't know if your problem is similar, I do not see any obvious divergence of graphs here, but it's worth to look at graphs if you are stuck :)"}, "19902": {"question": "At training time, pytorch batchnorm use biased batch var to normalize input, but running var is updated by unbiased batch var", "answer": "Finally,I find the problem.\r\nAt training time, pytorch batchnorm use biased batch var to normalize input, but running var is updated by unbiased batch var.So after model convergence and switch to eval model,the running var gives unbiased prediction, but this is inconsistency with train mode with biased prediction.\r\nChange the torch.var unbiased option to True or False for seeing the result.\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.nn import Parameter, init\r\n\r\nclass MyBatchNorm(nn.Module):\r\n    _version = 2\r\n    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',\r\n                     'running_mean', 'running_var', 'num_batches_tracked']\r\n\r\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\r\n                 track_running_stats=True):\r\n        super(MyBatchNorm, self).__init__()\r\n        self.num_features = num_features\r\n        self.eps = eps\r\n        self.momentum = momentum\r\n        self.affine = affine\r\n        self.track_running_stats = track_running_stats\r\n        if self.affine:\r\n            self.weight = Parameter(torch.Tensor(num_features))\r\n            self.bias = Parameter(torch.Tensor(num_features))\r\n        else:\r\n            self.register_parameter('weight', None)\r\n            self.register_parameter('bias', None)\r\n        if self.track_running_stats:\r\n            self.register_buffer('running_mean', torch.zeros(num_features))\r\n            self.register_buffer('running_var', torch.ones(num_features))\r\n            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\r\n        else:\r\n            self.register_parameter('running_mean', None)\r\n            self.register_parameter('running_var', None)\r\n            self.register_parameter('num_batches_tracked', None)\r\n        self.reset_parameters()\r\n\r\n    def reset_running_stats(self):\r\n        if self.track_running_stats:\r\n            self.running_mean.zero_()\r\n            self.running_var.fill_(1)\r\n            self.num_batches_tracked.zero_()\r\n\r\n    def reset_parameters(self):\r\n        self.reset_running_stats()\r\n        if self.affine:\r\n            init.uniform_(self.weight)\r\n            init.zeros_(self.bias)\r\n        \r\n    def forward(self, input):        \r\n        input_size = input.size()\r\n        input = input.transpose(1,0)\r\n        input = input.view(input.size(0), -1)\r\n\r\n        if self.training:\r\n            mean = input.mean(dim=1)\r\n            var = torch.var(input,dim=1, unbiased=True)\r\n            self.running_mean[:] = (1. - self.momentum) * self.running_mean + self.momentum * mean\r\n            self.running_var[:] = (1. - self.momentum) * self.running_var + self.momentum * var\r\n        else:\r\n            mean = self.running_mean\r\n            var = self.running_var\r\n\r\n        input = input - mean.view(-1,1)\r\n        input = input / (torch.sqrt(var+self.eps).view(-1,1))\r\n       \r\n        input = self.weight.view(-1, 1) * input + self.bias.view(-1, 1)\r\n        input = input.transpose(1,0)\r\n        input = input.view(*input_size)\r\n        return input\r\n\r\n    def extra_repr(self):\r\n        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\r\n               'track_running_stats={track_running_stats}'.format(**self.__dict__)\r\n\r\n    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\r\n                              missing_keys, unexpected_keys, error_msgs):\r\n        version = local_metadata.get('version', None)\r\n\r\n        if (version is None or version < 2) and self.track_running_stats:\r\n            # at version 2: added num_batches_tracked buffer\r\n            #               this should have a default value of 0\r\n            num_batches_tracked_key = prefix + 'num_batches_tracked'\r\n            if num_batches_tracked_key not in state_dict:\r\n                state_dict[num_batches_tracked_key] = torch.tensor(0, dtype=torch.long)\r\n\r\n        super(MyBatchNorm, self)._load_from_state_dict(\r\n            state_dict, prefix, local_metadata, strict,\r\n            missing_keys, unexpected_keys, error_msgs)\r\n\r\ndef test_batch_norm():\r\n    momentum = 1.0\r\n    torch.manual_seed(1234)\r\n\r\n    batch_norm = MyBatchNorm(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)\r\n    n1 = batch_norm\r\n    torch.save(n1.state_dict(),'n1.pth')\r\n    \r\n    torch_batch_norm = nn.BatchNorm1d(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)\r\n    n2 = torch_batch_norm\r\n    n2.load_state_dict(torch.load('n1.pth'))\r\n    \r\n    x = torch.FloatTensor([[1,2,3], [3,4,0], [3,3,1]])\r\n    y = torch.FloatTensor([[2], [3], [1]])\r\n    criterion = nn.MSELoss()\r\n\r\n    x = x.cuda()\r\n    y = y.cuda()\r\n    batch_norm.cuda()\r\n    torch_batch_norm.cuda()\r\n\r\n    print('Switch to eval mode.')\r\n    batch_norm.eval()\r\n    torch_batch_norm.eval()\r\n    out1 = n1(x)\r\n    out2 = n2(x)\r\n    eval1 = (torch.abs(out2-out1).sum().item() < 1e-4)\r\n\r\n    print('Swtich to train mode.')\r\n    batch_norm.train()\r\n    torch_batch_norm.train()\r\n\r\n    out1 = n1(x)\r\n    out2 = n2(x)\r\n    train2 = (torch.abs(out2-out1).sum().item() < 1e-4)\r\n\r\n    print('Switch to eval mode.')\r\n    n1.eval()\r\n    n2.eval()\r\n    print('MyBatchNorm:')\r\n    print('running_mean:',batch_norm.running_mean.cpu().numpy())\r\n    print('running_var:',batch_norm.running_var.cpu().numpy())\r\n    print('weight:',batch_norm.weight.data.cpu().numpy())\r\n    print('bias:',batch_norm.bias.data.cpu().numpy())\r\n    print()\r\n    \r\n    print('TorchBatchNorm:')\r\n    print('running_mean:',torch_batch_norm.running_mean.cpu().numpy())\r\n    print('running_var:',torch_batch_norm.running_var.cpu().numpy())\r\n    print('weight:',torch_batch_norm.weight.data.cpu().numpy())\r\n    print('bias:',torch_batch_norm.bias.data.cpu().numpy())\r\n    out1 = n1(x)\r\n    out2 = n2(x)\r\n    eval3 = (torch.abs(out2-out1).sum().item() < 1e-4)\r\n    print('eval1,train2,eval3:',eval1,train2,eval3)\r\n    assert eval1 and train2 and eval3\r\n\r\nif __name__ == '__main__':\r\n    test_batch_norm()\r\n```"}, "19913": {"question": "Is CudnnRNN thread-safe?", "answer": "can you reproduce this on 1.1?  A number of thread-safe fixes have gone in since 0.4.1.  I'm going to close for now, please reopen if you can reproduce this on 1.1."}, "19925": {"question": "ProcessGroupNCCL.cpp:260, unhandled cuda error, when using 2 nodes with 4 GPUs each", "answer": "@SerailHydra Hi, I finally solved it with setting NCCL_SOCKET_IFNAME=ib0 and NCCL_IB_DISABLE=1, where ib0 is my ip interface.\r\n\r\nHope it will help!"}, "19946": {"question": "DataLoader for Large Corpus File", "answer": "This is definitely a valid feature request. In fact, I implemented something called `IterableDataset` that will be used as an iterable (e.g., generator, data stream) in PyTorch. It is currently being reviewed at #19228 ."}, "19948": {"question": "[jit] Slice assignment is completely elided in Onnx graph", "answer": "This is an expected behavior, and appropriate warning have sent out. So close the issue."}, "19961": {"question": "torch.nn.LogSoftmax.__repr__() does not include dim argument", "answer": "@SdgJlbl  reminder that you can override `extra_repr` instead of  `__repr__`."}, "21270": {"question": "cuda-runtime error(4) on PyTorch 0.4.1", "answer": "> Is there some solution can solve it without upgrade?\r\n\r\nI was able to solve this problem on 0.4.1 by reinstalling NVIDIA drivers. "}, "21271": {"question": "[ONNX] The shape of PReLU weight is wrong", "answer": "> @ezyang Thanks! Could you please give directions on how to fix it? The weight [here](https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py#L396) has the type `prim::Param`. I haven't found a way to modify it.\r\n\r\nYou can try inserting ```onnx::unsqueeze``` for the weight\r\n```\r\nweight = g.op(\"Unsqueeze\", axes_i=[1, 2])\r\nreturn g.op(\"PRelu\", self, weight)\r\n```\r\nIf the weight is ```prim::Param``` or Constant, this unsqueeze will be optimized away if ```do_constant_folding``` is turned on while exporting. \r\n\r\nEdit: you'll also need to construct the ```axes_i``` parameter for ```Unsqueeze``` based on actual rank of ```self``` tensor, like here\r\nhttps://github.com/pytorch/pytorch/blob/0f58d20fe43e89138ffcbbf64fb48569539f2e4e/torch/onnx/symbolic_opset9.py#L316"}, "21344": {"question": "Make DDP failure recoverable", "answer": "## Trying Solution 2\r\n\r\n#21534 seems addressed the problem but in quite a dirty way. A better solution might need to satisfy the following requirements:\r\n\r\n1. As mentioned by @pietern, the hook deletion function should be implemented in `torch/csrc/autograd/function.h`, as it owns the data. \r\n2. We should not slow down existing use cases of `add_post_hook` and `post_hooks()`.\r\n\r\nI initially thought about using an `OrderedDict` to store named hooks, as what we did for params, buffers, and children in `nn/Module.h`, but that would violate the second requirement.\r\n\r\n~Another possibility is that, instead of using the default deleter, we create a special deleter for the hook unique ptr in DDP, e.g., `ReducerHookDeleter`, that wraps the default deleter.  The `add_post_hook` and `post_hooks()` APIs would stay the same, then we add one `delete_post_hook<DeleterType>()` API to `torch/csrc/autograd/function.h`, which loops over all post hooks, and only delete the ones with matching deleter type, i.e., `ReducerHookDeleter`. This would be slow, but is OK, as we only need this on failures, where timeout delay will dominate. Any comments?~ Should be able to directly check hook pointer type."}, "21363": {"question": "pytorch 1.1.0 fails to load on windows (python3.6, 3.7)", "answer": "I extracted the build scripts from your log, that is, the code below.\r\n```cmd\r\nset PYTHON=Python36\r\nset ARCH=-x64 \r\nset PYTORCH=1.1.0-cp36-cp36m\r\n\r\ngit clone -q --depth=25 --branch=travis-pytorch-avy https://github.com/jvesely/PsyNeuLink.git C:\\projects\\psyneulink-wuxsn\r\ncd C:\\projects\\psyneulink-wuxsn\r\ngit checkout -qf ae5a4dcbe1f72bf83c199ce8e95b21828b90219d\r\n\r\nchoco upgrade graphviz.portable -y\r\npip --version\r\npip install --user -U pip\r\npip --version\r\npip install --user -U certifi \"numpy<1.16\"\r\npip install --user git+https://github.com/benureau/leabra.git@master\r\nif not \"%PYTORCH%\" == \"\" pip install --user http://download.pytorch.org/whl/cpu/torch-%PYTORCH%-win_amd64.whl\r\nif \"%PYTORCH%\" == \"\" (findstr /V torch < dev_requirements.txt > tmp_req && move /Y tmp_req dev_requirements.txt)\r\npip install --user -e .[dev]\r\npytest --junit-xml=tests_out.xml -n auto --strict-markers tests/ %EXTRA_ARGS%\r\ncurl -X POST -F \"file=@tests_out.xml\" https://ci.appveyor.com/api/testresults/junit/%APPVEYOR_JOB_ID%\r\n```\r\nAnd then I tried to reproduce it locally, so I started to contrust the env according to your commands and did the smoke testing after that. And I found that it is throwing out an error.\r\n```cmd\r\nPython 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Python36\\lib\\site-packages\\torch\\__init__.py\", line 79, in <module>\r\n    from torch._C import *\r\nModuleNotFoundError: No module named 'numpy.core._multiarray_umath'\r\n>>> quit()\r\n```\r\nAfter that, I tried to upgrade numpy from 3.15 to 3.16, and it worked. I also tried to downgrade torch to 1.0.1 and it also worked. So you could actually remove the version constraint on numpy and try again."}, "21402": {"question": "Why module->eval() doesn't work in C++", "answer": "nvm, got it to work in annotation method as well.. Just had to add `model.eval();` after loading the model in C++. "}, "21412": {"question": "Support sublist arguments for torch.einsum", "answer": "Similar to numpy functionality `torch.einsum(([[0,1],[0,2],[0,3],[0,4]],[1,2,3,4]), comp_list)` can be implemented to transform first parameter to string and then call already implemented `torch.einsum('..', comp_list)`\r\n\r\nOne way to do this:\r\n1. implement additional `einsum` in  `ATen/native/Linear.cpp`\r\n```\r\nTensor einsum(Tensor lhs_t, Tensor rhs_t, TensorList tensors) {\r\n      // convert lhs_t and rhs_t to eqn\r\n     std::string eqn =  ... ;\r\n     return at::einsum(eqn, tensors);\r\n}\r\n``` \r\n2. add definition to `native_funcions.yaml`\r\n```\r\n- func: einsum.Tensor(Tensor lhs_t, Tensor rhs_t, Tensor[] tensors) -> Tensor\r\n  use_c10_dispatcher: unboxed_only\r\n```\r\n3. `torch/functional.py`  dispatch call in `def einsum(equation, *operands):` by checking if the first parameter is a string \r\n\r\nAnother:\r\n \r\n1. just transform the first argument to string directly in `torch/functional.py`, in that case, there won't be any cpp implementation for that. \r\n\r\nI have the following questions :\r\n1. Which approach suits best to PyTorch?\r\n2. Using only-python support we can treat python `ellipsis` , but I don't see how ellipsis can be supported in cpp implementation of subscripts.\r\n3. If it should be a cpp version, which types for subscripts arrays are better to use?  "}, "21415": {"question": "NCCL process groups don't support `.group_ranks()`", "answer": "In that case you should be able to use `torch.distributed.get_world_size(group=pg)` directly.\r\n\r\nThe `pg` is still to be considered an abstract object. In reality it's an instance of the C++ `c10d::ProcessGroup` class, and you can also call `pg.size` to get the same information. This is not considered public API so please prefer going through the `torch.distributed` function instead."}, "21536": {"question": "JIT RuntimeError: isTensor() ASSERT FAILED", "answer": "@wanchaol thanks for providing a solution, but I don't feel quite convinced about it. I just see one thing here: from the maintenance side of the library if we want to provide jit ready functions to be traced, does it mean that we have to wrap every single function to a ScriptModule, cast all to tensors and so ? Also from user side, in case we don't provide such wrapping could be really a pain to the user and convert the whole thing to the worst user friendly lib ever.\r\n\r\nA \"quick\" fix we figured out would be converting the non-tensors parameters from all signatures to tensor, meaning that this will break all our current api and its backward compatibility. We have also been tracking this issue: https://github.com/pytorch/pytorch/issues/20939 Do you think that this will make things easier from jit perspective side ?"}, "21537": {"question": "MKLDNN convolution leaks memory", "answer": "Thanks for reporting this issue. We've been able to identify the issue and come up with a workaround as below. A formal fix will be submitted soon. \r\n\r\nA simple workaround in mkldnn src/common/stream.cpp.\r\nhttps://github.com/intel/mkl-dnn/blob/rls-v0.18/src/common/stream.cpp\r\n\r\n```\r\ndiff --git a/src/common/stream.cpp b/src/common/stream.cpp\r\nindex 054fbb9..be11cf0 100644\r\n--- a/src/common/stream.cpp\r\n+++ b/src/common/stream.cpp\r\n@@ -46,7 +46,9 @@ status_t stream_t::submit(const nstl::vector<primitive_t *> &prims,\r\n\r\n     const size_t start = stream_.size();\r\n     stream_.insert(stream_.end(), prims.begin(), prims.end());\r\n-    return submit_impl(start, stream_.size(), error_prim);\r\n+    auto res = submit_impl(start, stream_.size(), error_prim);\r\n+    stream_.clear();\r\n+    return res;\r\n }\r\n\r\n bool stream_t::closed() const { return true; }\r\n```\r\n\r\n"}, "21551": {"question": "How to disable MKL-DNN 64-bit compilation?", "answer": "It may be caused by the use of `sudo` here, because some configuration of sudo would drop or only keep some current environment variables. Normally you shouldn't run the build command with `sudo`; but if you have to do so, you can try `sudo -E` to force `sudo` to preserve environment variables."}, "21574": {"question": "Using Ninja instead of Make results in inability to find correct headers when building from source on Linux", "answer": "Yes it seems to all work normally with USE_NINJA=OFF so far"}, "25390": {"question": "dist.new_group() failed. BUG or I misunderstood something?", "answer": "Hey @lecoan, the doc says:\r\n\r\n> This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. \r\n\r\nIn the code snippet above, you have:\r\n\r\n```python\r\n    start = 0\r\n    while rank not in perm[start: start + 2]:\r\n        start += 2\r\n    group = perm.tolist()[start: start + 2]\r\n    pg = dist.new_group(group, timeout=timedelta(seconds=30))\r\n```\r\n\r\nSo all processes will enter `dist.new_group` but with different `group` ranks. As a result, all of them will enter `_new_process_group_helper` together with a smaller world size and duplicated group ranks. \r\n\r\nhttps://github.com/pytorch/pytorch/blob/590619ab8c2d237d4e0b55c8cc3552932afe7da5/torch/distributed/distributed_c10d.py#L1471-L1486\r\n\r\nCan you try making multiple `dist.new_group` calls on all processes, one call per new group. Say if you would like to create two new groups [0, 1] and [2, 3] out of 4 processes, then each process should do sth like the following, even if they are not in the group.\r\n\r\n```python\r\ndis.new_group([0, 1], ...)\r\ndis.new_group([2, 3], ...)\r\n```"}, "25399": {"question": "No type hints on nn.Parameter", "answer": "In general, you should replicate the module import structure of the original py files, which definitionally don't have a cycle. It's possible we messed up some imports in the pyi files; in that case, you'd have to fix it."}, "25433": {"question": "[jit] Can't script .type() ", "answer": "Thanks for the report! We should fix this but as a workaround until then you can use `x.to(torch.int8)`"}, "25463": {"question": "torch.distributed.gather(): the type of gather_list parameter must be list[Tensor]?", "answer": "This works for me:\r\n\r\n```python\r\nimport torch\r\nimport torch.distributed as dist\r\n\r\ndist.init_process_group(\"gloo\")\r\n\r\ntensor = torch.tensor([dist.get_rank()], dtype=torch.int32)\r\nif dist.get_rank() == 0:\r\n    output = [tensor.clone() for _ in range(dist.get_world_size())]\r\n    dist.gather(tensor=tensor, gather_list=output, dst=0)\r\n    print(output)\r\nelse:\r\n    dist.gather(tensor=tensor, gather_list=[], dst=0)\r\n```\r\n\r\nThat said, we can improve this such that the non-dst doesn't have to specify `gather_list`."}, "25498": {"question": "[jit] torch.jit.script range() input type raises isInt() when used with int tensor values", "answer": "You can find more info on https://pytorch.org/ with the Quick Start selector (change it to \"Preview (Nightly)\" at the top), but this should work for your env:\r\n\r\n```bash\r\npip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\r\n```\r\n"}, "25537": {"question": "RuntimeError: CUDA error: device-side assert triggered - yesterday it worked", "answer": "I met the same error, then I set 'device = torch.device(\"cpu\")' to run codes on cpu, the error statements were much more clear.\r\nAnyone has a similar error can have a try."}, "23862": {"question": "nn.functional.conv2d is a factor 5 slower when using specific weight tensor on CPU.", "answer": "It seems your performances gets a hit by handling denormal values.\r\nTry to set [torch.set_flush_denormal(True)](https://pytorch.org/docs/stable/torch.html#torch.set_flush_denormal) and profile the code again."}, "23876": {"question": "interfaces of many schedulers in lr_scheduler.py are missing in lr_scheduler.pyi", "answer": "I checked the lr_scheduler.pyi file and only found the missing type checking for two classes. I have submitted a PR #23934 . If you find other files in which there is a missing type check, I am willing to help."}, "23900": {"question": "Memory leak when using itertools.cycle", "answer": "Ok upon further investigation it seems that `itertools.cycle` attempts to save all outputs in order to re-cycle through them. Replacing it with something like this:\r\n\r\n```python\r\ndef cycle(iterable):\r\n    iterator = iter(iterable)\r\n    while True:\r\n        try:\r\n            yield next(iterator)\r\n        except StopIteration:\r\n            iterator = iter(iterable)\r\n```\r\n\r\nsolves the issue. Is there a canonical way of creating an infinite iterator in pytorch? It is unfortunate that `itertools.cycle` is implemented in this way, although I suppose it makes sense in the average case where the amount of data is very low.\r\n\r\nI will close this issue since it's not a pytorch issue."}, "23907": {"question": "count_nonzero", "answer": "We have added torch.count_nonzero, which I believe addresses this issue. See: https://pytorch.org/docs/master/generated/torch.count_nonzero.html?highlight=count_nonzero#torch.count_nonzero."}, "23928": {"question": "New _batch_mahalanobis slower than in previous commit", "answer": "Yes, it is undesirable. For such shapes of A, and b, I would like to make `b` have shape `(10, 1000)` and apply triangular solve for A and new b (so no broadcasting is triggered).\r\n\r\nBtw, the regression here is caused by a performance issue of triangular_solve in GPU with batch_size=1. Consider\r\n```\r\nimport torch\r\nx = torch.eye(2).cuda()\r\ny = torch.eye(2).cuda()\r\nbx = x.reshape(1, 2, 2)\r\nby = y.reshape(1, 2, 2)\r\n\r\n%%timeit\r\ntorch.cuda.synchronize()\r\ntorch.triangular_solve(x, y)\r\n\r\n%%timeit\r\ntorch.cuda.synchronize()\r\ntorch.triangular_solve(bx, by)\r\n```\r\nwhich will return\r\n```\r\n45.4 \u00b5s \u00b1 318 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n```\r\nfor the first case and\r\n```\r\n243 \u00b5s \u00b1 3.81 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n```\r\nfor the second case.\r\n\r\nI guess we can fix it in `triangular_solve` implementation. Or I can add an if/else check to squeeze the input when `batch_shape=(1,)`. What do you think? Fixing it in `triangular_solve` seems more reasonable to me."}, "23929": {"question": "Quantized Linear does not work for bias=False ", "answer": "The issue is that we don't have the direct test coverage for the `from_float` function with `bias = NULL`. The current unit test with \"indirect\" test coverage is to test `convert` and `quantize` function from here:\r\nhttps://github.com/pytorch/pytorch/blob/master/test/test_quantization.py#L21\r\n\r\nPreviously we have fixed the general `bias = nullptr` here: https://github.com/pytorch/pytorch/pull/22403"}, "23930": {"question": "forward_packed operator in LSTM not supported by jit scriptmodule", "answer": "I think this is expected for your example, the hidden state is supposed to be of `Optional[Tuple[Tensor, Tensor]]` instead of just `Tensor`. Your example fails in eager mode as well\r\n\r\n```\r\nRuntimeError: Expected hidden[0] size (1, 1, 512), got (1, 512)\r\n```\r\n\r\nYour example runs fine if you use `h1` for the hidden state instead. Closing since this looks like user error, feel free to re-open if you are still running into errors."}, "23941": {"question": "Getting gradient of element of tensor wrt the element itself", "answer": "```\r\nprint(torch.autograd.grad(b[0][0], b[0], allow_unused=True)\r\n```\r\n\r\nThis doesn't do what you want because the autograd graph for `b[0][0]` is not related to `b[0]` (no CSE happens here.) The second code is correct and you should do it that way."}, "23957": {"question": "[naming] Promote _LRScheduler to LRScheduler", "answer": "Since this could encourage people to experiment with the learning rate scheduler class, I'm ok if @vadimkantorov wants to open a pull request promoting `_LRScheduler` to `LRScheduler`, and keeping the old name for backward compatibility, potentially with a deprecation warning. "}, "23982": {"question": "[docs] Update and momentum formulas in SGD docs", "answer": "IMO, this is a no brainer. I'll put up a PR.\r\n\r\nShould it be\r\n`v_{t+1} = p_{t}*v_{t} + g_{t+1}`? (Added a subscript for `p`)"}, "23993": {"question": "torch.jit.trace() does not work without check_trace =False", "answer": "> hoe to slove it ?\r\nJust don't pass the same tensor 3 times as the same var. See the next my comment.\r\n\r\n"}, "23999": {"question": "TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error: Not within tolerance", "answer": "I solved the problem by adding model.eval() and deleting the model.cuda(), aka, map the model to cpu"}, "10715": {"question": "[JIT] Making JIT work with Pyro's VAE example", "answer": "> Do the examples do a lot of operations with scalar tensors?\r\n\r\nThe arguments passed to the traced functions are not scalars in many cases, but there may be other places within the traced function call (e.g. in distributions or our own internals) that have scalar operations. Is there a way to get a more detailed JIT log to localize where the issue might be?"}, "10742": {"question": "[feature request][pytorch] finfo as in numpy and finfo for default dtype", "answer": "If you don't care about where the code lives: https://github.com/pytorch/pytorch/blob/ddf187c198f8e249e78351ba94e773bf9d21de3a/torch/distributions/utils.py#L20"}, "10757": {"question": "Performance improvement on sparse CUDA coalesce()", "answer": "Closing this because when nnz is large enough, CUDA kernel actually performance reasonably well:\r\n```\r\n>>> from random import *\r\n>>> n = 100000\r\n>>> I = torch.tensor([[randint(0, 99) for _ in range(3)] for _ in range(n)])\r\n>>> V = torch.randn(n)\r\n>>> size = torch.Size([1000, 1000, 1000])\r\n>>> S = torch.sparse_coo_tensor(I.t(), V, size)\r\n\r\n>>> %timeit S.coalesce()\r\n10 loops, best of 3: 30.7 ms per loop\r\n\r\n>>> S = torch.sparse_coo_tensor(I.t(), V.cuda(), size)\r\n>>> %timeit torch.cuda.synchronize(); S.coalesce(); torch.cuda.synchronize();\r\n100 loops, best of 3: 9.59 ms per loop\r\n```"}, "10784": {"question": "autograd's elu_backward usage seems not correct", "answer": "Closed it thinking I made a mistake but actually indeed seems wrong.\r\n\r\nThis line is on ATen's Type.h:\r\n`virtual Tensor elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, const Tensor & output) const;`\r\n\r\n`elu_forward` declaration in derivatives.yaml seems wrong too, no input_scale in ATen"}, "10817": {"question": "Parameters in dict not registered", "answer": "I believe you have to use [`register_parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.register_parameter) for this purpose."}, "10853": {"question": "Cuda runtime error : the launch timed out and was terminated", "answer": "Problem cause: https://devtalk.nvidia.com/default/topic/1043126/linux/xid-8-in-various-cuda-deep-learning-applications-for-nvidia-gtx-1080-ti/"}, "10855": {"question": "[Outdated documentation] Previous Versions Installation with Conda", "answer": "pointing `previous versions` to use `soumith` channel was a mistake. We switch from `soumith` to `pytorch` channel (i think in 0.3.1), but forgot to update the page. I just fixed the link via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa\r\n\r\nPyTorch Windows support officially released from v0.4.0, and hence we only have 0.4.0 available from the `pytorch` channel since then. For unofficial / previous releases, see `peterjc123`'s channel: https://anaconda.org/peterjc123\r\n\r\nClosed via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa\r\n"}, "229": {"question": "flip a Tensor", "answer": "Here's @dmarnerides code but with cuda support\r\n\r\n```py\r\n# https://github.com/pytorch/pytorch/issues/229\r\ndef flip(x, dim):\r\n    dim = x.dim() + dim if dim < 0 else dim\r\n    inds = tuple(slice(None, None) if i != dim\r\n             else x.new(torch.arange(x.size(i)-1, -1, -1).tolist()).long()\r\n             for i in range(x.dim()))\r\n    return x[inds]\r\n\r\n# Code to test it with cpu\r\na = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4)\r\nprint(a)\r\nprint(flip(a, 0)) # Or -4\r\nprint(flip(a, 1)) # Or -3\r\nprint(flip(a, 2)) # Or -2\r\nprint(flip(a, 3)) # Or -1\r\n\r\n# Code to test it with cuda\r\na = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4).cuda()\r\nprint(a)\r\nprint(flip(a, 0)) # Or -4\r\nprint(flip(a, 1)) # Or -3\r\nprint(flip(a, 2)) # Or -2\r\nprint(flip(a, 3)) # Or -1\r\n```"}, "241": {"question": "PyTorch goes distributed", "answer": "Shubho here from SVAIL @ Baidu\r\n\r\nOne long-term thing to keep in mind is interfacing with a job scheduler - SLURM is pretty standard. I think using `salloc` with `pytorch_exec` should be fairly easy.\r\n\r\nThe framework that I helped architect at Baidu follows a peer-to-peer model - all workers are peers - no master and no slave - each peer has access to 1 GPU - and has a synchronous view of the parameter space that is completely replicated on each worker. Workers use MPI to communicate - and the MPI byte-transport layer deals with IBVerb and CUDA shared mem transport (if GPUs are on the same PCI-E root complex and many are). This peer-to-peer architecture is relatively simple and scales really well - at least to 256 GPUs and possibly more (at this point people started losing friends for hogging the cluster). It can also sustain about 70% of InfiniBand's peak bandwidth (for FDR Infiniband) but I had to reimplement the MPI collective operations since they are not optimized. This simple architecture has served us well for anything we train - various recurrent nets with or without attention, wavenet and I don't see why convnets should be an issue. This setup is however not fault tolerant - if a peer dies - the training comes to a halt - and SLURM will time it out and reschedule and this is fairly painless since we save checkpoints and it restarts from the last checkpoint. People rarely notice these failures in practice.\r\n\r\nPossibly `torch.distributed` is more full featured than this... I haven't started looking at the code yet.. but will soon...\r\n\r\nHappy to test on our cluster since we have FDR InfiniBand backplane with OpenMPI and SLURM and also contribute our learnings and code"}, "254": {"question": "UnboundLocalError when importing torch.cuda", "answer": "In pytorch you can import torch.cuda always. cuda is lazy loaded only when actually first used"}, "259": {"question": "ubuntu 16.04, CUDA8 and pytorch have compile issues, investigate", "answer": "Hi @soumith \r\n\r\nI had this problem. Mine was because of the thrust library in Cuda.  (my version was 8.0.27)\r\n\r\nI updated to 8.0.44 (recent version) and it was solved. \r\n\r\n"}, "260": {"question": "define default GPU device", "answer": "Yeah I think `set_device` would be better. It has much clearer semantics. With `set_default_device` it might sometimes work inside a nested function, and sometimes silently have no effect.\r\n\r\n@colesbury I also don't like this very much, but as @soumith says, I think it's useful for notebooks. However, we should clearly discourage its usage in the docs, except for these situations."}, "262": {"question": "allow forward / backward hooks to rewrite outputs and gradients", "answer": "You can overcame the need of the `input`/`output` by using upvalues to the function, and keeping record of them in list/dictionaries."}, "274": {"question": "auto-wrap tensors as inputs to autograd", "answer": "It's pretty much all about making the flag viral or not. The output of the function doesn't require grad if **all** inputs don't require it. This allows you to efficiently use pretrained models and never backprop through them. The output is volatile if **any** of the inputs is volatile. This is convenient for inference because you don't need to modify your parameters, but simply create a volatile input.\r\n\r\nAnother difference is that at the moment variables with requires_grad=False are still constructing the graph, while volatile ones don't (creator is None for all of them)"}, "289": {"question": "squeeze dimension after mean / sum", "answer": "I've been working on broadcasting, and it probably makes sense to introduce broadcasting and squeeze dimension together (i.e. \"Broadcasting itself can be added sooner as it doesn't need any thought on backward-compatibility\" isn't quite true).\r\n\r\nFor example, consider test_nn.test_InstanceNorm1d:\r\n```\r\n        output = IN(input_var)\r\n        \r\n        input_reshaped = input_var.transpose(1, 0).contiguous().view(c, -1)\r\n        mean = input_reshaped.mean(1)\r\n\r\n        # do some calculation based on mean.data - IN.running_mean\r\n```\r\n\r\nHere, mean.data is (4,1) (because the dimension is not squeezed) and In.running_mean is (4).  The broadcast causes the result of the calculation to be (4,4) instead of (4,1), which changes the calculation (e.g. imagine a sum).  If mean is changed to squeeze the dimension, this calculation \"just works\" as written.  Clearly there are cases where this won't hold, but it seems like introducing them together may cause less disruption."}, "309": {"question": "gradient clip for optimizer", "answer": "For those reading this thread, note that clip_grad_norm is now deprecated for clip_grad_norm_"}, "327": {"question": "Segmentation fault when dividing by zero with integer tensors", "answer": "integer division by zero getting a FP exception is something we cannot avoid, it's a hardware exception. Numpy wraps it's integer division code to literally check the denominators with conditionals and instead generates fpectl exceptions. I dont think this can be a hi-pri issue for us just looking at the amount of work involved in refactoring TH.\r\n\r\nYour second issue looks more like a bug."}, "329": {"question": "Softmax2d and LogSoftmax don't work for 2d and higher", "answer": "It looks correct to me. `Softmax2d` does a softmax over channels, in each point of the 3rd and 4th dimension independently. You have only a single channel, so all values get normalized to ones."}, "334": {"question": "CUDA initialization fails fatally in multiprocessing ", "answer": "Seems that multithreading works in this example. I will first try this line and see how it works. \r\n\r\nThanks all for the help! "}, "345": {"question": "cudnn backend needs contiguity checks to report better error messages", "answer": "Ok, so it appears that cuDNN conv modules could give invalid results, but that could happen only if one had non-contiguous weights. **As long as you sticked to nn modules this bug did not affect you in any way.** For RNN modules, we haven't seen any issues like these with conv, but we'll go over the code to make sure there are no more errors like these.\r\n\r\nAnother other error is that sometimes non-contuguous inputs gave `CUDNN_STATUS_INTERNAL_ERROR` for some unknown reason. But it was harmless in a sense, that it would quietly mess up your results.\r\n\r\n@ngimel do you know of any other places where we should be more careful with contiguity checks? any hints on why non-contig inputs can give internal errors?"}, "358": {"question": "nn.Sequential should have an add_module(module) instead of add_module(name, module)", "answer": "If you want to just change the stride/padding/dilation, you can just directly change the [property](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py#L22) on the `ConvNd` object.\r\n\r\nIf you want to reuse the pretrained weight, just do `new_layer.weight = orig_layer.weight` and then use the `new_layer` when constructing the new `Sequential`."}, "381": {"question": "Some function don't implement \"out=result\" convention", "answer": "`out` has to be a `ByteTensor`."}, "393": {"question": "THNN unnecessary .zero() calls on gradients", "answer": "this is now fixed in master: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/batch_normalization.cpp#L119"}, "413": {"question": "binaries compiled against newer numpy, but executed with older numpy error out", "answer": "upgrade your numpy"}, "427": {"question": "simpler explode and join", "answer": "Just tried `torch.stack`, it's great, thanks!\r\n\r\nRe #289, numpy-like broadcast would be awesome (even if it was an explicit broadcast, like `t.broadcast(dim, num_repeat)`)."}, "44460": {"question": "How to package pytorch with the file build from source.", "answer": "Use `python setup.py bdist_wheel` to build a wheel file. The wheel file will be in the `dist` sub-directory."}, "44467": {"question": "Missing tests for most torch.*(out=...) tensor operators", "answer": "I think https://github.com/pytorch/pytorch/pull/53259 addresses this issue. Let me know if you agree @antocuni. If it doesn't, let's maybe open a new issue that explains what else should be done."}, "44527": {"question": "BatchSampler & PEP 479", "answer": "try making the list [1,2,3] into an iterator and try again. That iterator would raise StopIteration at the end and it would be converted to RuntimeError in this parent iterator according to the PEP."}, "44530": {"question": "`CatTransform` should work with `event_dim > 0` transforms", "answer": "@vikigenius hmm I think I see your point now, and I think my previous comment was wrong. Does this look right to you:\r\n1. The concatenation `dim` can be either a batch dim or an event dim.\r\n2. If `dim` is a batch dim then the current logic is correct, i.e. concatenate the log-det-jacobians:\r\n    ```py\r\n    logdeghacs = []\r\n    for trans, length in zip(self.transforms, self.lengths):\r\n        ...\r\n        logdetjacs.append(trans.log_abs_det_jacobian(...))\r\n    return torch.cat(logdetjacs, dim=self.dim)\r\n    ```\r\n3. If `dim` is an event dim then we need new logic to instead *add* the component log-det-jacobians:\r\n    ```py\r\n    ...\r\n    return sum(logdetjacs)\r\n    ```\r\n\r\nIf this sounds correct, then I think there's a pretty simple fix:\r\n\r\n<details>\r\n\r\n```diff\r\ndiff --git a/torch/distributions/transforms.py b/torch/distributions/transforms.py\r\nindex 09e00d55e8..eddce21c25 100644\r\n--- a/torch/distributions/transforms.py\r\n+++ b/torch/distributions/transforms.py\r\n@@ -633,6 +633,7 @@ class CatTransform(Transform):\r\n     \"\"\"\r\n     def __init__(self, tseq, dim=0, lengths=None, cache_size=0):\r\n         assert all(isinstance(t, Transform) for t in tseq)\r\n+        assert len({t.event_dim for t in tseq}) == 1\r\n         if cache_size:\r\n             tseq = [t.with_cache(cache_size) for t in tseq]\r\n         super(CatTransform, self).__init__(cache_size=cache_size)\r\n@@ -686,7 +687,15 @@ class CatTransform(Transform):\r\n             yslice = y.narrow(self.dim, start, length)\r\n             logdetjacs.append(trans.log_abs_det_jacobian(xslice, yslice))\r\n             start = start + length  # avoid += for jit compat\r\n-        return torch.cat(logdetjacs, dim=self.dim)\r\n+        dim = self.event_dim + (self.dim if self.dim < 0 else self.dim - x.dim())\r\n+        if dim < 0:  # concatenate along a batch_dim\r\n+            return torch.cat(logdetjacs, dim=dim)\r\n+        else:  # concatenate along an event_dim\r\n+            return sum(logdetjacs)\r\n+\r\n+    @property\r\n+    def event_dim(self):\r\n+        return self.transforms[0].event_dim\r\n\r\n     @property\r\n     def bijective(self):\r\n```\r\n\r\n</details>"}, "44594": {"question": "Build-from-source Failed on Mac OS", "answer": "Hi,\r\n\r\nThis is simple to fix. All you have to do is to change the `%ld` to `%lld` at `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:29` and `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:31`. Can you make that change and try to build?"}, "44599": {"question": "[Windows CUDA Build] Unknown option '-Xcompiler /w -w'", "answer": "Seems it's similar but not exactly that issue (and that's already fixed, probably)\r\n\r\nFound 2 ways around:\r\n1. Turn off cuda separable compilation.\r\n2. msbuild is probably OK. But it's way too slow (no concurrency for nvcc, will run for hours) so I didn't do a full test."}, "44601": {"question": "Implementation of 2d bicubic grid sampler", "answer": "Answer to myself. If the forward is implemented, the numerical method could be used to get the ground truth of backward gradient. i.e. forward a small amount dh, then get the output dH. dH/dh is the partial differential for the specific pixel output and pixel input"}, "44606": {"question": "Something goes wrong with pytorch build from source,", "answer": "Hi,\r\n\r\nWe use github issues only for bugs or feature requests.\r\nPlease use the forum to ask questions: https://discuss.pytorch.org/\r\n\r\nIn your case it looks like some issues with the GLOO detection. Are you planning on using distributed? If not you can try to set `USE_DISTRIBUTED=0`."}, "44619": {"question": "OneCycleLR Scheduler does not have argument for verbose", "answer": "Fixed by #41580"}, "44635": {"question": "rewrite the torch.sparse main doc page", "answer": "For me it was super-confusing multitude of different matrix multiply ops:\r\ntorch.mm, torch.matmul torch.sparse.mm, torch.sparse.FloatTensor.spmm, torch.sparse.FloatTensor.hspmm, torch.sparse.FloatTensor.sspmm\r\n\r\nMy practical usecase was sketching: dense-sparse multiply with +1/-1 sparse matrix (requires no gradient to the sparse tensor): https://gist.github.com/vadimkantorov/d9b56f9b85f1f4ce59ffecf893a1581a#file-compact_bilinear_pooling-py-L15 "}, "44636": {"question": "[feature request] ONNX export for torch.std_mean / torch.var_mean", "answer": "We recently merged a PR (https://github.com/pytorch/pytorch/pull/45678) adding support for `var_mean` and `std_mean`. Please try it out and see if we can close this item."}, "44708": {"question": "[JIT] RuntimeError:  ill formed octal specifier for List[str]", "answer": "Thanks for the help @SplitInfinity. I can confirm that 1.7 solved the problem :D"}, "44714": {"question": "torch.utils.data.random_split crashes without an error message with non CPU Generator object", "answer": "> > @Kae1101\r\n> > I got the same problem 2 day ago in colab, my code also ran find before. I think the problem happen when your DataLoader for training set attribute shuffle=True, you can try with your test DataLoader, which shuffle attribute set to False, the problem won't happen.\r\n> > I find the way to make my code run again, hope it will help you\r\n> > add attribute generator to your DataLoader and set it like this:\r\n> > train_loader = DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=0,pin_memory=False, generator=torch.Generator(device='cuda'))\r\n> > Hope it gonna help :)))\r\n> \r\n> @NLQVan\r\n> Thanks for your solution~But I already fixed the problem by commenting out the following command:\r\n> \r\n> # torch.set_default_tensor_type(torch.cuda.FloatTensor)\r\n> does dataiter.next() return cpu.floatTensor by default? If it does, I think that is why the error was reported... ...\r\n> \r\n> But I still confused because I ran the same code may be more than 50 times a week before 2021/06/19 and didn't get any errors like this.\r\n\r\nThanks. I was having the same issue and downgrading to 1.8.1 (March release) from 1.9.0 (June release, current latest) fixes this. "}, "7415": {"question": "Memory surges when loading models", "answer": "Came across this while training on a GPU with very limited memory. Training worked fine but upon restoring from a previous checkpoint, got OOM on the GPU. Thanks so much for the hint to \"load the checkpoint to cpu first and then move onto GPU\" and the note in the torch.load() function! This easily worked for me:\r\n```\r\n# Bad gives OOM on GPU\r\n# params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\r\n# Good. Load to cpu first, then to GPU\r\nparams = torch.load(model_save_path, map_location='cpu')\r\nmodel.load_state_dict(params['state_dict'])\r\nmodel = model.to(device)\r\n```\r\nSubsequently, I ran into them same problem with the optimizer, where I got OOM while the GPU tensors where copied. The same strategy was successful here too, namely\r\n```\r\n# Bad. Gives OOM\r\n# optimizer.load_state_dict(torch.load(model_save_path + '.optim')\r\n# Good. Works.\r\noptimizer.load_state_dict(torch.load(model_save_path + '.optim', map_location='cpu'))\r\noptimizer_to(optimizer,device)\r\n```\r\nHere, optimizer_to is the code snippet from @0phoff posted in #8741 \r\n```\r\ndef optimizer_to(optim, device):\r\n    for param in optim.state.values():\r\n        # Not sure there are any global tensors in the state dict\r\n        if isinstance(param, torch.Tensor):\r\n            param.data = param.data.to(device)\r\n            if param._grad is not None:\r\n                param._grad.data = param._grad.data.to(device)\r\n        elif isinstance(param, dict):\r\n            for subparam in param.values():\r\n                if isinstance(subparam, torch.Tensor):\r\n                    subparam.data = subparam.data.to(device)\r\n                    if subparam._grad is not None:\r\n                        subparam._grad.data = subparam._grad.data.to(device)\r\n```\r\n(This issue came up prominently on google when searching my error, so just would like to share how one can solve.)"}, "7416": {"question": "[feature request] `select_index` for sparse tensors", "answer": "Thanks. I have solved the problem. I just need to get one-hot embedding for the diag sparse matrix. So I do not need to select the specific row. Instead, I directly generate the one-hot embeddings in the training process rather than select from the sparse matrix."}, "7425": {"question": "grad of output with respect to inputs  functions on cpu but not gpu", "answer": "Your code had a bug, I fixed it for you:\r\n\r\n```\r\ndef test_gpu():\r\n    mod = testModule().cuda()\r\n    t = torch.ones([1, 10], requires_grad=True, device=\"cuda:0\")\r\n    output = mod(t)\r\n    output[0].backward()\r\n    test = t.grad\r\n```"}, "7426": {"question": "Caffe2 install failure", "answer": "Serendipitously, I was able to eventually find the solution to my problem. I will post my solution here, in case someone else stumbles upon this same problem, then I'll close this issue. \r\n\r\n1. Someone else had a similar issue on the caffe2 repo: https://github.com/caffe2/caffe2/issues/2487 and the solution was to re-install the eigen3 library from the master branch at the [github mirror](https://github.com/eigenteam/eigen-git-mirror.git). So that solved this particular issue.\r\n2. Next, I had another problem with cuda/cudnn somehow (that I did not screenshot) but it was similar to [this problem](https://discuss.pytorch.org/t/solved-source-compile-error/9490). Incredibly, as this link suggests, the solution was to change a line in the file: /usr/include/cudnn.h from\r\n```c++\r\n#include \u201cdriver_types.h\u201d\r\n```\r\nto\r\n```c++\r\n#include <driver_types.h>\r\n```\r\nSo that it looks in the correct location for the header file `driver_types.h`...Now everything is installed properly and hopefully this helps someone else down the road."}, "7434": {"question": "THD refactoring", "answer": "Don't think we need this issue from now on."}, "7450": {"question": "[Caffe2] Are you still maintaining Caffe2 docker?", "answer": "@jgong5 @pjh5\r\nI think it's better to update the Caffe2 images. Because the Detectron Dockerfile is based on Caffe2 base. \r\nIt's inconvenient to build our own Caffe2 while I just want to use Detectron."}, "7460": {"question": "[Feature Request] nn.Module should also get a `device` attribute", "answer": "That\u2019s not possible. Modules can hold parameters of different types on different devices, and so it\u2019s not always possible to unambiguously determine the device."}, "7461": {"question": "magma in pytorch", "answer": "because we statically link magma, the magma package is not needed at runtime."}, "7481": {"question": "Cannot deepcopy torch.(int/float/...)*", "answer": "That's because `numpy.float32` isn't a numpy dtype:\r\n```\r\ntype(numpy.float32)\r\n<class 'type'>\r\n\r\n\r\n>>> type(numpy.array(0).dtype)\r\n<class 'numpy.dtype'>\r\n\r\n>>> isinstance(numpy.array(0).dtype, type)\r\nFalse\r\n```"}, "7485": {"question": "Multiprocessing runtime error freeze_support() in Windows 64 bit ", "answer": "Read the windows [doc](https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection) please. "}, "7494": {"question": "AttributeError: module 'torch.nn' has no attribute 'BCEWithLogitsLoss'", "answer": "That is too old. You should upgrade to the latest version of pytorch (which is 0.4). Installation instructions can be found at our [main website](https://pytorch.org/). Hope that helps!\r\n\r\nIf you don't want to upgrade you can probably copy and paste the BCELossWithLogits code and make your own Loss function."}, "7499": {"question": "[PyTorch] KeyError: 'momentum' while using SGD optimizer", "answer": "Because I loaded checkpoints from an Adam optimizer."}, "7502": {"question": "Drop support for magma v1 (compilation with it is broken right now)", "answer": "Remove all the `MAGMA_V2` ifdefs"}, "7812": {"question": "Can't get attribute 'Net' on <module '__main__' from 'D:/demo/cnn/test1.py'>", "answer": "You should save & load the statedict instead. :)\r\nSee https://pytorch.org/docs/master/notes/serialization.html"}, "7840": {"question": "Failure to install caffe2 builded from source", "answer": "Closing this issue due to age and because it is now recommended to use PyTorch, not Caffe2. If this is still relevant please file a new issue. "}, "7841": {"question": "Compiling Pytorch 0.4 from source for Tegra (arm processor) fails", "answer": "I'm closing this since the original problem was solved with commit `fb5cc630f6f4cbdebde08e0e82a9679431afa9d2 `.\r\n\r\n@juanmed, if you're running into build issues on Tegra can you open up a new issue with the relevant details (build logs, etc.)? Although, to be honest, I'm not sure how much effort we will put into to supporting PyTorch 0.4. "}, "7844": {"question": "\"Parameters of a model after .cuda() will be different objects with those before the call.\" is wrong.", "answer": "> If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. Parameters of a model after .cuda() will be different objects with those before the call.\r\n>\r\n> In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.\r\n\r\nStarting from https://github.com/pytorch/pytorch/pull/21613, the new behavior we will have in future releases is consistent with this warning, which is that parameters of a model after dtype/device conversion functions such as `.cuda()`/`.cpu()`/`.to()`/`.float()`/`.double()` will be different objects with those before the call (you can enable this new behavior by setting `torch.__future__.set_overwrite_module_params_on_conversion(True)`. Hence we strongly recommend converting the model to a different device / dtype **before** constructing optimizers for it."}, "7868": {"question": "[PyTorch] [ONNX] Peephole optimizer transpose fusion broken", "answer": "Try this on for size https://github.com/pytorch/pytorch/pull/7872\r\n"}, "7883": {"question": "[PyTorch] torch.stft is slow on cpu compared to numpy", "answer": "@Rikorose Now the master has a `stft` with a new signature consistent with `librosa` and using `fft`."}, "7899": {"question": "Cannot import onnx_caffe2.backend", "answer": "I finally resolved it. Seems like I need to use the protobuf that caffe2 depends on instead of the one from conda-forge (they are both 3.5.2 though), to do this just install caffe2 before onnx. And I have to build onnx from source."}, "7903": {"question": "Serious perf drop on CPU ", "answer": "@mingfeima - Thank you for looking into this further and the update! \r\n\r\nI looked into [removing tbb and replacing it with openmp](https://github.com/pytorch/pytorch/pull/8255) and then running the benchmark. I get much better performance (i.e. am able to close the perf gap) when doing so. You could give this a try as well if you like. We generally prefer tbb for many reasons, but might need to (temporarily) go ahead with this if it turns out to be the main source for this and then reintroduce it as part of a larger effort.\r\n\r\nGenerally we can't expect the user to set or know about things like `KMP_BLOCKTIME` or \r\n `KMP_AFFINITY`, so we'd either need to set them statically or change our setup so that they are not necessary.\r\n\r\nEDIT: @mingfeima - I assume this is mkldnn through your channel? I've also seen some issues where it won't set the rpath to link against openmp and mklml\r\n\r\n```\r\n/usr/bin/ld: warning: libmklml_intel.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)\r\n/usr/bin/ld: warning: libiomp5.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)\r\n```\r\n\r\nIt can be fixed by setting my LD_LIBRARY_PATH to point to `/private/home/cpuhrsch/miniconda2/lib`, but I'm wondering if this is something you could adjust as part of your conda package in general? To give you more context, I'm building a binary against libcaffe2.so within a repository outside of pytorch, which was build using mkldnn using [this cmake setup](https://github.com/pytorch/benchmark/blob/f8fb533f5c62f0766eef2e67d372e94b8be7312f/timing/cpp/CMakeLists.txt)."}, "7911": {"question": "How to implement the internal zero padding for the feature map?", "answer": "This works:\r\n```py\r\n>>> import torch.nn.functional as F\r\n>>>\r\n>>> def pad_within(x, stride=2):\r\n...   w = x.new_zeros(stride, stride)\r\n...   w[0, 0] = 1\r\n...   return F.conv_transpose2d(x, w.expand(x.size(1), 1, stride, stride), stride=stride, groups=x.size(1))\r\n...\r\n>>> x = torch.arange(8, dtype=torch.float).view(2, 4).expand(1, 3, 2, 4)\r\n>>> x\r\ntensor([[[[ 0.,  1.,  2.,  3.],\r\n          [ 4.,  5.,  6.,  7.]],\r\n\r\n         [[ 0.,  1.,  2.,  3.],\r\n          [ 4.,  5.,  6.,  7.]],\r\n\r\n         [[ 0.,  1.,  2.,  3.],\r\n          [ 4.,  5.,  6.,  7.]]]])\r\n>>> pad_within(x)\r\ntensor([[[[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\r\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\r\n\r\n         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\r\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\r\n\r\n         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\r\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\r\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]]])\r\n\r\n```\r\n\r\nyou can wrap this in a module to avoid recomputing the weight every time."}, "7940": {"question": "Some confusion about the grad of torch.sign() ", "answer": "You should call `x_binary.retain_grad()`"}, "7960": {"question": "[feature request] Official CUDA support for macOS through eGPU", "answer": "We do support macOS with CUDA GPUs. However atm you have to build from source."}, "56378": {"question": "Pytorch Model Summary", "answer": "So something along the lines of:\r\n\r\n```\r\nfrom prettytable import PrettyTable\r\n\r\ndef count_parameters(model):\r\n    table = PrettyTable([\"Modules\", \"Parameters\"])\r\n    total_params = 0\r\n    for name, parameter in model.named_parameters():\r\n        if parameter.requires_grad:\r\n            param = parameter.numel()\r\n            table.add_row([name, param])\r\n            total_params+=param\r\n    print(table)\r\n    return f\"{total_params:,}\"\r\n    \r\ncount_parameters(net)\r\n```\r\n\r\nwhich outputs:\r\n\r\n```\r\n+---------------------+------------+\r\n|       Modules       | Parameters |\r\n+---------------------+------------+\r\n|  features.0.weight  |    1728    |\r\n|   features.0.bias   |     64     |\r\n|  features.3.weight  |   73728    |\r\n|   features.3.bias   |    128     |\r\n|  features.6.weight  |   294912   |\r\n|   features.6.bias   |    256     |\r\n|  features.8.weight  |   589824   |\r\n|   features.8.bias   |    256     |\r\n|  features.11.weight |  1179648   |\r\n|   features.11.bias  |    512     |\r\n|  features.13.weight |  2359296   |\r\n|   features.13.bias  |    512     |\r\n|  features.16.weight |  2359296   |\r\n|   features.16.bias  |    512     |\r\n|  features.18.weight |  2359296   |\r\n|   features.18.bias  |    512     |\r\n| classifier.0.weight | 102760448  |\r\n|  classifier.0.bias  |    4096    |\r\n| classifier.3.weight |  16777216  |\r\n|  classifier.3.bias  |    4096    |\r\n| classifier.6.weight |  4096000   |\r\n|  classifier.6.bias  |    1000    |\r\n+---------------------+------------+\r\n```\r\n\r\ncould be done.\r\n\r\nWell, this doesn't give shape after each layer."}, "56380": {"question": "register_full_backward_hook does not consistently fire", "answer": "The temporary fix you can use is to make the input require gradients.\r\n\r\n"}, "56382": {"question": "[docs] Rendering type hint issues at torchvision", "answer": "Hello @vadimkantorov \r\nThis was issued due to Sphinx 3.x. This has been fixed over torchvision master by downgrading Sphinx to 2.4.4\r\nDocumentations look are fine now over the webpages for master as well as stable!\r\n\r\n@vadimkantorov rendering is not unified, for each domain library text, vision and audio docs are built and deployed over GitHub pages from their respective repos!\r\n\r\nFeel free to close this as it is fixed!"}, "56576": {"question": "Fail to LOADING A TORCHSCRIPT MODEL IN C++", "answer": "> Here is the other one: https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip. Can you try using this? I used this one and the tutorial is working as expected.\r\n\r\nThank you very much! I have succeed to build the example use the libtorch: \r\n downloaded from `https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip`."}, "56608": {"question": "Create a context manager to enable InferenceMode in python frontend", "answer": "You can already use `torch.is_grad_enabled()` (or the corresponding c++ API) to see if you are in a context where you might end up creating order + 1 derivative."}, "56632": {"question": "Cleaner mechanism in the source-code to check if multiple tensors are on the same device", "answer": "We should avoid wrapping Tensor arguments in TensorArg for perf reasons (this bumps ref count), and all the mentioned functions act on TensorArgs. "}, "56685": {"question": "Incorrect example output in sparse_csr_tensor doc-string", "answer": "We would definitely accept PRs correcting examples to reflect the current version of PyTorch!"}, "56687": {"question": "sparse_csr_tensor segfaults when crow_indices or col_indices are non-tensors", "answer": "This issue waqs solved here: https://github.com/pytorch/pytorch/pull/59010"}, "56699": {"question": "default_pg_timeout in torch/testing/_internal/distributed/distributed_test.py is not sufficient to change system-wide timeouts", "answer": "@ezyang You are running into the RPC timeout which is different from the process group timeout. The default timeout for RPC is set here: https://github.com/pytorch/pytorch/blob/d83ae5d1b74df1ad6e2b80652392ce0c2b31f3f3/torch/csrc/distributed/rpc/init.cpp#L80. \r\n\r\nYou can also modify the rpc timeout by specifying it in rpc_backen_options.rpc_timeout as part of the init_rpc call: https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/dist_utils.py#L83"}, "56702": {"question": "quick-check didn't capture error until PR landed", "answer": "Just checked; the reason is because the Facebook-internal diff [D27774396](https://www.internalfb.com/diff/D27774396) did not match the GitHub pull request.\r\n\r\nYou can also see this by comparing the PR diff...\r\n\r\n- https://github.com/pytorch/pytorch/pull/55992/files#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36\r\n\r\n... to the commit that landed to `master`:\r\n\r\n- https://github.com/pytorch/pytorch/commit/0df239e55082ab947e07161468b61f324eb6bed5#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36"}, "56734": {"question": "Element-wise max of two Tensors computes the wrong gradient in case of equality", "answer": "Looks like the [`torch.amax()`]() docs are aware of this behavior:\r\n```\r\namax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.\r\n```"}, "56741": {"question": "[NNC] Vectorization caused wrong results", "answer": "Fixed by #59423 "}, "41532": {"question": "torch.gather behavior changed from 1.5.1 to master", "answer": "Fixed by #41672."}, "41533": {"question": "Is STFT in torchaudio consistent with librosa?", "answer": "As far as I know, `stft` was not part of `torchaudio`. (@vincentqb can clarify on this one)\r\n\r\n`istft` was originally `torchaudio`, but recently moved to `pytorch`. \r\n\r\nSee https://github.com/pytorch/pytorch/issues/3775 for the development around FT.\r\n\r\nWe check that `istft` is inverse of `torch.stft`, (see [the tests here](https://github.com/pytorch/audio/blob/master/test/functional_cpu_test.py) the same tests have been ported to PyTorch main repo.) but we do not check the parity against `librosa`.\r\n\r\n@LordOfLuck\r\n\r\n> consistent with librosa implementations?\r\n\r\nIf you can give the configuration you are considering to use, we can provide you better insight. In general, I do not expect PyTorch's `stft` to match `librosa` for all the possible combination of parameters. (at least I did not see a test torch check the parity of `stft`)."}, "41534": {"question": "quantization.fuse_modules fails with Conv1d and BatchNorm1d", "answer": "Here (https://github.com/pytorch/pytorch/blob/0c77bd7c0bbd4d6e50a5f3ce7b4debbee85d7963/torch/quantization/fuse_modules.py#L106) is a list of modules which can be fused.  LeakyRELU fusion does not work because we don't have a fusion implemented for it."}, "41536": {"question": "weird bug of torchscript: it thinks my python bool is a tensor but it's not", "answer": "I think this is working as intended (other than the confusing error expr range issue).\r\n\r\nEMPTY_FLOAT is a mutable tensor outside of the scripted function, TorchScript wouldn't know how to deal with it. Think about the case where you save this model on disk and another program loads it back. This second program doesn't have access to the EMPTY_FLOAT object in the first program. Therefore TorchScript is being conservative here.\r\n\r\nWith that being said, I assume your intention is actually to use the value of EMPTY_TENSOR as constant. In that case, I would suggest adding EMPTY_TENSOR as another functional argument.\r\n\r\n@suo, any better suggestions?"}, "41592": {"question": "Add SpectralOps CPU implementation for ARM/PowerPC processors (where MKL is not available)", "answer": "Thanks @malfet. I was able to compute fft on ARM by using CUDA device on waveform:\r\n```\r\nimport torchaudio\r\nimport torch\r\n\r\nwaveform, sample_rate = torchaudio.load('test.wav')\r\nwaveform = waveform.to(\"cuda:0\")\r\n\r\nspectrogram = torchaudio.transforms.Spectrogram(sample_rate).to(\"cuda:0\")(waveform)\r\n```"}, "41593": {"question": "cudnn8 version check fails", "answer": "These lines has already been changed to\r\n```cmake\r\n  # Get cuDNN version\r\n  if(EXISTS ${CUDNN_INCLUDE_PATH}/cudnn_version.h)\r\n    file(READ ${CUDNN_INCLUDE_PATH}/cudnn_version.h CUDNN_HEADER_CONTENTS)\r\n  else()\r\n    file(READ ${CUDNN_INCLUDE_PATH}/cudnn.h CUDNN_HEADER_CONTENTS)\r\n  endif()\r\n```\r\nplease update to latest master branch"}, "41598": {"question": "AssertionError: Torch not compiled with CUDA enabled - DETECTRON CPU/LINUX TRAINING ERROR ", "answer": "@svideloc have you reported this to detectron2?  This looks like a detectron2 issue to me, not a pytorch issue.\r\n\r\nIf you find that that is not the case, feel free to reopen this issue."}, "41614": {"question": "torch.distributed and RPC cannot both be initialized with the same host:port pair.", "answer": "@froody and @blefaudeux brought up a good point that, it might be too much hurdle for users to initialize twice (c10d and RPC), it might be easier to use if we support initializing an RPC gang using an initialized ProcessGroup. \r\n\r\nI recall we discussed the possibility to have a new `init_rpc` API that takes a Store instead of `rank`, `world_size`, `init_method` etc. Can this be one step further for solving this issue? E.g., we let `ProcessGroup` expose an API to return its store, and then init RPC using that Store. \r\n\r\nThere are different options to implement this behavior.\r\n\r\nOption 1: As the c10d gang is stable with fixed ranks, the derived RPC gang can also stay that way and share the same rank/id with the ProcessGroup instance. \r\nOption 2: Let the RPC stay decoupled from the c10d ProcessGroup, and still allows dynamic join/leave. In this case, we cannot match the rank/id between ProcessGroup and RPC agents any more. \r\n\r\n"}, "41633": {"question": "whitelist keyword to quantization.prepare is implemented incorrectly", "answer": "the default argument is changed to None in https://github.com/pytorch/pytorch/pull/42576"}, "41651": {"question": "Feature request: torch.isclose should set default atol and rtol based on the dtype of the tensors it's given", "answer": "Basically if you do any arithmetic on `float32`, it will rarely hit that `1e-8` atol tolerance, so the defaults are almost useless for the default type (unlike `np.isclose` where the default type is usually float64). "}, "41656": {"question": "torch.nn.functional.grid_sample segfaults on large inputs", "answer": "@erdmann Thank you for your perspective, it's fascinating! \r\nIt's true that as resolution goes up, images become bigger. But in practical terms \r\n1) GPU memory is usually not big enough to hold multiple large tensors \r\n2) Even if grid_sample was fixed to properly handle 64-bit indices on the GPU, typically you'd want to call a convolution next, and it won't work because of cudnn limitation. \r\nThat said, binary size increase from fixing grid_sample will probably be pretty small, so if we can, we should just do it. "}, "41664": {"question": "CUDA not found when using latest pre-built version (Libtorch 1.5.1  - CUDA 10.1 - Cudnn7.6.4 - VS2017)", "answer": "Please add the linker option `-INCLUDE:?warp_size@cuda@at@@YAHXZ`."}, "41665": {"question": "Parameter `timeout` in torch.distributed.init_process_group cannot work ", "answer": "@worsecoder the timeout does not include initial socket connection establishment. After initializing process group, the time out will be applied to NCCL collective APIs such as all reduce as etc. \r\n\r\nFor your need, do you want to try to use torchelastic for failure handling?"}, "41696": {"question": "Provide a convenient way for the user to reset the grad", "answer": "Having both zero_grad() and reset_grad() might be kind of confusing to the end users. \r\n\r\nAre zero-ing all gradients always equivalent to setting all gradients to None? Wonder if there's any case where zero_grad() is preferred over reset_grad().\r\n\r\nIf the two are not equivalent, and have pros and cons, we probably should mention those pros and cons in the documentations to articulate the difference in order to prevent confusion. We should probably have some default recommendation (e.g. do we think reset_grad() will be better in most cases, which looks like the case, because most references I found are just calling .backward() right after calling zero_grad())."}, "41706": {"question": "Replace blacklist/whitelist in caffe2/opt/tvm_transformer.cc", "answer": "Simply replacing the names does not help. Assistance is needed."}, "824": {"question": "Behavior of torch.mean (and std) compared to numpy mean (std)", "answer": "A WIP implementation can be found in https://github.com/pytorch/pytorch/pull/2116\r\nBut it needs someone to complete it."}, "825": {"question": "Easy way of creating your own custom cuda kernels", "answer": "we also provide examples of easily interfacing with CUDA code in https://github.com/pytorch/extension-ffi where you dont have to write additional bindings "}, "827": {"question": "HalfTensor Training Needs non-Stateless Method in F.Linear", "answer": "If it has an mm method then we should enable `torch.mm` for it too. It should be a one line change in cwrap."}, "841": {"question": "Just one cpu core in use, until I use numpy...", "answer": "Sometimes mkl caps the number of threads below your system's max. See mkl's cap via `mkl.get_max_threads()`. To increase the cap to your actual max, use `export MKL_DYNAMIC=FALSE` in bash before running Python. Find your actual max via `echo $(nproc)` in bash.\r\n\r\nIntel's documentation:\r\nhttps://software.intel.com/en-us/mkl-linux-developer-guide-mkl-dynamic\r\n\r\nI came across this solution in this issue:\r\nhttps://github.com/ContinuumIO/mkl-service/issues/2\r\n"}, "853": {"question": "Support Caffe2 export/pure C(++) inference mode", "answer": "If I were to suggest something, I think I'd convert all the weights to numpy and dump them in a well established format like HDF5. These should be easily readable from C.\r\n\r\nOur main idea is to add a model exporter, that could dump it to a Caffe2 graph, as that framework is very well optimized for production usage. And even though we don't plan to put a great amount of work on packaging production-ready models trained in PyTorch right now, there are some projects in the community that are aiming to do that, and we're going to provide them with support. You can find them in the forum link that @fmassa posted."}, "871": {"question": "GPU torch.multinomial produces an out-of-bounds index", "answer": "In case this is helpful to anyone, a possible temporary workaround is to use\r\n```\r\n_, sample = torch.max(log_dist - torch.log(-torch.log(torch.rand(*log_dist.size()).cuda())), 1)\r\n```\r\nwhere `log_dist` is batchwise log probabilities (e.g. output of `F.log_softmax`)."}, "872": {"question": "Flexible state dict loading for model (or optimizer)", "answer": "If you have partial state_dict, which is missing some keys you can do the following:\r\n\r\n```python\r\nstate = model.state_dict()\r\nstate.update(partial)\r\nmodel.load_state_dict(state)\r\n```"}, "891": {"question": "Support int16 numpy conversions", "answer": "The error message is quite self explanatory. PyTorch doesn't support `int8` tensors at the moment."}, "896": {"question": "The command '/bin/sh -c' returned a non-zero code: 2 during docker image", "answer": "As you can see [here](https://github.com/pytorch/pytorch/blob/f2d72ba10fabe6f78e67246031c8b1da48e7ddf1/Dockerfile#L13-L19) the WHOLE command following `RUN` has to be a single line. You should add `\\` when necessary to avoid problem.\r\nYour sample:\r\n```\r\nRUN\r\nchmod +x ~/miniconda.sh && \r\n~/miniconda.sh -b -p /opt/conda && \\\r\nrm ~/miniconda.sh && \r\n/opt/conda/bin/conda install conda-build && \r\n/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl&& \r\n/opt/conda/bin/conda clean -ya\r\n```\r\nshould be\r\n```\r\nRUN chmod +x ~/miniconda.sh && \\\r\n~/miniconda.sh -b -p /opt/conda && \\\r\nrm ~/miniconda.sh && \\\r\n/opt/conda/bin/conda install conda-build && \\\r\n/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl && \\\r\n/opt/conda/bin/conda clean -ya\r\n```"}, "897": {"question": "old lua torch model to pytorch, nn.JoinTable(1,3) error", "answer": "Because the forward looks like that `def forward(self, input):` so it expects two arguments, but it gets self, Z and Y from you. `nn.Sequential` only accepts a single input."}, "909": {"question": "torch ModuleNotFoundError in ipython ", "answer": "1. try to uninstall your installed pytorch.\r\n2. conda install -c peterjc123 pytorch-cpu\r\n3. on your conda type python\r\n4. import torch "}, "910": {"question": "Device memory not released", "answer": "I've seen this before. If the main process segfaults, then the background data loading processes might still be running and holding the cuda context.\r\n\r\nRun:\r\n`killall python` (or `killall python3`)"}, "914": {"question": "Manually unrolling cuDNN RNN OOM", "answer": "First one is not using cudnn, second one is. Both are unrolled in the same way. You can switch between them by commenting/uncommenting the following line in the script:\r\n```\r\ntorch.backends.cudnn.enabled=False\r\n```"}, "915": {"question": "padding for nn.AvgPool3d?", "answer": "there isn't a particular reason, we haven't implemented it yet in our C backend."}, "4499": {"question": "Compiling error of gloo when installing pytorch from source", "answer": "I just added `#define SPEED_UNKNOWN -1` to the file `torch/lib/gloo/gloo/common/linux.cc`.  It appears that this is suppose to be defined by one of the headers from the linux kernel, but I am guessing isn't defined for older versions."}, "4507": {"question": "RuntimeError: DataLoader worker (pid 23616) is killed by signal: Terminated.", "answer": "I run into the same problem as @shirishr. It was simply because of not enough memory, you may add more memory / swap space to solve the problem. "}, "4518": {"question": "from torch._C import *  (ImportError: DLL load failed: The specified module could not be found.", "answer": "I had the same issue and it was caused by the directory torch which is generated in the same directory by compiling the source. The solution for me was simply changing the directory before open python.\r\n  "}, "4534": {"question": "nn.BatchNorm1d fails with batch size 1 on the new PyTorch 0.3", "answer": "Like the error message says, you can't use feature-wise batch normalization if you only have 1 element per-feature.\r\n\r\nhttps://arxiv.org/abs/1502.03167"}, "4546": {"question": "NVIDIA driver too old error", "answer": "I solved it by downgrading the pytorch. That is less cumbersome compared to updating the driver. \r\nI followed `https://pytorch.org/get-started/previous-versions/` for compatible pytorch version with cuda toolkit"}, "4570": {"question": "Feature request: torch.bincount", "answer": "@sunshineatnoon, how about:\r\n```python\r\n>>> import torch\r\n>>> help(torch.bincount)\r\n```\r\n```rst\r\nHelp on built-in function bincount:\r\n\r\nbincount(...)\r\n    bincount(self, weights=None, minlength=0) -> Tensor\r\n\r\n    Count the frequency of each value in an array of non-negative ints.\r\n\r\n    The number of bins (size 1) is one larger than the largest value in\r\n    :attr:`input`. If :attr:`minlength` is specified, the number of bins is at least\r\n    :attr:`minlength`. If ``n`` is the value at position ``i``,\r\n    :math:`out[n] += weights[i]` if :attr:`weights` is specified else\r\n    :math:`out[n] += 1`.\r\n\r\n    Arguments:\r\n        input (Tensor): 1-d int tensor\r\n        weights (Tensor): optional, weight for each value in the input tensor.\r\n            Should be of same size as input tensor.\r\n        minlength (int): optional, min number of bins. Should be non-negative.\r\n\r\n    Shape:\r\n        output (Tensor): ``Size([max(input) + 1])``\r\n\r\n    Example::\r\n\r\n        >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\r\n        >>> weights = torch.linspace(0, 1, steps=5)\r\n        >>> input, weights\r\n        (tensor([4, 3, 6, 3, 4]),\r\n         tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\r\n\r\n        >>> torch.bincount(input)\r\n        tensor([0, 0, 0, 2, 2, 0, 1])\r\n\r\n        >>> input.bincount(weights)\r\n        tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\r\n```"}, "4578": {"question": "[feature request] Weight norm option for RNN cells", "answer": "You can already achieve it now. For instance,\r\n```\r\n>>> rnn = nn.RNN(10,10,2)  # build an RNN\r\n>>> nn.utils.weight_norm(rnn, 'weight_hh_l0')  # apply weight_norm to any particular weight you want\r\n```"}, "4582": {"question": "[Feature request] PackedSequence with length = 0", "answer": "I don't think that's a good idea. If you know it can happen in your use case just filter out the empty sequences before you call this function. It would make it more convenient for you, but may silently accept inputs that are incorrect for other uses."}, "4596": {"question": "Cannot install pytorch cuda 8.0 using conda", "answer": "do you already have the cuda90 feature installed, for some reason...\r\n\r\nTry first doing:\r\n\r\n`conda uninstall cuda90`"}, "4605": {"question": "Warning on infinite acos gradients?", "answer": "A lot of our ops work this way, and adding these checks would cause massive slowdowns. Thanks for the suggestion, but we can't afford that."}, "4619": {"question": "Autogradpp issue masterthread", "answer": "Regarding `tensor.max()` I think a better way would be to auto-generate \"named-tuples\" in C++ (i.e. you can still do `std::get<0>(tensor.max())` OR `tensor.max().values` (we'd generate a struct with two fields that also supports the `get` interface)"}, "4620": {"question": "`torch.normal` accepts Variables but does not propagate gradients", "answer": "if you are on master, you can do `x = torch.distributions.Normal(mu, sigma).rsample()`"}, "57569": {"question": "`torch.jit.freeze`'d models cannot be moved to GPU with `.to()`", "answer": "@Linux-cpp-lisp this is currently expected behavior. We need to be able to inline the attributes as constants in order to do anything useful in optimizing them. There is also nothing preventing the user from having device-specific logic we also bake in.\r\n\r\n```\r\n    def forward(self, x):\r\n       if self.twos.device.is_cuda():\r\n              ....\r\n```\r\nModels might also contain some CPU & some GPU compute. However, as you've shown, there are many models where it is completely valid to remap devices after freezing. \r\n\r\nCan I ask what the specific use case is ?\r\n\r\n\r\n"}, "57649": {"question": "gradgradcheck fails if the function does not depend on the input", "answer": "Still fails.\r\n\r\n    Traceback (most recent call last):\r\n      File \"<stdin>\", line 1, in <module>\r\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1212, in gradgradcheck\r\n        return gradcheck(\r\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1088, in gradcheck\r\n        return _gradcheck_helper(**args)\r\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1096, in _gradcheck_helper\r\n        func_out = func(*tupled_inputs)\r\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1209, in new_func\r\n        grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True)\r\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 226, in grad\r\n        return Variable._execution_engine.run_backward(\r\n    RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\r\n\r\nThe problem is this line:\r\nhttps://github.com/pytorch/pytorch/blob/807bea1c4e6fdf896570f1fac83897cf42231f49/torch/autograd/gradcheck.py#L1209\r\n\r\nProbably the fix is as simple as put `allow_unused=True`"}, "57681": {"question": "With & without -O2 compilation optimization level, AVX512 Complex multiplication & division results aren't equal", "answer": "@quickwritereader, this is so embarrassing!!! I had wrongly assumed that `CPU_CAPABILITY_DEFAULT` is always defined, unless a user uses the environment variable `ATEN_CPU_CAPABILITY` to set it to a particular value! That's why I hadn't bothered to change that if clause because it seemed redundant to do so.\r\n\r\nSo, the issue was just that the testing for AVX512 was being done against the default implementation? \ud83e\udd23 \r\nThanks to you, all tests pass now! \ud83d\ude04 "}, "57744": {"question": "Linking statically with CUPTI using gold linker disrupts exception handling", "answer": "The CMake file change at https://github.com/pytorch/pytorch/commit/c71459602785bcc9f0f93a880c642ff61299d3d5#diff-12e8125164bbfc7556b1781a8ed516e333cc0bf058acb7197f7415be44606c72R1859-R1863 now actually honors `USE_CUPTI_SO`. This was previously not the case which meant building PyTorch \"correctly\" i.e. with dynamic cupti, was impossible and the issue unavoidable (we patched that for our builds)\r\n\r\nTo me linking to static cupti is the bug which is now fixed: Define USE_CUPTI_SO=1 and all works (testing this right now)\r\n\r\nBut yes the (likely) faulty cupti static library is not fixed by this but I think this is rather a cupti bug than a pytorch one. Not sure here, so please reopen if you still want to track this bug here.\r\nJust from my side I'm happy if USE_CUPTI_SO is now working."}, "57747": {"question": "Retiring ONNX Optimizer", "answer": "Discussed this issue during triage review: let's disable optimizer support as it is no longer supported."}, "57865": {"question": "Cannot pass script remote module object to over the RPC", "answer": "@SciPioneer I wonder if we can employ a method similar to https://github.com/pytorch/pytorch/blob/master/torch/distributed/rpc/internal.py#L110-L112, although I wonder why it isn't already supported out of the box. I think `RecursiveScriptModule` is an instance of `torch.jit.ScriptModule` so not sure why this codepath isn't hit. "}, "57938": {"question": "torch.cat should not do type promotion when one input is empty tensor", "answer": "NumPy type promotes in this case and I don't think it makes sense to complicate the type promotion rules even further:\r\n```\r\n>>> np.concatenate([np.zeros(5, dtype=np.long), np.array([])]).dtype\r\ndtype('float64')\r\n```"}, "57956": {"question": "A bug on the document interpreter in `torch.autograd.profiler` help page", "answer": "Fixed on master https://pytorch.org/docs/master/generated/torch.autograd.profiler.profile.key_averages.html?highlight=key_averages#torch.autograd.profiler.profile.key_averages"}, "57982": {"question": "`nan_to_num` produces incorrect output for `BFloat16` on CUDA", "answer": "Ok, so it looks like `nan_to_num` is actually doing the correct thing, and it's the compare with reference that goes wrong. That's good to know. "}, "38958": {"question": "torch.cat fails with torch.jit.script and torch.cuda.amp.autocast", "answer": "At a wild guess: this patch should fix it:\r\n\r\n```\r\ndiff --git a/aten/src/ATen/autocast_mode.cpp b/aten/src/ATen/autocast_mode.cpp\r\nindex 7d85211e9c..7809b7d323 100644\r\n--- a/aten/src/ATen/autocast_mode.cpp\r\n+++ b/aten/src/ATen/autocast_mode.cpp\r\n@@ -487,9 +487,9 @@ TORCH_LIBRARY_IMPL(aten, Autocast, m) {\r\n   KERNEL_UNBOXED_ONLY(ADD_NS(tensordot), \"tensordot\", Tensor (const Tensor &, const Tensor &, IntArrayRef, IntArrayRef), promote)\r\n   KERNEL_UNBOXED_ONLY(ADD_NS(dot), \"dot\", Tensor (const Tensor &, const Tensor &), promote)\r\n   KERNEL(ADD_NS(equal), \"equal\", bool (const Tensor &, const Tensor &), promote)\r\n-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), \"cat\", Tensor (TensorList, int64_t), promote)\r\n-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), \"cat.names\", Tensor (TensorList, Dimname), promote)\r\n-  KERNEL_UNBOXED_ONLY(ADD_NS(_cat), \"_cat\", Tensor (TensorList, int64_t), promote)\r\n+  KERNEL(ADD_NS(cat), \"cat\", Tensor (TensorList, int64_t), promote)\r\n+  KERNEL(ADD_NS(cat), \"cat.names\", Tensor (TensorList, Dimname), promote)\r\n+  KERNEL(ADD_NS(_cat), \"_cat\", Tensor (TensorList, int64_t), promote)\r\n   KERNEL_UNBOXED_ONLY(ADD_NS(stack), \"stack\", Tensor (TensorList, int64_t), promote)\r\n \r\n   m.impl_UNBOXED(\"binary_cross_entropy\", &at::autocast::binary_cross_entropy_banned);\r\n```\r\n\r\n@smessmer not sure if we should just rush \"get rid of unboxed only\" or start adding some checks to reject incorrect invocations of unboxed only."}, "38962": {"question": "[JIT] lack of type support in tensor indexing.", "answer": "Hi @chenbohua3, thanks for the detailed issue and succinct repro! This was recently added in https://github.com/pytorch/pytorch/pull/38378 and works on master, so I'm closing. "}, "38991": {"question": "One PyTorch Upsample op balloons into over 20 ONNX operations", "answer": "This is expected since by default, interpolate recomputes the given scale_factor using input size. If you try torch.nn.Functional with recompute_scale_factor = False, you'll see a single Resize node in the graph.\r\nThe default behavior of interpolate is going to change to set recompute_scale_factor = False by default,  as part of the PR: https://github.com/pytorch/pytorch/pull/38362"}, "39053": {"question": "Unrecognized attribute: min for operator Clip", "answer": "seems fixed and can't repro with `pytorch 1.6`"}, "39088": {"question": "``ToTensor()`` Exception for ``num_workers`` in ``DataLoader`` when ``torch.set_default_tensor_type(torch.cuda.FloatTensor)``", "answer": "the root cause is that `fork` inherits the current process state, including default tensor type, while `spawn` doesn't. this is a fundamental difference between the two start methods, and not really pytorch/torchvision specific. "}, "39129": {"question": "`verbose` unused in `torch.backends.cudnn`", "answer": "I can fix it by removing this parameter."}, "39141": {"question": "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation?", "answer": "Hi,\r\n\r\nThis happens because the `opt_D.step()` modifies the parameters of your discriminator inplace. But these parameters are required to compute the gradient for the generator. Hence the error.\r\nWe fixed the inplace detection for the optimizers in 1.5, this is why it works in 1.4.\r\n\r\nYou should re-organize your code to only do the `steps()` after all the gradients have been computed or make sure you don't modify parameters that are required.\r\nSomething like that should work.\r\n```\r\nfor step in range(10000):\r\n    artist_paintings = artist_works()  # real painting from artist\r\n    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas\r\n    G_paintings = G(G_ideas)  # fake painting from G (random ideas)\r\n\r\n    prob_artist1 = D(G_paintings)  # G tries to fool D\r\n\r\n    G_loss = torch.mean(torch.log(1. - prob_artist1))\r\n    opt_G.zero_grad()\r\n    G_loss.backward()\r\n    opt_G.step()\r\n\r\n    prob_artist0 = D(artist_paintings)  # D try to increase this prob\r\n    # detach here to make sure we don't backprop in G that was already changed.\r\n    prob_artist1 = D(G_paintings.detach())  # D try to reduce this prob\r\n\r\n    D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))\r\n    opt_D.zero_grad()\r\n    D_loss.backward(retain_graph=True)  # reusing computational graph\r\n    opt_D.step()\r\n```\r\n\r\nIn the future, I would recommend to ask these questions on the forum: https://discuss.pytorch.org/\r\nWe keep github issues for bug and features only. And more people look at the forum so you will get a faster answer."}, "39165": {"question": "[JIT] Expected integer literal for index:", "answer": "@yangsenius changing the forward to look like this:\r\ndef forward(self, x: List[torch.Tensor]):\r\nfixed that error for me. Now, I am stuck on this:\r\n```\r\nExpected a default value of type Tensor on parameter \"mask\".:\r\n  File \"/usr/local/lib/python3.8/dist-packages/torchvision/ops/deform_conv.py\", line 152\r\n    def forward(self, input: Tensor, offset: Tensor, mask: Tensor = None) -> Tensor:\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n        \"\"\"\r\n        ~~~\r\n        Args:\r\n        ~~~~~\r\n            input (Tensor[batch_size, in_channels, in_height, in_width]): input tensor\r\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n            offset (Tensor[batch_size, 2 * offset_groups * kernel_height * kernel_width,\r\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                out_height, out_width]): offsets to be applied for each position in the\r\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                convolution kernel.\r\n                ~~~~~~~~~~~~~~~~~~~\r\n            mask (Tensor[batch_size, offset_groups * kernel_height * kernel_width,\r\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                out_height, out_width]): masks to be applied for each position in the\r\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                convolution kernel.\r\n                ~~~~~~~~~~~~~~~~~~~\r\n        \"\"\"\r\n        ~~~\r\n        return deform_conv2d(input, offset, self.weight, self.bias, stride=self.stride,\r\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                             padding=self.padding, dilation=self.dilation, mask=mask)\r\n                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n```\r\n\r\nNote: I didn't need to change anything to trace but I got size issues.\r\nThis is all to fix scripting."}, "39235": {"question": "[FR] Add space as delimiter in TORCH_CHECK and other macros", "answer": "Agree with @ShawnZhong, in most cases error strings already contain the necessary spaces, those that don't can be fixed at the call site, it does not make sense to silently insert spaces in the TORCH_CHECK macro itself. "}, "20380": {"question": "torch.distributed support on MacOS is missing", "answer": "For Windows support, please check this RFC (#42095)\r\n\r\nHey @neggert, yes PyTorch + Gloo works on MacOS, but you will need to compile from source using the following steps:\r\n\r\n0. follow the readme in https://github.com/pytorch/pytorch to setup conda and dependencies\r\n1. then conda install libuv and pkg-config\r\n2. then run `time env MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ BUILD_CAFFE2_OPS=0 USE_CUDA=0 USE_MKLDNN=0 USE_DISTRIBUTED=1 python setup.py develop`"}, "20403": {"question": "Installing PyTorch 1.1 into cloned conda env that contained PyTorch 1.0 gives \"Getting \"module 'torch._C' has no attribute 'BoolStorageBase'\" with PyTorch 1.1\"", "answer": "This seems to be similar issue to https://github.com/pytorch/pytorch/issues/12031 . It looks like you must do `conda uninstall pytorch -y` before `conda install pytorch` or you end up in unrecoverable bad state"}, "20408": {"question": "DLL error on Windows 10", "answer": "Let me conclude it for you:\r\n1. Install WDK https://docs.microsoft.com/en-us/windows-hardware/drivers/download-the-wdk\r\n2. Execute these commands:\r\n```powershell\r\ngflags /i ${YOUR EXECUTABLE} +sls # e.g. python.exe # Turn on loader snaps\r\ncdb ${YOUR COMMAND} # e.g. python -c \"import torch\"\r\n# Keep typing in `g` and `Enter` until the end. Use `q` and `Enter` to exit.\r\ngflags /i ${YOUR EXECUTABLE} -sls # e.g. python.exe # Turn off loader snaps\r\n```"}, "20411": {"question": "Non blocking tensor copy to GPU not working from torch 1.0", "answer": "You are not measuring the effect of non_blocking because you are explicitly synchronizing inside your measurement.  You end up measuring some complicated interaction between copy overhead and copy time.\r\n\r\nnon_blocking=True doesn't make the copy faster. It just allows the copy_ call to return before the copy is completed. If you call `torch.cuda.synchronize()` immediately after a copy you've added back the synchronization you just tried to remove.\r\n\r\nAnyways, `non_blocking=True` works in PyTorch 1.0 (from CPU->CUDA). You can see it by adding a GPU delay before your copy:\r\n\r\n```python\r\nimport time\r\nimport torch\r\n\r\nDELAY = 100000000 \r\nx = torch.randn((1024, 1024), pin_memory=True)\r\n\r\ntorch.cuda.synchronize()\r\nstart = time.time()\r\ntorch.cuda._sleep(DELAY)\r\nx.cuda(non_blocking=True)\r\nend = time.time()\r\n\r\nprint('non_blocking=True', (end - start)*1000.)  # ~7 ms on my GPU\r\n\r\ntorch.cuda.synchronize()\r\nstart = time.time()\r\ntorch.cuda._sleep(DELAY)\r\nx.cuda(non_blocking=False)\r\nend = time.time()\r\n\r\n\r\nprint('non_blocking=False', (end - start)*1000.)  # ~77 ms on my GPU\r\n```"}, "20421": {"question": "The result of  gloo all_gather error", "answer": "@qijianan777 confirm that I can reproduce, and this is indeed a bug in `ProcessGroupGloo`. More specifically, when you do `chunk` on `dim=0`, the result tensors share the same underlying storage and are contiguous, but with different offset. So, when we do [`flat`](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L842) the tensors here, it will do nothing. Later, when retrieving [data pointer](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L110) of the tensors, it will return the same ptr value (this is the bug). As a result, both processes are reading the first 2 elements. \r\n\r\nThanks for reporting, I will add a fix for it. "}, "20423": {"question": "Error downloading MNIST dataset", "answer": "I solved my own problem too...\r\nthere was a mismatch of torchvision I installed using pip...it was 0.2.2 with torch 1.8.0...\r\nI then built and installed torchvision 0.9.0 from source and it works correctly"}, "20449": {"question": "\"derivative for _thnn_fused_lstm_cell_backward is not implemented\" while using GPU", "answer": "Apparently, double backward on gpu has been broken for some time. What's happening is in the forward _thnn_fused_lstm_cell is called https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/RNN.cpp#L259-L265, and it has its derivative specified in derivatives.yaml, it's _thnn_fused_lstm_cell_backward. _thnn_fused_lstm_cell_backward does not have derivative, and throws an error. \r\nThe easiest fix would be to determine that there would be double backward and not use fused cell in forward, but I don't think there's a way to tell in forward if there will be double backward or not. So it looks like it's necessary to define differentiable backward functions for rnn cells, similarly to how it's done for weight norm, and fall back to them if `GradMode::is_enabled()`."}, "20464": {"question": "Implementing GELU activation", "answer": "This is fairly simple (see below for what would need to be added to `functional.py`), but seeing as it is common enough now could be worthwhile. If you're able to submit a PR with this (including a Module that wraps this and tests as necessary) then that would be useful.\r\n\r\n```py\r\ndef gelu(x):\r\n  return 0.5 * x * (1 + torch.tanh(math.sqrt(math.pi / 2) * (x + 0.044715 * x ** 3)))\r\n```"}, "20477": {"question": "tensorboard not updating", "answer": "My current workaround:\r\n\r\nwhile true; do\r\n        timeout -sHUP 1m tensorboard --logdir=runs;\r\ndone"}, "20489": {"question": "RuntimeError: Creating MTGP constants failed", "answer": "This is now solved because curandStateMtgp constants are never created as a result of https://github.com/pytorch/pytorch/pull/20886. Closing this issue."}, "20494": {"question": "Install only a specific version via pip", "answer": "this issue can now be closed, because we use local version identifiers. \r\nFor example, CPU-only version is:\r\n```\r\npip3 install torch==1.2.0+cpu torchvision==0.4.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\r\n```\r\n\r\nSo, in `requirements.txt`, having a constraint such as `1.2.0+cpu` should be good.\r\n\r\nWe dont ship the variant wheels via PyPI though, and have no plans to do so (PyPI doesn't allow local version identifiers to be uploaded live)"}, "20516": {"question": "MobileNetV2 export to ONNX fails", "answer": "look into the mobilenet.py in torchvision, change the forward function:\r\n    def forward(self, x):\r\n        x = self.features(x)\r\n        # x = x.mean([2, 3])   # this line will result in bug\r\n        x = x.mean(3).mean(2)\r\n        x = self.classifier(x)\r\n        return x\r\n\r\n"}, "20522": {"question": "nn.CTCLoss RuntimeError on GPU", "answer": "Perfect! I compile torchvision from source and it works well. Thanks you @t-vi "}, "20527": {"question": "StepLR, MultiStepLR, ExponentialLR and CosineAnnealingLR scheduler wrong lr value", "answer": "Thanks for the example. As mentioned in #26423, `get_lr` should be replaced by `get_last_lr`, see below.\r\n\r\n```python\r\nimport torch\r\nprint(\"pytorch version\",torch.__version__) \r\nimport torch.nn as nn\r\nmodel = nn.Linear(1, 1) # 'Net' is a simple MLP\r\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\r\nschedular = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [4,7], gamma=0.1)\r\n\r\nprint('Initial LR : {0:.8f}'.format(schedular.get_lr()[0]))\r\nfor e in range(8):\r\n  optimizer.step()\r\n  schedular.step()\r\n  print('Epoch {0}, LR: {1:.8f}, opt LR {2:.8f}'.format(e, schedular.get_last_lr()[0],\r\n          optimizer.param_groups[0]['lr']))\r\n```\r\n\r\nSince #26423 has been merged, I will close this issue. Please feel free to re-open if the issue persists."}, "20532": {"question": "Couple hundred MB are taken just by initializing cuda", "answer": "> why it's need so many memory on the GPU?\r\n\r\nIt's used by the CUDA driver. I think it's used to store the CUDA kernels and PyTorch has a lot of CUDA kernels.\r\n\r\n> can i release it?\r\n\r\nNo (other than quitting the process)\r\n\r\nThe standard practice is to **not** access the same GPU from multiple processes. i.e. use one process per GPU. "}, "20546": {"question": "Saving state_dicts should capture shared state", "answer": "I don't see anything we should change in PyTorch. `load_state_dict` doesn't change the structure of the Module and **should not** change the structure.\r\n\r\nIf you create your first version with:\r\n\r\n```\r\na = A()\r\nb = B(a=a)\r\nc = C(a=a, b=b)\r\n```\r\n\r\nThen your second version of C needs to be created with the same structure:\r\n\r\n```\r\na2 = A()\r\nb2 = B(a=a2)\r\nc2 = C(a=a2, b=b2)\r\n```\r\n\r\nIf you instead do:\r\n\r\n```\r\nc2 = C()\r\n```\r\n\r\nYou've create a different structure and shouldn't expect it to work the same as your first incarnation.\r\n"}, "8930": {"question": "Multiple GPU, Batch Normalization - RuntimeError: the derivative for 'running_mean' is not implemented", "answer": "Low chance this applies to your situation, but I got this error when I used a parameter with `requires_grad` set to `True` as the `running_mean` in the `nn.functional.batch_norm` function. Hopefully this helps someone else who finds themselves in the same situation."}, "8938": {"question": "[Feature request] Get cell state from the last layer for each t when using LSTM", "answer": "considering that using custom RNNs is the only way this is possible (because nn.LSTM's backing CUDNN LSTM does not allow extraction of intermediate states), I am closing this request with @zou3519 's answer above as the way forward."}, "8974": {"question": "Compiling pytorch on MacOSX 10.13.5 with support for CUDA GeForceGT 750M", "answer": "https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html\r\n\r\nYou need to download Command_Line_Tools_macOS_10.13_for_Xcode_9.2.dmg from https://developer.apple.com/download/more/ and install then execute `sudo xcode-select --switch /Library/Developer/CommandLineTools`"}, "8976": {"question": "RuntimeError: DataLoader worker is killed by signal: Killed.", "answer": "It is very likely there was a out-of-memory(OOM) in your system so the data worker got killed by the system. Try to use `dmesg -T` to see the detailed reason."}, "8988": {"question": "[Feature Request] tensordot", "answer": "I'd do tensordot based on the (private) sumproduct_pair function used by einsum. In fact, if we want this, I'd be glad to send a PR.\r\nMedium-term, I'd like to incorporate the optimizations into einsum itself. (Numpy, did, too.)"}, "8989": {"question": "[proposal] out= doesn't resize storage", "answer": "We can create memory pool and enforce using it for memory operations. This will allows us to control who can resize and who can't"}, "9067": {"question": "[jit] gen_jit_dispatch generates duplicate \"descriptor\"s", "answer": "We no longer use descriptors, so this is fixed."}, "9069": {"question": "[jit] cannot trace tensor factory methods", "answer": "This is kind of expected. There are two main problems at play here:\r\n\r\n1. RNG calls are not functional - they mutate the state of the global PRNG, and therefore we probably shouldn't rearrange them. This however requires us to add the \"world token\" to inhibit optimizations.\r\n2. We never trace constructors. If we want to do this, we probably need to make the tracing a thread-local property, instead of auto-detecting it as a per-Variable property."}, "9071": {"question": "ImportError: DLL load failed: The specified module could not be found", "answer": "Downgrade sqlite3 in Anaconda is the solution, as it was the last version I couldn't upgrade it, and no option for install or uninstall is available. after the downgrade it worked fine"}, "22251": {"question": "PackedSequence's sorted_indices is not put on cuda when to('cuda') is called.", "answer": "Issue is still present on `pytorch==1.3.1`\r\n\r\nTo fix, replace the following:\r\n`X = X.to(device)`\r\nWith this:\r\n`X = X.to(device=device)`\r\nProvided that \"X\" is a packed sequence."}, "22255": {"question": "Schema not found for node torch::eye", "answer": "Are you able to extract a smaller repro?\r\n\r\n`import geoopt` works fine on master, and re-producing [the code](https://github.com/geoopt/geoopt/blob/master/geoopt/linalg/_expm.py#L29) causing the failure runs fine as well\r\n\r\n```python\r\n@torch.jit.script\r\ndef test(A):\r\n    return torch.eye(A.shape[1], dtype=A.dtype, device=A.device)\r\n```"}, "22305": {"question": "element-wise multiplication out of memory", "answer": "You need ~12.6 GB of memory for your example (20 *75 * 1024* 1024 * 4 bytes * 2 = 12.58 GB). The last 2 is because you need space for `g` and `res`. If you don't care about keeping the value of `g` do:\r\n\r\n```python\r\nimport torch\r\n\r\ng = torch.rand([20, 75, 1024, 1024])\r\nw = torch.rand([1024, 1024])\r\ng *= w\r\nres = g\r\n```\r\n\r\nThat will require ~6.3 GB of memory.\r\n\r\nAlso, element-wise multiplication is just `g * w` in general. You can use einsum if you want, but it's not necessary.\r\n\r\nYou're Tensorflow example isn't actually running anything (you'd need a `session.run()` call) so it doesn't require any memory for storing tensor data."}, "22318": {"question": "torch::tensor(std::vector) does not work properly in Microsoft Visual Studio Windows", "answer": "`int64_t` works fine. "}, "22327": {"question": "unable to load istream by using torch::jit::load(istream)", "answer": "@lantiga\r\n I have solved it \uff0cthanks\r\n\r\n1. read pt model to char buffer by using ifstream\r\n\r\n2.change buffer to istream  \r\nstrstreambuf  buf(pModelData,length);\r\nstd::istream in(&buf);\r\n\r\n3\u3001load istream using torch::jit::load\r\n "}, "22329": {"question": "autodiff for user script functions aka torch.jit.script for autograd.Function", "answer": "I think the fix should be to allow users to provide derivatives.yaml for the extensions, and generate the autograd tracking code that Tom had to write manually if derivatives.yaml is provided. That would make extensions differentiable without having to wrap them in the custom autograd function, and improve UX in python eager mode and in C++. It will also reduce the need for custom autograd functions - it looks like in 99% {?) of cases custom autograd functions are needed to make extensions differentiable. "}, "22346": {"question": "Build with MKLDNN broken", "answer": "@mdreammao\r\nPls try to apply the patch in #22910 , then run \"git submodule update --init --recursive\".\r\n\r\nThanks."}, "22353": {"question": "Auto-differentiating torch.cdist is broken on GPU", "answer": "Interesting. After instantiating a new Python kernel, the first time I run the above code, I receive the \"Backward is not reentrant\" error message. Subsequent reruns of the same code (with new and identical `X` tensor and same Python kernel) result in the original error message and the analytical gradients are very different each call.\r\n\r\nNotebook: \r\n[cdist_issue_repeated.ipynb.txt](https://github.com/pytorch/pytorch/files/3341208/cdist_issue_repeated.ipynb.txt)\r\n"}, "22382": {"question": "conversion  to non-scalar type  torch::jit::load(\"model.pt\")", "answer": "It looks like you are using a nightly build. We recently changed the output type of load. This should work:\r\n\r\n```\r\n torch::jit::script::Module module = torch::jit::load(\"model.pt\");\r\n```\r\n\r\nTutorials/documentation are still for the 1.1 release. It will be updated before we release 1.2."}, "22389": {"question": "Dependency issues with torch.utils.tensorboard: \"No module named past\" and \"No module named 'PIL'\"", "answer": "Note that the right fix for the `past.stringbase` is really to stop using the deprecated API (which is not that hard too...)"}, "22395": {"question": "Cuda required when loading a TorchScript with map_location='cpu'", "answer": "It looks like this model came from a trace, is that correct? When exporting a model for use on CPU, we recommend that you switch it to CPU mode _before_ tracing it and exporting it. Otherwise it will hard-code details of using the GPU into the model, as was done here (`torch.to(CONSTANTS.c0, torch.device(\"cuda:0\"), 6, False, False)`). Telling the parameters to map to the CPU will not change traced details like the `to` call which copies the Tensor to the GPU)."}, "17144": {"question": "runtime error (7) : too many resources requested for launch at pytorch/aten/src/THC/THCTensorSort.cu:62", "answer": "The main thing I'm aware of is that the switchover from bitonic sort to thrust as a fallback is different for 64 bit dtypes. These have been lowered for sort but that has not been applied to topk's sorting.\r\nThe natural questions would be\r\n- What is the tensor dtype and size and what are dim and k used ins topk?\r\n- Does the non-sorting topk work for that?\r\n- Does sort work for the non-sorted values?\r\n\r\n(The last two also might offer a workaround until it is fixed in PyTorch).\r\n"}, "17148": {"question": "[Build Error]undefined reference to `__cudaPushCallConfiguration'", "answer": "This probably means that the inconsistent version of NVCC compile and your conda CUDAToolKit package\r\n\r\n```\r\n# Check the NVCC compile version(e.g.)\r\n/usr/cuda-9.2/bin/nvcc --version\r\n# Check the CUDAToolKit version(e.g.)\r\n~/anaconda3/bin/conda list | grep cuda\r\n\r\n# If you need to update your CUDAToolKit\r\n~/anaconda3/bin/conda install -c anaconda cudatoolkit==9.2\r\n```\r\nBoth of them should have the same version. For example, if NVCC==9.2 and CUDAToolKit==9.2, this will be fine while when NVCC==9.2 but CUDAToolKit==9, it fails."}, "17172": {"question": "forward function can not insert pdb.trace", "answer": "Hi @DanlanChen,\r\n\r\nWe do not have deep integration of pdb into our runtime. To debug a ScriptModule, you can do a few things:\r\n\r\n1) Remove the `@torch.jit.script_method` decorator from the method and debug the method as Python code\r\n2) Create a free python function, pass in values you're interested to inspect into that function, and within the function call `pdb`. Example:\r\n\r\n```\r\nimport torch\r\n\r\ndef debug_fn(a, b, c):\r\n    print(a, b, c)\r\n    import pdb; pdb.set_trace()\r\n\r\nclass FooMod(torch.jit.ScriptModule):\r\n    @torch.jit.script_method\r\n    def forward(self, x, y):\r\n        z = x + y\r\n        debug_fn(x, y, z)\r\n        return z\r\n\r\nfm = FooMod()\r\nfm(torch.rand(3), torch.rand(3))\r\n```\r\n\r\nHope this helps with your debugging!"}, "17179": {"question": "zip not allowed in forward function in torch.jit.ScriptModule", "answer": "As a workaround, you can do the following:\r\n\r\n```\r\nmod_list = []\r\nfor mod1, mod2 in zip(module1, module2):\r\n    mod_list.append(nn.Sequential(mod1, mod2))\r\nself.module = nn.ModuleList(mod_list)\r\n```\r\n"}, "17181": {"question": "Cannot build libtorch: SLEEF does not allow in-source builds", "answer": "Sleef doesn't support nesting. I could fix it similarly to https://ceres-solver-review.googlesource.com/c/ceres-solver/+/9780:\r\n\r\n- Using sleef_[SOURCE/BINARY]_DIR (which are defined by CMake when\r\n  project(sleef) is called, in favour of CMAKE_[SOURCE/BINARY]_DIR\r\n  enables sleef to be nested within (and built by) a larger CMake\r\n  project (which also contains other projects).\r\n- CMAKE_[SOURCE/BINARY]_DIR always refers to the top-level source\r\n  and binary directories (i.e. the first encountered), as a result if\r\n  sleef is a nested project within a larger project, these would not\r\n  correctly identify the source/binary directories for sleef (as they\r\n  would refer to the root project in which sleef is nested).\r\n- Using sleef_[SOURCE/BINARY]_DIR should ensure that sleef always uses\r\n  the correct source/binary directories, irrespective of whether sleef\r\n  is nested or not.\r\n\r\nPatch for `aten/src/ATen/CMakeLists.txt`:\r\n------------------------------------------------------\r\n```\r\ndiff --git a/aten/src/ATen/CMakeLists.txt b/aten/src/ATen/CMakeLists.txt\r\nindex 07b3c106b..6133c583a 100644\r\n--- a/aten/src/ATen/CMakeLists.txt\r\n+++ b/aten/src/ATen/CMakeLists.txt\r\n@@ -172,10 +172,13 @@ if(NOT MSVC AND NOT EMSCRIPTEN)\r\n   set(BUILD_DFT OFF CACHE BOOL \"Don't build sleef DFT lib\" FORCE)\r\n   set(BUILD_GNUABI_LIBS OFF CACHE BOOL \"Don't build sleef gnuabi libs\" FORCE)\r\n   set(BUILD_TESTS OFF CACHE BOOL \"Don't build sleef tests\" FORCE)\r\n+  set(sleef_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef\")\r\n+  set(sleef_BINARY_DIR \"${CMAKE_BINARY_DIR}/sleef\")\r\n   add_subdirectory(\"${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef\" ${CMAKE_BINARY_DIR}/sleef)\r\n   set_property(TARGET sleef PROPERTY FOLDER \"dependencies\")\r\n-  list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)\r\n-  link_directories(${CMAKE_BINARY_DIR}/sleef/lib)\r\n+  list(APPEND ATen_THIRD_PARTY_INCLUDE ${sleef_BINARY_DIR}/include)\r\n+  link_directories(${sleef_BINARY_DIR}/lib)\r\n   list(APPEND ATen_CPU_DEPENDENCY_LIBS sleef)\r\n \r\n   set(CMAKE_C_FLAGS_DEBUG ${OLD_CMAKE_C_FLAGS_DEBUG})\r\n```\r\n\r\nPatch for cmake files in sleef:\r\n---------------------------------------\r\n[sleef_patch.txt](https://github.com/pytorch/pytorch/files/2979917/sleef_patch.txt)\r\n"}, "17185": {"question": "Confusing behavior with *= operator with torch.expand", "answer": "This is a duplicate of https://github.com/pytorch/pytorch/issues/957\r\n\r\nI agree this is confusing and has bitten a lot of users in the past, we should discuss about how to circumvent this (maybe by adding a `WRITABLE = False` flag to expanded tensors?)"}, "17203": {"question": "from torch._C import * ImportError: DLL load failed: The specified module could not be found.", "answer": "> the same issue today\r\n> i've tried python 3.6.x and 3.7.1, didn't work.\r\n\r\nfirst, install python3.6.7.(other version such as 3.6.0 and 3.6.8 didn't work)\r\nand then ,` pip install --upgrade numpy  and pip install intel-openmp` \r\nit works for me\r\n"}, "17206": {"question": "multivariate_normal.log_prob is slow", "answer": "@t-vi Thanks for notifying me! I think that it is my fault when trying to use `trtrs` instead of `inverse`. The slowdown seems lie at [this line](https://github.com/pytorch/pytorch/blob/master/torch/distributions/multivariate_normal.py#L43) where we expand `bL` batch shape to match the later part of `bx` batch shape, then do triangle solve with \"expanded\" bL. If we use `inverse`, then after taking the inverse of `bL`, we just simply do `matmul` with `bx` to get the result. So it will be much faster in the above example.\r\n\r\nThe current implementation works well when `x` has additional batch shapes instead. For example, the following version will be fast because we won't expand bL.\r\n```\r\nsigma = torch.eye(8).expand(6, 1, 8, 8).contiguous().requires_grad_()\r\nx_repeat = torch.randn(8000, 6, 1, 8)\r\n```\r\n\r\nI'll sketch out a solution which I have in mind for further discussion."}, "17221": {"question": "Training on 360 sequences, validating on 0 sequences. python3: symbol lookup error: /home/ankitakulkarni/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so: undefined symbol: PySlice_Unpack", "answer": "Solved ...Update the version of python from 3.6.0 to 3.6.2 \r\nThanks!!"}, "17233": {"question": "Windows pytorch CUDA 10 error", "answer": "FWIW, I fixed this by reinstalling torch on a fresh installation of miniconda3 (the one available on their website right now). However, when I do conda update --all, the problem comes back. This means the problem has to do with installing the most recent version of one of these packages: cryptography, pyopenssl, python, setuptools. I've managed to get around this for now by installing torch on a clean new environment."}, "17237": {"question": "Broken indexing?", "answer": "Resolved by upgrading to >= Python 3.6.1"}, "51592": {"question": "\"invalid parameter combination for AltiVec intrinsic\" error with ppc64le, g++ v7.4", "answer": "Ahh!  I see, I over-corrected.  \r\nSo I put back vec_sldw on line 297, and the compile completed without error.  Whew!\r\nGranted, this was a test of the one file that had failed rather than a full build, but it does look good."}, "51643": {"question": "torch.linalg.cond return dtype inconsistent with doc and np.linalg.cond", "answer": "`torch.linalg.cond` was changed to always return a real-valued tensor in https://github.com/pytorch/pytorch/pull/48284. We've discussed that with @kurtamohler and @mruberry yesterday and came to the conclusion it's the right thing to be divergent from NumPy in this case. This behavior should be fixed in NumPy and the issue was filed https://github.com/numpy/numpy/issues/18304.\r\n\r\nWhen normal `torch.linalg.cond` is called it should return the real number always (float32 for complex64 inputs, float64 for complex128 inputs). When the out variant is called float and complex tensors should be allowed according to the description of the correct \"out=\" behavior https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch.\r\n"}, "51688": {"question": "Is `torch.multiprocessing.spawn` compatible with `DataLoader`? ", "answer": "hey @PetrochukM!\r\n\r\nI think this is less a PyTorch issue and something the Lightning team should investigate. \r\n\r\nI'll hijack this issue for now to make sure my thoughts are documented!\r\n\r\nForgive me since I wasn't there when the original DDP docs for Lightning were made and this caveat was introduced, but from working with DDP in a spawn setting vs using `Popen` like `torch.distributed.launch`, the main issue with spawn is the need to serialize all state objects. This becomes an issue when using functions defined at runtime like so:\r\n\r\nhttps://gist.github.com/SeanNaren/f4a99235fc736a438637c31ee94f082f\r\n\r\nTo fix you just need to move `TestModel` outside of the main script. I also tested with multiple workers and it seems fine. The message in the Lightning Docs may be outdated or incorrect, and if thats the case we should update it ASAP!"}, "51700": {"question": "Using LAPACK on Pi4 (64Bit Raspberry Pi OS RAM 8GB)", "answer": "I've got a way to work out: \r\n1. https://github.com/ljk53/pytorch-rpi download torch-1.7.0a0-cp38-cp38-linux_aarch64.whl and pip3 install\r\n2. apt install libopenblas-dev\r\n3. success!"}, "51718": {"question": "Quantized LeakyReLu Bug: Input tensor size determines the output values", "answer": "Hi @Krosus , thanks again for the report.  After some internal discussion we decided that the discrepancy is acceptable, we are getting higher performance in the vectorized path, and the discrepancy will only be present if we are comparing tensors of different sizes.  Please let us know if this is blocking you, happy to help brainstorm workarounds."}, "51735": {"question": "aten::normal_ not handled as a special op in RemoveTensorMutation pass.", "answer": "The easiest fix is to probably just implement `normal`"}, "51788": {"question": "test_variant_consistency_jit tests fail on CPU for min & max when dtype is bfloat16 & dim argument is passed", "answer": "@imaginary-person Thanks for the update! Yes there are known problems with the BFloat16 operator in JIT, it's not fully supported. We have an issue #48978 (I see you've commented on it already) to figure out a way to clean up the test. You should be good to skip the BFloat16 dtype for the test_variant_consistency_jit tests without worry "}, "51840": {"question": "[collect_env] Unable to collect CUDA version anymore", "answer": "I encountered the same problem some time ago. Please see the description in https://github.com/k2-fsa/k2/pull/584\r\n\r\n- For CUDA 11.0\r\n```\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2020 NVIDIA Corporation\r\nBuilt on Thu_Jun_11_22:26:38_PDT_2020\r\nCuda compilation tools, release 11.0, V11.0.194\r\nBuild cuda_11.0_bu.TC445_37.28540450_0\r\n```\r\n\r\n- For CUDA 10.1\r\n```\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243\r\n```\r\n\r\nThere is an extra line `Build cuda_11.0_bu.TC445_37.28540450_0` for CUDA 11.0, which cannot be handled\r\nby `torch.utils.collect_env.get_running_cuda_version` since it uses a pattern `r\"V(.*)$\"` without `re.MULTILINE`.\r\nTherefore, `torch.utils.collect_env.get_running_cuda_version` returns `None` for CUDA 11.0"}, "51854": {"question": "M1 release and nightly binaries", "answer": "The compilation should be working just fine now. Note that some tests are failing though (for various reasons, not only M1): https://github.com/pytorch/pytorch/issues/56779\r\n\r\nYou can also get binaries from nightly by just following the instructions here: https://pytorch.org/get-started/locally/ (an x86 python will pull the x86 package and an arm64 python will pull the arm64 package)."}, "51887": {"question": "[FR] Initialize module on specified device", "answer": "This might be related to the meta-learning paradigm, i.e. to use one network to predict parameters of another network. There are libraries made for this (https://github.com/tristandeleu/pytorch-meta/blob/master/torchmeta/modules/linear.py). It seems to be another potential use case of this feature.\r\n\r\nWith this feature and #53144 to allow creating a module on null/meta device, it seems we'll be able to do the following to easily achieve meta learning, reusing nn.Conv2d without having to create custom MetaConv2d:\r\n\r\n```python\r\ndef __init__():\r\n   self.meta_conv = Conv2d(..., device='meta')\r\n   self.parameter_predictor = PredictorNet(num_param=sum(k.numel() for k in self.meta_conv.parameters()))\r\n\r\ndef forward(x):\r\n  params = self.parameter_predictor(x)\r\n  with temporary_set_params(self.meta_conv, params):\r\n      return self.meta_conv(x)\r\n```"}, "13951": {"question": "Please fix all the related links format from http to https", "answer": "closed via https://github.com/pytorch/pytorch.github.io/pull/128"}, "13964": {"question": "ImportError: libcurand.so.9.0: cannot open shared object file: No such file or directory", "answer": "@ezyang  thanks for your help. I think it is because that I used to install old caffe2.  The old caffe2 `libcaffe2.so`, `libcaffe2_detectron_ops_gpu.so`,` libcaffe2_gpu.so`, `libcaffe2_module_test_dynamic.so`, `libcaffe2_observers.so` is in `/usr/local/lib`, but now new installed caffe2 they all in `pytorch/build/lib`. I delete all in  `/usr/local/lib`. Now it seems OK. do you think my approach is correct? thanks!"}, "13966": {"question": "Exception in Thread: ValueError: signal number 32 out of range", "answer": "I have solved the problem by updating Python3.5 to Python3.7"}, "13969": {"question": "[JIT] torch.tensor doesn't trace devices correctly", "answer": "I have a tentative fix, that, unless someone explains me that this is a bad idea, I'll make into a PR.\r\nThe basic idea is to use a bit more tensor methods (in particular `Tensor.to(...)`) instead of doing this manually.\r\n\r\n```\r\niff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp\r\nindex f9d6ffc62..d8c939232 100644\r\n--- a/torch/csrc/utils/tensor_new.cpp\r\n+++ b/torch/csrc/utils/tensor_new.cpp\r\n@@ -244,7 +244,9 @@ Tensor internal_new_from_data(\r\n       (char*)tensor.data_ptr(), tensor.sizes(), tensor.strides(), 0,\r\n       scalarType, tensor.type().elementSizeInBytes(), data);\r\n   const auto& type_to_use = type_inference ? type.toScalarType(scalarType) : type;\r\n-  return new_with_type_conversion(type_to_use, tensor, device_index);\r\n+  auto device = device_opt.has_value() ? *device_opt : tensor.device();\r\n+  return tensor.to(device, type_to_use.scalarType(), /*blocking=*/false, /*copy=*/false);\r\n+  //return new_with_type_conversion(type_to_use, tensor, device_index);\r\n }\r\n \r\n Tensor new_from_data_copy(\r\n```\r\n"}, "14007": {"question": "gradient difference between single GPU and multi-GPU DataParallel", "answer": "the order of doing operations accumulates floating point errors in different ways. This is the reason for this difference, and it's expected."}, "14013": {"question": "UserWarning: ONNX export failed on ATen operator _argmax because torch.onnx.symbolic._argmax does not exist", "answer": "If I add the following function into torch/onnx/symbolic.py\r\n\r\n@parse_args('v', 'i', 'i')\r\ndef _argmax(g, self, dim, keepdim=None):\r\n    return g.op(\"ArgMax\", self, axis_i=dim, keepdim_i=keepdim)\r\n\r\n\r\nThen the export is able to proceed without error.\r\n"}, "14025": {"question": "cmake error about rocrand", "answer": "@NIEYALI I see. So to do ROCm build rocblas is a required dependency, so you need to install that. If you don't want to do ROCm build (but just happens you have partially installed ROCm before in your system), I have put up #14261 to allow disabling ROCm build with USE_ROCM=0 environment variable."}, "14031": {"question": "[Aten] at::randint doesn't return a variable", "answer": "The variable in the case of `at::normal` is because of `torch::zeros`. Functions in the `torch` namespace are by default `Variable`s."}, "14050": {"question": "libtorch latest-deps cuasing an error: PyTorch script module file is too old", "answer": "Nevermind -- re-exported model with new nightly & it worked fine."}, "14057": {"question": "Unable to pickle torch dtype objects in Python 3.5", "answer": "Actually torch dtype object is already serializable. Closing....\r\n```\r\nIn [6]: b = copy.deepcopy(a)\r\n\r\nIn [7]: id(b)\r\nOut[7]: 139818768678472\r\n\r\nIn [8]: id(a)\r\nOut[8]: 139818768678472\r\n\r\nIn [9]: import pickle\r\n\r\nIn [10]: with open('/tmp/a', 'wb') as f:\r\n    ...:     pickle.dump(torch.float32, f)\r\n    ...:\r\n```\r\n"}, "14067": {"question": "do pytorch c++ jit trace run model need more gpu memory than python env of the same model?", "answer": "It is possible, if you model has a bunch of in-place ops like relu_. We are adding mutability to JIT so this will be fixed eventually. cc @zou3519  "}, "14072": {"question": "the device of tensor can not be change", "answer": "> As I said in your previous issue `.to` returns new tensors, so you need to re-assign results of these two lines\r\n> \r\n> ```\r\n>         lp07.to(device)\r\n>         lp14.to(device)\r\n> ```\r\nthanks for answering my question. i figure out it by modify my code:\r\n`\r\n            lp07 = lp07.to(device)\r\n            lp14 = lp14.to(device)\r\n`\r\nbut i don't know why should i do this. In pytorch 0.3.0, the tensor just use:\r\n`tensor.cuda()`"}, "14085": {"question": "torch.full and torch.randint are inconsistent in arg order", "answer": "I think the intention was probably to keep PyTorch's syntax aligned with NumPy's syntax, because this is how NumPy does it too."}, "27083": {"question": "JIT pickler should support both little endian and big endian systems", "answer": "Endianness is also part of the pickle format itself and should be handled on loading / saving (the binary output from a big endian or little endian system should be the same), so if there are issues with endianness in the pickler that should be considered a bug in our implementation"}, "27129": {"question": "Improve the way RPC unit tests are skipped for windows", "answer": "I think `@unittest.skipIf(not dist.is_available())` would be much cleaner and we should probably add it to the test class rather than functions. Regarding the try-catch, why do we need that in the test file? The imports don't throw at the moment."}, "2715": {"question": "Build pytorch from source in osx", "answer": "I've installed Pytorch from source on Mac OS successfully. You can take a look at my tutorial: https://zhaoyu.li/post/install-pytorch-on-mac-with-nvidia-gpu/"}, "2718": {"question": "Python crash during backward", "answer": "Resolved in latest builds."}, "2723": {"question": "Upsampling is not implemented for 1D (temporal) inputs", "answer": "i think i know this.\r\n\r\n```\r\npip uninstall torch\r\npip uninstall torch\r\npython setup.py clean\r\npython setup.py build develop\r\n```"}, "2730": {"question": "With backward method of CUDA Variables, grad is None if gradient is supplied", "answer": "try this:\r\n```\r\n_x = Variable(torch.ones(10), requires_grad=True)\r\nx = _x.cuda()\r\ny = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()\r\ny.backward(torch.ones(10).cuda())\r\nprint(x.grad)\r\nprint(_x.grad)\r\n```\r\nThis is because x is an intermediate node.\r\nAlternative is:\r\n```\r\nx = Variable(torch.ones(10).cuda(), requires_grad=True)\r\ny = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()\r\ny.backward(torch.ones(10).cuda())\r\nprint(x.grad)\r\n```"}, "2731": {"question": "from torch._C import * ImportError: numpy.core.multiarray failed to import", "answer": "$ pip install numpy -I \r\nhas worked on me, I'll close the issue. But I don't know why does $ pip install numpy --upgrade not work.\r\n(py35) user@user-ASUS:~$ pip install numpy -I  \r\nCollecting numpy\r\n  Using cached numpy-1.13.1-cp35-cp35m-manylinux1_x86_64.whl\r\nInstalling collected packages: numpy\r\nSuccessfully installed numpy-1.13.1\r\n\r\n(py35) user@user-ASUS:~$ python\r\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n>>> x = torch.Tensor(2,3)\r\n>>> x\r\n\r\n 1.6534e+05  4.5673e-41  1.3032e-37\r\n 0.0000e+00  4.4842e-44  0.0000e+00\r\n[torch.FloatTensor of size 2x3]\r\n"}, "2740": {"question": "tensor.rand() and uniform_() returns numbers from [0,1] (right-hand inclusive)", "answer": "Here's a small test, reproducing device- and host-side generation, as implemented in TH/THC:\r\n```\r\n#include <curand_globals.h>\r\n#include <limits.h>\r\n#include <iostream>\r\n#include <iomanip>\r\n\r\nint main(){\r\n   uint x = UINT_MAX;\r\n   double d = x * CURAND_2POW32_INV_DOUBLE + (CURAND_2POW32_INV_DOUBLE/2.0f);\r\n   std::cout << std::setprecision(14) << \"double max \" << d << \"\\n\";\r\n   float f = x * CURAND_2POW32_INV + (CURAND_2POW32_INV/2.0f);\r\n   std::cout << \"float max \" << f << \"\\n\";\r\n//TH backend\r\n   double dc = UINT_MAX * (1.0/4294967296.0);   \r\n   std::cout << std::setprecision(14) << \"double max cpu \" << dc << \"\\n\"; \r\n   float fc = (float)(UINT_MAX * (1.0/4294967296.0));\r\n   std::cout << std::setprecision(14) << \"float max cpu \" << fc << \"\\n\"; \r\n  \r\n}\r\n```\r\nOnly cpu-side double generation is doing what is promised: [0, 1). cpu-side for floats of course rounds max to 1, thus generating [0 1]\r\ngpu-side for floats generates (0,1], as promised by curand manual, but that's not what torch manual specifies. \r\ngpu-side for doubles has a bug, thus generates (0,1), contrary to what's promised in the manual. \r\nReplacing x by (1-x) is expected to fail because of the different density of fp numbers near 0 and near 1 (thus, when 1-x is calculated for a small non-zero x, result will be rounded to 1, so (0,1] range will essentially be mapped to [0 1] range). \r\n"}, "2741": {"question": "tensor[...,None] adds unit axis to wrong dimension (inconsistent with numpy)", "answer": "fixed in master. soon to be in nightlies that we're building."}, "2748": {"question": "How to extract middle layer features", "answer": "pytorch is built around programs, not graphs. copying/modifying a forward function and returning the particular layer you want is the right way to do things. If you want to return a middle layer more conveniently, write your model `forward` to take a `name` string as well, and return that `name` layer."}, "2753": {"question": "topk cudaerror traceback...", "answer": "This is not related to topk itself, but because the operation is pretty intense / taking too long on your GPU.\r\nThe NVIDIA driver realizes that the operation is taking too long, and **because an active display is also attached to this GPU**, it kicks out the CUDA kernel.\r\n\r\nThis thread gives more context: https://devtalk.nvidia.com/default/topic/483643/cuda-the-launch-timed-out-and-was-terminated/\r\n\r\nThere's not a whole lot we can do from the pytorch end.\r\n\r\nWhat GPU do you have? Is it an option to not turn on the display on the GPU (maybe use this GPU purely over SSH). Is it an option to run your code on a second gpu (if you have two GPUs)"}, "2759": {"question": "Variable input size training is slow", "answer": "do you set `cudnn.benchmark=True` anywhere in your code? that is probably the culprit."}, "2762": {"question": "Can't import saved pytorch model", "answer": "Update:\r\n\r\nI finally figure it out the way to make it right.\r\nsolution for those who want to use SDN pox controller:\r\nadd the \"hparamDict\" definition before boot() function in the ~/any/pox/path/pox.py\r\n\r\nI am wondering if there is a way to avoid this? I mean avoiding copy the definition source code when load model. I save my model with this method \"torch.save(model.state_dict())\".\r\nThe attribute missing is actually a dict class I define for hyperparameters.\r\n\r\n=========================================================\r\nHi, guys. any solutions?\r\n\r\nI got the same problem. In fact, I tried to use pytorch with SDN controller POX. I can not load model because of the similar problem. \r\nSaying \"'module' object has no attribute 'hparamDict'\".\r\n\r\nI am very frustrated. I import and add the code of definition just right before torch.load(*). If this can not be sovled I have to reimplement all my experiments in tensorflow and give up pytorch, which is quite painful.\r\nThe command I use is \"./pox.py pytorch_model\"\r\n"}, "2769": {"question": "LSTM architecture has met a explosive growth in the training process", "answer": "I had this problem just earlier today and I was implementing an RNN as well.  it went away  when i properly detatched my hidden states using h = h.detach() ; c = c.detach(). The model was backpropagating the hidden state further than it should and it was causing massive memory usage. Not sure if thats your problem but I cant see detach or .'repackage_hidden' in your code so that could be it."}, "2772": {"question": "TypeError: Type torch.LongTensor doesn't implement stateless method mean", "answer": "We dont implement `mean` for LongTensor.\r\nYou can do: `y=torch.mean(x.float())`"}, "2774": {"question": "None Grad with Custom Loss", "answer": "This line seems the one which interferes with the gradient:\r\n```\r\n            score_final = score_final * (score[..., i] <= 0).float()\r\n```"}, "2787": {"question": "[request] Encode/decode variables", "answer": "I can see two solutions to this issue:\r\n\r\n1. Having `encode` and `decode` functions on class Variable. Every time the variable is computed/stored these functions will be called.\r\n2. Allowing `register_hook` to accept a keyword argument `check`, which will default to `True`. if `check == True`, [python_hook.cpp#L147] will be run.\r\n\r\n[python_hook.cpp#L147]:https://github.com/pytorch/pytorch/blob/1290e586fbc3d6266423f3417723d6620267054b/torch/csrc/autograd/python_hook.cpp#L147"}, "2797": {"question": "no module named reduction", "answer": "Just in case anyone else stumbles into this issue like I just did:\r\n\r\nYou will see this issue if your current working directory is `site-packages/torch`, in which case any `import multiprocessing` will import torch's own conveniently named `multiprocessing` package, and not the default one.\r\n\r\n(I saw this, because I was diagnosing LD_LIBRARY_PATH issues with my wheel build of PyTorch 1.0 preview + CUDA 10.)"}, "46983": {"question": "Constructing a ParameterDict raises a warning", "answer": "Thanks, I am ignoring it now with:\r\n\r\n```\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\", message=\"Setting attributes on ParameterDict is not supported.\")\r\n```\r\n\r\nI will wait for the new release!!"}, "46985": {"question": "flake8 errors are not shown by github actions", "answer": "Good catch, thanks! I'll go ahead and fix that, sorry for introducing the regression"}, "47007": {"question": "RuntimeError: \"mul_cuda\" not implemented for 'Bool'", "answer": "Maybe just changing, includeBool to `true` will work. https://github.com/pytorch/pytorch/blob/cd26d027b3357cd913412ee13060cdbb28fc178a/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu#L68\r\n"}, "47012": {"question": "[POLL][RFC] Can we retire Single-Process Multi-Device Mode from DistributedDataParallel?", "answer": "The ranking model distributed setting (where the many negatives benefits) has been de-prioritized in ParlAI due to advancements in generative models. It's acceptable for me to lose that functionality now."}, "47039": {"question": "Convolution operations are extremely slow on RTX 30 series GPU", "answer": "> The binaries use cudnn8.0.3, which doesn't ship with tuned heuristics for 3090 and cudnn8.0.5 will provide them.\r\n> Until then performance regressions on these devices are unfortunately expected.\r\n\r\nVery thanks for your help.\ud83d\ude03\r\nSo the solution may be: I should compile PyTorch from source once when cuDNN 8.0.5 is available\uff1f"}, "47043": {"question": "torch.arange numerics are different after 1.7 update on CPU", "answer": "I tried using intrinsics to see if it would avoid FMA, and it didn't work at first--but then I found a way to get it working. First, I changed from this\r\n\r\n```\r\n  template<typename step_t>\r\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\r\n    return Vec256<double>(\r\n        base + step * index_offset,\r\n        base + step * (index_offset + 1),\r\n        base + step * (index_offset + 2),\r\n        base + step * (index_offset + 3));\r\n  }\r\n```\r\n\r\nto this:\r\n```\r\n  template<typename step_t>\r\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\r\n    Vec256<double> base_v(base, base, base, base);\r\n    Vec256<double> step_v(step, step, step, step);\r\n    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); \r\n    return _mm256_add_pd(base_v, _mm256_mul_pd(step_v, index_v));\r\n  }\r\n```\r\n\r\nEvidently the `_mm256_add_pd` and `_mm256_mul_pd` calls get combined such that I'm effectively calling `_mm256_fmadd_pd`, so this didn't fix the problem. But then I wondered, what would happen if I just used the multiply intrinsic and then used regular add operations? Like this:\r\n\r\n```\r\n  template<typename step_t>\r\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\r\n    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); \r\n    Vec256<double> step_v(step, step, step, step);\r\n    Vec256<double> tmp = _mm256_mul_pd(step_v, index_v);\r\n    return Vec256<double>(\r\n      base + tmp.values[0],\r\n      base + tmp.values[1],\r\n      base + tmp.values[2],\r\n      base + tmp.values[3]);\r\n  }\r\n```\r\n\r\nThis worked! Since I'm using an intrinsic only for the multiply and not the add, the two operations don't get combined, and now we get the proper behavior for the example we've been looking at:\r\n\r\n```\r\n$ ATEN_CPU_CAPABILITY=avx2 python\r\n>>> import torch\r\n>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).floor()\r\ntensor([-5., -4., -3., -1.,  0.,  2.,  3.,  4.], dtype=torch.float64)\r\n>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).storage()\r\n -5.0\r\n -3.6\r\n -2.2\r\n -0.8000000000000007\r\n 0.5999999999999996\r\n 2.0      <---- NOTE: this is exactly 2 now, and not 1.99999... like before\r\n 3.3999999999999986\r\n 4.799999999999999\r\n[torch.DoubleStorage of size 8]\r\n```\r\n\r\nThis result is exactly the same for `ATEN_CPU_CAPABILITY=avx2`, `ATEN_CPU_CAPABILITY=avx`, and `ATEN_CPU_CAPABILITY=default`. And it agrees with the Pytorch 1.6 result as well.\r\n\r\nI'll admit that it's a bit of an odd solution, but it does work. Is it alright if we go ahead with this?"}, "47091": {"question": "`F.grid_sample` fails to dispatch correctly when args are of different subclasses", "answer": "Many thanks for these suggestions @hameerabbasi !\r\n\r\nI'm happy to go the wrapping approach, although iirc i'd been warned off it before as being hard to get full compatibility\r\n\r\nI've tried this\r\n\r\n```\r\nclass TensorBase:\r\n    def __init__(self, data, metadata=None, **kwargs):\r\n        self._fa_tensor = tensor(data)\r\n        for k,v in kwargs.items(): setattr(self, k, v)\r\n    def __torch_function__(self, func, types, args=(), kwargs=None):\r\n        if kwargs is None: kwargs = {}\r\n        args = [getattr(a,'_fa_tensor',a) for a in args]\r\n        ret = func(*args, **kwargs)\r\n        return TensorBase(ret, **self.__dict__)\r\n    def __getattr__(self, k): return getattr(self._fa_tensor, k)\r\n    def __getitem__(self, k): return self._fa_tensor[k]\r\n```\r\n\r\nBut I'm not sure how to support stuff like `+` with an int, without doing something a bit hacky"}, "47104": {"question": "torch.fft does not give the same result as torch.stft", "answer": "The difference is because `torch.fft`'s second argument isn't the transform axis, instead it's `signal_ndim` or the number of dimensions to transform. So, `torch.fft(x_torch, 3)` is actually equivalent to `scipy.fft.fftn(x_scipy, axes=(-1, -2, -3))`. For a more `numpy`-like interface use the new [`torch.fft.fft`](https://pytorch.org/docs/master/fft.html#torch.fft.fft) function, or if you're stuck with PyTorch 1.6 you can use `transpose(axis, -2)` to move the desired transform axis into the right place:\r\n\r\n```\r\nS_torch = torch.fft(x_torch.transpose(1, -2), signal_ndim=1).transpose(1, -2)\r\n```\r\nWhich results in:\r\n![image](https://user-images.githubusercontent.com/13238737/97711956-abe13d00-1ab5-11eb-9910-7fa99580663f.png)"}, "47112": {"question": "Legacy tensor ctor returns uninitialized tensor when input and output device differ", "answer": "> When the input is a 0-D or 1-size tensor, behavior depends on the dtype: int32: uninitialized tensor\r\n\r\nwhat is happening here is that the constructor is interpreting the Tensor as an IntArrayRef because a 1-element integer tensor passed PyLong_Check."}, "47123": {"question": "Independent Distribution Wrapper Disguises Negative StdDev in Underlying Normal Distribution", "answer": "Hi @decodyng, I think the best we can guarantee in the `torch.distributions` library is to correctly catch errors *when validation is enabled*. I believe this error would have been caught earlier if you had initially called\r\n```py\r\ntorch.distributions.Distribution.set_default_validate_args(True)\r\n```\r\nIn fact we recently [enabled validation by default](https://github.com/pyro-ppl/pyro/pull/2701) in Pyro (a downstream library). If this seems useful we could consider enabling validation by default also in PyTorch. What's your opinion?"}, "47127": {"question": "torch.trace type promotion behavior is different on CPU vs CUDA", "answer": "1.6.0:\r\n```\r\n>>> import torch\r\n>>> torch.__version__\r\n'1.6.0'\r\n>>> x = torch.ones(5, 5, dtype=torch.uint8)\r\n>>> x = x.cuda()\r\n>>> x.trace()\r\ntensor(5, device='cuda:0')\r\n>>> x.trace().dtype\r\ntorch.int64\r\n```\r\n\r\n1.5.1:\r\n```\r\n>>> import torch\r\n>>> torch.__version__\r\n'1.5.1'\r\n>>> x = torch.ones(5, 5, dtype=torch.uint8)\r\n>>> x = x.cuda()\r\n>>> x.trace()\r\ntensor(5, device='cuda:0', dtype=torch.uint8)\r\n```"}, "47138": {"question": "Jit Error with CUDA and FP16 -- identifier \"aten_add_flat__1\" is undefined", "answer": "The fix for this wasn't cherry picked into 1.7.1.  It should be in nightly tho."}, "47167": {"question": "Distributed weight updates", "answer": "@jianjiandandande \r\n\r\nAfter calling `loss.backward()`, can you try the following?\r\n\r\n```python\r\nfor name, param in m.named_parameters():\r\n  if not param.grad:\r\n    print(f\"detected unused parameter: {name}\")\r\n```\r\n\r\nThis should tell you what parameters are not used. \r\n\r\nBTW, any reason for not using [`DistributedDataParallel`](https://pytorch.org/docs/stable/notes/ddp.html) and set `find_unused_parameters=True`? \r\n"}, "29745": {"question": "How to add PyTorch to requirements.txt", "answer": "Looks like this works:\r\n\r\n```\r\n--find-links https://download.pytorch.org/whl/torch_stable.html\r\ntorch==1.3.1+cpu\r\n```\r\n\r\nThanks for the help!"}, "29801": {"question": "Use non-system cuda path", "answer": "@mahmoodn I assume you are building from source right? \r\nTry setting `CUDA_HOME` env?"}, "29807": {"question": "list of registered buffers does not move to cuda ", "answer": "@xonobo \r\nSo if you do `print(list(a.buffers()))` you'll see your buffer are on CUDA device. \r\n`a.params` as an attribute will be copied when `register_buffer` and `params` won't move along with `.to()`. But the buffer does. \r\nPlease let us know if this doesn't solve your question. Thanks!"}, "29809": {"question": "Memory leak with Conv1d on CPU", "answer": "Wow, indeed, `LRU_CACHE_CAPACITY=1` (from https://github.com/pytorch/pytorch/issues/27971) solves my issue!\r\n\r\nThanks a lot for pointing out, @ezyang!"}, "29820": {"question": "nn.Transformer.generate_square_subsequent_mask does not behave as expected", "answer": "It's an old issue related to type promotion https://github.com/pytorch/pytorch/pull/28231 and has been fixed by v.1.3.1. Please update your pytorch with the latest binary package or master branch. Feel free to re-open the issue if you still have questions."}, "29821": {"question": "Vanilla Resnet50 Not Computing on iOS via Libtorch (Pytorch Mobile)", "answer": "Hi Hussain, \r\n\r\nAs we've discussed in the PyTorch forum, this is a known issue, and we've fixed it - #29885  .If you want to try out the fix, feel free to pull the latest code from master and re-compile the static libraries from source code. Note that the fix will be available in the next release of Cocoapods."}, "29887": {"question": "Error when loading model with traced `to` call", "answer": "@sysuzyq This looks like a forward-compatibility issue, i.e. the version of PyTorch you use to save your model is newer than the libtorch version. Can you check if this works if these versions match?"}, "29889": {"question": "died with <Signals.SIGSEGV: 11>", "answer": "Try Python 3.6."}, "29893": {"question": "Memory leak when evaluating model on CPU with dynamic size tensor input.", "answer": "os.environ['LRU_CACHE_CAPACITY'] = '1' also work for me, THANKS!"}, "29939": {"question": "torch.nn.parallel.DistributedDataParallel is slow on backpropagation", "answer": "A difference between Apex and PyTorch DDP that comes to mind here is that Apex figures out the order in which gradients are produced at runtime, whereas PyTorch DDP still assumes that gradients are produced in reverse order how they are defined in the `nn.Module`. If the model you link to defines some first encoders / MLPs / etc as the last parameters in the module, you would see head-of-line blocking where all reductions will be sequenced after the final gradients, effectively removing all opportunity for overlapping reduction with gradient computation."}, "29967": {"question": "Illegal instruction : 4", "answer": "YESSSSSS !!\r\nThanks a lot.\r\n\r\nHere it is :\r\n\r\n(base) iMac27:miniconda3 xxxxxx$ **conda install --update-all pytorch-nightly torchvision -c pytorch**\r\nCollecting package metadata (current_repodata.json): done\r\nSolving environment: done\r\n\r\n## Package Plan ##\r\n\r\n  environment location: /Users/xxxxxx/miniconda3\r\n\r\n  added / updated specs:\r\n    - pytorch-nightly\r\n    - torchvision\r\n\r\n\r\nThe following packages will be downloaded:\r\n\r\n    package                    |            build\r\n    ---------------------------|-----------------\r\n    **pytorch-nightly-1.0.0.dev20190328**|          py3.7_0        45.3 MB  pytorch\r\n    tqdm-4.38.0                |             py_0          51 KB\r\n    ------------------------------------------------------------\r\n                                           Total:        45.4 MB\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n  pytorch-nightly    pytorch/osx-64::pytorch-nightly-1.0.0.dev20190328-py3.7_0\r\n\r\nThe following packages will be UPDATED:\r\n\r\n  tqdm                                          4.36.1-py_0 --> 4.38.0-py_0\r\n\r\n\r\nProceed ([y]/n)? y\r\n\r\n\r\nDownloading and Extracting Packages\r\npytorch-nightly-1.0. | 45.3 MB   | #################################################################################### | 100% \r\ntqdm-4.38.0          | 51 KB     | #################################################################################### | 100% \r\nPreparing transaction: done\r\nVerifying transaction: done\r\nExecuting transaction: done\r\n(base) iMac27:miniconda3 xxxxxx$ python3\r\nPython 3.7.5 (default, Oct 25 2019, 10:52:18) \r\n[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch\r\n>>> \r\n(base) iMac27:miniconda3 xxxxxx$ \r\n"}, "29980": {"question": "ONNX export failed: Couldn't export operator aten::upsample_bilinear2d", "answer": "Fixed after adding \r\n\r\ntorch.onnx.export(..,opset_version=11)\r\n"}, "29994": {"question": "Add SWA to PyTorch mainline", "answer": "In general, we focus on including methods that the community uses as a standard, or else the code maintenance problem balloons up for us. We do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch). In terms of rejected methods, we've rejected newly minted papers such as Swish (#3260, #3182), [Yellowfin](https://github.com/pytorch/pytorch/issues/1960) and many others that haven't become standardized in the community (like LSTM/Transformer/BatchNorm).\r\n\r\nIn this case, since at least a year has passed following the addition in contrib, the github repo and the paper have gathered momentum, I do agree with bringing SWA in pytorch mainline. Would you like to open a pull request with the relevant code, algorithm and tests?"}, "42995": {"question": "The Bug of onnx model format exported by torch.onnx.export", "answer": "For exporting a model to ONNX, you should not set the export type to OperatorExportTypes.ONNX_ATEN.\r\nIs there a specific reason you're using a different export type?\r\n\r\nTo try out the repro code, I added the code below to read the image:\r\n\r\n```\r\ninput_img = osp.join(\r\n            osp.dirname(__file__), '../tests/data/color.jpg')\r\noriginal_image = mmcv.imread(input_img)\r\noriginal_image = torch.from_numpy(original_image)\r\n```\r\n\r\nWith this code, I two errors in exporter regarding missing symbolics:\r\n1- Unsupported: ONNX export of roi_align with aligned=True\r\n2-  Exporting the operator new_empty to ONNX opset version 12 is not supported\r\n\r\nCan you please confirm if you see a similar behavior with pytorch 1.6 and detectron2 build from source?\r\n\r\nAlso, is there a reason you're exporting this model to external data format?\r\nRunning ONNX checker on models in external data format might be a bit different.\r\nI tried bypassing the issue with symbolics (1- set aligned=False, and inserted a symbolic for new_empty),\r\nand then tried export and checker with: onnx.checker.check_model(\"mask_rcnn.onnx\")\r\nWhich seems to work and pass checker successfully."}, "42996": {"question": "How to use and debug mixed-precision in 1.6.0 ?", "answer": "You should check if Tensor cores are used at all, there are some dimension requirements on batch size and number of hidden units. With apex it was done using for example: https://github.com/NVIDIA/apex/tree/master/apex/pyprof. \r\n\r\nBtw, while being advertised,  mixed precision is not quite there yet, e.g. RNN modules/cells don't work at all see:\r\n\r\n1. https://github.com/pytorch/pytorch/issues/42605\r\n2. https://github.com/pytorch/pytorch/issues/36428\r\n\r\nAlso you mentioned data parallelism and I'm not sure to which parallelism model are you referring to, I think it doesn't work with DataParallel, it only works with DDP (I'm not 100% sure for this, there were some problems, maybe it is fixed now).\r\n\r\nFinally, here you have some pointers for debugging at the end of slides: https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf."}, "42998": {"question": "after updating to pytorch 1.6 mypy does not recognise the tensor attributes ndim, nonzero and T ", "answer": "`ndim` was fixed yesterday (gh-42908), `T` was already present in master for longer, `nonzero` is still missing."}, "43135": {"question": "How to use torch.utils.checkpoint and DistributedDataParallel together", "answer": "Hey @devilztt if you are manually synchronizing gradients, then you don't need DDP anymore. \r\n\r\n```python\r\ninit_process_group(...)\r\nmodel = MyModel(...)\r\nmodel(inputs).sum().backward()\r\nworks = []\r\nfor p in model.parameters():\r\n    # to speed it up, you can also organize grads to larger buckets to make allreduce more efficient\r\n    works.append(dist.all_reduce(p.grad, async_op=True))\r\nfor work in works:\r\n    work.wait()\r\n...\r\n```"}, "43143": {"question": "complex32 seems to be doing very very weird things on CPU", "answer": "Currently there is almost no support for `torch.complex32`, so yeah we should disable it."}, "43146": {"question": "[jit] TorchScript does not work with Python coverage package", "answer": "Hi @janeyx99, is this resolved by your recent code coverage enhancement for TorchScript? "}, "2430": {"question": "Trying to get pytorch working for the first time", "answer": "Looks like you've installed pytorch with cuda 7.5, and it is trying to jit the code for you 1080 card. When you set device to 1, you are running on 610, which has compute capability 3.0 and most likely not supported. Try installing cuda 8 version of pytorch (conda install pytorch torchvision cuda80 -c soumith)"}, "2456": {"question": "Feature Request: Add Pixel Unshuffle", "answer": "The implementation from the topic starter is quite efficient already. I haven't seen it being used too often and usually, it's only used once at the beginning of the network. So benefits from adding it to the core are questionable. If you need class you could copy-paste this:\r\n```python\r\nclass SpaceToDepth(nn.Module):\r\n    def __init__(self, block_size=4):\r\n        super().__init__()\r\n        assert block_size in {2, 4}, \"Space2Depth only supports blocks size = 4 or 2\"\r\n        self.block_size = block_size\r\n\r\n    def forward(self, x):\r\n        N, C, H, W = x.size()\r\n        S = self.block_size\r\n        x = x.view(N, C, H // S, S, W // S, S)  # (N, C, H//bs, bs, W//bs, bs)\r\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\r\n        x = x.view(N, C * S * S, H // S, W // S)  # (N, C*bs^2, H//bs, W//bs)\r\n        return x\r\n\r\n    def extra_repr(self):\r\n        return f\"block_size={self.block_size}\"\r\n```"}, "2474": {"question": "Timeout option for parallel DataLoader", "answer": "We have switched to using `mp.Queue` (not `SimpleQueue`) now. I tested the above code on master. Interestingly, it makes the worker segfault. It works with `num_workers=0`.\r\n\r\nI adapted as following to make it run:\r\n```\r\nimport torch.utils.data\r\n\r\nclass Dataset(object):\r\n  def __len__(self):\r\n    return 100\r\n\r\n  def __getitem__(self, i):\r\n    return list(range(100000))\r\n\r\n\r\nclass Sampler(torch.utils.data.Sampler):\r\n  def __iter__(self):\r\n    return (range(100000) for batch_ind in range(100))\r\n\r\n  def __len__(self):\r\n    return 100\r\n\r\nd = torch.utils.data.DataLoader(dataset = Dataset(), sampler = Sampler(None), num_workers = 0)\r\nfor i, x in enumerate(d):\r\n    print(i)\r\n    \r\n```"}, "2481": {"question": "numpy like tensor.all and tensor.any", "answer": "As a note, `any` and `all` exist on ByteTensors, but do not appear in online documentation."}, "2485": {"question": "Bad error message when concatting different type Tensor(Variable)", "answer": "FYI, the same misleading message happens when trying to concat a Variable with a Tensor.\r\n\r\n> TypeError: cat received an invalid combination of arguments - got (list), but expected one of:\r\n>  * (sequence[torch.cuda.FloatTensor] seq)\r\n>       didn't match because some of the arguments have invalid types: (list)\r\n>  * (sequence[torch.cuda.FloatTensor] seq, int dim)\r\n> "}, "2486": {"question": "Feature request: nn.View", "answer": "You could define a module like this:\r\n```\r\nclass View(nn.Module):\r\n       def __init__(self):\r\n            super(View, self).__init__()\r\n   \r\n        def forward(self, x):\r\n            return x.view(-1) \r\n```\r\n\r\nThis flattens the input but similarly you could provide a size object.\r\n\r\nNow you can use this as part of the model."}, "2491": {"question": "function expand_as() works incorrectly on latest Pytorch 0.2.0_1", "answer": "Replace sum(1) with sum(1, keepdim=True)\r\n\r\nThis is caused by the change of sum in 0.2.0."}, "2496": {"question": "PyTorch 0.2.0_1 Freezes at nn.Conv2d() ", "answer": "Just to Update:\r\n\r\nIt works fine if we use 'spawn' start method.\r\n\r\nUpdated Snippet:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\nimport torch.multiprocessing as mp\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self, input_size):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(input_size, 32, 3, stride=2, padding=1)\r\n\r\n    def forward(self, input):\r\n        print('Before Conv1 call')\r\n        x = F.elu(self.conv1(input))\r\n        print('After Conv1 call')\r\n        return x\r\n\r\n\r\ndef train():\r\n    net = Net(1)\r\n    net(Variable(torch.randn(1, 80, 80).unsqueeze(0)))\r\n    print('Passed!')\r\n\r\n\r\nif __name__ == '__main__':\r\n    mp.set_start_method('spawn')\r\n\r\n    # directly calling the method works\r\n    train()\r\n\r\n    # Works fine as well with 'spawn'\r\n    p = mp.Process(target=train, args=())\r\n    p.start()\r\n    p.join()\r\n```\r\nOutput:\r\n\r\n```\r\nBefore Conv1 call\r\nAfter Conv1 call\r\nPassed!\r\nBefore Conv1 call\r\nAfter Conv1 call\r\nPassed!\r\n```\r\n"}, "2497": {"question": "Advanced Indexing doesn't work with uniform", "answer": "This is the expected behavior. Advanced Indexing always returns a copy of the indexed Tensor unless you perform assignment. From NumPy:\r\n\r\n> Advanced indexing always returns a copy of the data."}, "2507": {"question": "Segfault (free() on invalid pointer)", "answer": "Ok. So here's an ***_incredibly_*** hacky temorary workaround for Jupyter notebooks:\r\n\r\nFind the file called `ipykernel_launcher.py`  (mine is at `~/.local/lib/python3.6/site-packages/ipykernel_launcher.py` for example)\r\n\r\nNow, just after `import sys` insert a `import torch`. This gets rid of the notebook crashes, but comes at a \r\nslight \"cost\" that all the notebooks you start has torch already loaded into memory.\r\n\r\nJust remember to remove this hack after this issue is fixed :P ;)\r\n"}, "2514": {"question": "undefined symbol in master", "answer": "you can compile with `NO_DISTRIBUTED=1 python setup.py install` to avoid this. This might be a consequence of gloo not compiling for pre-3.x cards."}, "2515": {"question": "weight_norm assertion error when using bias=False and using cuda", "answer": "This issue is the same issue as raised in https://github.com/pytorch/pytorch/issues/2343 and is due to the way in which RNNs now flatten their weights when used on the GPU (otherwise it's a null op). This also breaks code for the [weight dropped LSTM](https://github.com/salesforce/awd-lstm-lm/blob/master/weight_drop.py).\r\n\r\nI realized that weight norm would have the same issue so searched hoping for an elegant solution, but alas :)\r\n\r\nI've fixed it by setting `rnn.flatten_parameters = lambda *args, **kwargs: None`, which results in a warning (below) but otherwise running code (except see caveat re: lambda).\r\n`UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().`\r\n(Just realized the error has a typo in \"greatly\" but oh well ;))\r\n\r\nYou could (and others have) converted the RNN to CUDA early but that would break other parts of my code. Killing the `flatten_parameters` function seems the most consistently backward compatible option. Only issue is you need to create an empty function (i.e. not use lambda) as otherwise you get pickling issues (`AttributeError: Can't pickle local object 'WeightDrop._setup.<locals>.<lambda>'`) on model save.\r\n\r\n@apaszke, have you got any insights as to a more elegant solution either temporarily or longer form? I know `flatten_parameters` is already a complicated and low level beast."}, "2517": {"question": "CUDA error (3): initialization error (multiprocessing)", "answer": "OK so I narrowed it down to `torch.manual_seed` of all things. Here is a minimal script reproducing the issue.\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.multiprocessing as mp\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\n\r\ndef task(pid, model):\r\n    x = Variable(torch.rand(64, 10))\r\n    y = model(x)\r\n    t = y.clone() * 0.99\r\n    loss = F.smooth_l1_loss(y, t)\r\n\r\n    # here it breaks\r\n    loss.backward()\r\n\r\n    print(\"Process %d finished\" % pid)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    # comment manual_seed and the CUDA initialization error is gone.\r\n    torch.manual_seed(23)\r\n\r\n    net = nn.Linear(10, 4)\r\n    net.share_memory()\r\n\r\n    processes = []\r\n    for pid in range(8):\r\n        p = mp.Process(target=task, args=(pid, net))\r\n        p.start()\r\n\r\n    for p in processes:\r\n        p.join()\r\n\r\n    print(\"Done.\")\r\n```\r\n\r\nedit: this can be solved by setting `mp.set_start_method('spawn')` before setting the rng seed which in turn calls cuda. Although I am not sure it is ideal."}, "14098": {"question": "Runtime error when trying to swap first two axes of a four dimensional Tensor with torch.transpose.", "answer": "This is a printing issue... We fixed it on master. Sorry about it."}, "14099": {"question": "Transposed Convolution output shape does not take into account dilation", "answer": "I just noticed this as well because I use the formula provided in by the docs in my [OutputShapeFor](https://github.com/Erotemic/netharn/blob/master/netharn/output_shape_for.py) class.\r\n\r\nConsidering the code\r\n\r\n```python.\r\n        module = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=1, dilation=2)\r\n        input_shape = (1, 1, 10, 10)\r\n        module(torch.rand(*input_shape)).shape\r\n```\r\n\r\nAccording  to the docs the output shape should be (1, 1, 12, 12), but in reality the output shape is (1, 1, 14, 14).\r\n\r\nModifying the formula as suggested by @Coolnesss does result in the correct answer, and the changes matches my intuition of how dilation increases the effective kernel size. "}, "14102": {"question": "Caffe2: ONNX building with lite proto failure", "answer": "Hi @bddppq\r\nyes, by applying #14140 and #14150 I am able to successfully build Caffe2 with protobuf-lite for Android.\r\nThank you very much for helping!"}, "14104": {"question": "[Caffe2] ONNX Caffe2Backend.prepare() initializes input as float64", "answer": "Due to this bug, I was getting error in a different form. \r\nI was not using CUDA, but was trying to export the model using -\r\n```\r\nfrom caffe2.python.predictor import mobile_exporter\r\n\r\nmobile_exporter.Export(prepared_backend.workspace, prepared_backend.predict_net, prepared_backend.predict_net.external_input) \r\n```\r\nThis was throwing `KeyError: dtype('float64')` from [here](https://github.com/pytorch/pytorch/blob/d55b25a633b7e2e6122becf6dbdf0528df6e8b13/caffe2/python/predictor/mobile_exporter.py#L41).\r\n\r\nThanks for @laggui for the fix given above. I was circumventing this in different way."}, "14119": {"question": "FP16 results in \"Floating point exception\"", "answer": "Unfortunately 9.2 nightlies are built with cudnn 7.1.4 that has a known fpe bug. The solution here would be to build nightlies with more recent cudnn versions. "}, "14147": {"question": "torch.argmin behaves differently on CPU and GPU", "answer": "@carefree0910 argmin returns the index of the minimum value in the dimension. It doesn't have a guarantee to return the index of the **first** minimum. The GPU result is also correct from what `argmin` is supposed to return"}, "14165": {"question": "Library not loaded: libmklml.dylib use c++ front", "answer": "@yf225 apparently this is fixed in latest MKL-DNN upgrade. It is pending land https://github.com/pytorch/pytorch/pull/22910"}, "14175": {"question": "Batched SVD using cuSolver", "answer": "Closing this since the feature is available on master. The current implementation uses sequential MAGMA calls in a for-loop."}, "14184": {"question": "cross_entropy - class weights is a bit unclear", "answer": "I believe `elementwise_mean` has been removed from `reduction` options now. https://pytorch.org/docs/stable/nn.html?highlight=cross_entropy#torch.nn.CrossEntropyLoss\r\nClosing, please feel free to reopen if you have other questions. Thanks!"}, "14187": {"question": "nn.parallel.DistributedDataParallel raise CUDA error", "answer": "The problem is not dense vs. sparse, but cpu vs. cuda. Use `.to('cuda')` to put the network on gpu."}, "14224": {"question": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'torch.LongTensor'", "answer": "Softmax doesn't work on a long tensor -- convert it to a float or double tensor first (via `tensor.float()`)"}, "14260": {"question": "RNN have CuDNN error: CUDNN_STATUS_SUCCESS with Tesla T4", "answer": "Updating to the CUDA 10 compiled version of pytorch along with CUDA 10 runtime resolved this issue for me."}, "5025": {"question": "Crash when autograd function returns list instead of tuple", "answer": "Now it's `TypeError: NewFunctionBackward.forward: expected Variable (got list) for return value 0` which is better, but if it said that a Tuple is expected instead of List or if it cast List to a Tuple automatically, it would be better"}, "5037": {"question": "[feature request] Removing hooks from module", "answer": "```python\r\nmodel = ...\r\nhandle = model.register_forward_hook(...)\r\nhandle.remove()\r\n# hook will no longer trigger\r\n```"}, "5038": {"question": "Unable to build from source", "answer": "Searching issues for \"PRId64\" shows that this error popped up and has been fixed in various places in the pytorch codebase, e.g. https://github.com/pytorch/pytorch/issues/3571\r\n\r\nThe problem seems to be that this format specifier is not defined by default in C++ and/or old gcc versions. A hacky way to fix this is replacing `\"%\" PRId64 \"` with `\"%lld\"` in the two files currently causing this error on master. A better way is to find the proper place for addding\r\n\r\n```\r\n#define __STDC_FORMAT_MACROS\r\n#include <inttypes.h>\r\n```\r\nas described in the issue linked above."}, "5040": {"question": "Give a better error when we run out of shared memory, instead of \"RuntimeError: DataLoader worker (pid 13) is killed by signal: Bus error.\"", "answer": "Do `nvidia-docker run -d --shm-size 50G -p 8888:8888 -p 6006:6006 -v ${PWD}:/notebook -v ${PWD}/data/:/notebook/data sachinruk/pytorch_gpu`. Main point is --shm-size ...\r\n\r\nIf anyone is wondering when I checked `docker stats`, it was showing that there is 59G available memory and I was using only 1G or so. So seems that you have to explicitly set `--shm-size`."}, "5046": {"question": "RuntimeError: cuda runtime error (38) ", "answer": "Problem solved.\r\nI made a very stupid mistake.\r\nThere is a line in the head which is\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'.\r\nI did not notice it first time I ran the program and got another error. I revised it to os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' without restarting the kernel. Then l got this error.\r\n\r\nRestart the program with os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'can solve the problem."}, "5059": {"question": "np.random generates the same random numbers for each data batch", "answer": "This is because numpy doesn't properly handle RNG states when `fork` subprocesses. It's numpy's issue with multiprocessing tracked at https://github.com/numpy/numpy/issues/9248). But we do provide some workarounds, e.g. the `worker_init_fn` in DataLoader (see http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). Or you can use other start methods like spawn."}, "5062": {"question": "Potential bug when sampling from categorical distribution", "answer": "On master the issue has been solved for the categorial distribution but not for the multinomial distribution as it seems: \r\n\r\n```\r\nimport torch.distributions as dis\r\nimport torch\r\nimport numpy as np\r\n\r\nG = 3\r\nD = 2\r\np_dG = torch.Tensor(G, D)\r\np_dG[:, 0] = torch.Tensor([0.1, 0.8, 0.1])\r\np_dG[:, 1] = torch.Tensor([0.1, 0.8, 0.1])\r\n\r\np_dg = p_dG[:, 0]\r\nz = p_dg.multinomial(250, replacement=True)\r\ntrue_z_np = z.numpy()\r\nv, c = np.unique(true_z_np, return_counts=True)\r\nprint(v)\r\nprint(c)\r\n```\r\nand also\r\n```\r\nz = torch.multinomial(p_dg, 250, replacement=True)\r\ntrue_z_np = z.numpy()\r\nv, c = np.unique(true_z_np, return_counts=True)\r\nprint(v)\r\nprint(c)\r\n```\r\n"}, "5069": {"question": "Recent git pull breaks working pytorch build", "answer": "after a git pull, do:\r\n\r\n```\r\ngit submodule update --init --recursive\r\n```\r\n\r\nor whatever it takes to update your submodules to their marked commits.\r\n\r\nif that doesn't work, just do a fresh git clone."}, "5099": {"question": "MKL Error when import torch after installing 0.3.0 on CentOS", "answer": "Hi @malbergo,\r\nMy solution was quite hacky. \r\nI add `anaconda2/envs/myenv/lib` into LD_LIBRARY_PATH in my .bashrc.\r\n\r\nI still not quite like it because it should be automatically triggered when I call `source activate myenv`"}, "5132": {"question": "Segmentation fault (core dumped) on LSTMCell on pytorch", "answer": "Hi @saitarslanboun \r\nSome sizes are missing in the snippet you provided, so I cannot exactly replicate it and get the segfault error. However, I think I can see a few potential mistakes in your code. I think you might be mistaking the input specifications for `LSTM` and `LSTMCell`. Note that by default, `LSTM` takes as input a tensor of shape `(seq_len, batch, input_size)`, whereas `LSTMCell` takes as input a tensor of shape `(batch, input_size)`.\r\n\r\nIn your code snippet, you seem to have `x` where `x.size(1)` specifies the number of timesteps. If I assume that `x` is then of the form `(batch_size, seq_len, input_size)`, the first thing we might want to do is to transpose the first 2 dimensions, by `x.transpose_(0,1)`, to make things `seq_len` first, or `batch_size` second. This is only because recurrent layers like to have the timesteps in the first dimension.\r\n\r\nNow, ideally, with a tensor `x` in such a shape, you do not need to use either a for loop, or a `LSTMCell`, and can simply pass it to a `LSTM` module to get the desired output. You can find the documentation for LSTM [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM). Your entire code will look like this:\r\n```\r\n# create lstmcell module\r\nlstm = nn.LSTM(embed_size * 2, hidden_size)\r\n\r\n# transpose from batch first to batch second for recurrent layers\r\nx = x.transpose(0, 1).contiguous()\r\noutput, (h_t, c_t) = lstm(x)\r\n```\r\n\r\nNow let us assume that for some reason, you must use a `LSTMCell` instead of a `LSTM`. In this case, if you go through the documentation for `LSTMCell` [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTMCell), you will see that the input needs to be passed 1 time step at a time. So, assuming your `x` is of the form `(batch_size, seq_len, input_size)`, your entire code will look like this:\r\n```\r\n# create lstmcell module\r\nlstm = nn.LSTMCell(embed_size * 2, hidden_size)\r\n\r\n# initialize h,c outside for loop\r\nh_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\r\nc_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\r\n\r\n# loop over time steps\r\nfor time_step in range(x.size(1)):\r\n    x_t = x[:, time_step, :]\r\n    (h_t, c_t) = lstm(x_t, (h_t, c_t))\r\n```\r\n\r\nFrom what I have understood from your snippet, there are a few potential mistakes:\r\n- Wrong sizes to initialize `hidden` and `cell`, which should be `(batch_size, input_size)` for `LSTMCell`\r\n- Two nested for loops, first over time steps, and second possibly over batch size, which is wrong\r\n\r\nLet me know if this helps you and/or if there are any details in your question that I have misunderstood."}, "10910": {"question": "ImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory", "answer": "Ok, great. For now i fixed it with: \r\n> conda install -c anaconda cudatoolkit==9.0"}, "10943": {"question": "`Normal` distribution: Gaussian policy with zero gradient of mean head", "answer": "Yep, you need to block gradient flow through sampling by calling `.sample()` instead of `.rsample()` (or detach it before calling `.logprob(.)`)"}, "10965": {"question": "Wrong torch.svd Calculation Result", "answer": "Your NumPy is using Accererate, but PyTorch uses MKL. It is natural that they behave slightly differently. Given that the singular value is so small. I would just classify this as precision problem."}, "10971": {"question": "Issue with pytorch update (version 0.4.1) with cuda9.1.85", "answer": "@darolt do `conda uninstall cuda90 cuda91 cuda92 pytorch -y`, and then re-run your command. That will fix it."}, "11009": {"question": "Incorrect PROTOBUF version", "answer": "Solved by doing\r\n```\r\nconda uninstall libprotobuf\r\n```"}, "11030": {"question": "implement dirichlet / beta GPU grad ", "answer": "Note that pyro's `Beta` **does** support GPU `rsample` and there are no issues there. So just need to get those changes ported in."}, "11074": {"question": "seg fault on import caffe2.python.onnx.backend ", "answer": "Yes this works. Thanks for the fix. \r\n```\r\n>>> import caffe2.python.onnx.backend as backend\r\n>>> import numpy as np\r\n```\r\n"}, "11089": {"question": "cufft errors after lots of plan generation", "answer": "As of CUDA 10 release last week, the bug has been fixed in cuFFT and I have updated the note here: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349. Also, I have added CUDA 10 guards to `CuFFTPlanCache.h` through this commit https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa. Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array should grow as intended without failing at the 1024th plan. I have tested this commit by building pytorch with CUDA 10 and running the test suite successfully.\r\n\r\nPlease let me know if there are any other queries regarding this issue. Otherwise, I think we are good to close this issue. "}, "49683": {"question": "Backward through sparse_coo_tensor", "answer": "the gradient of sum is by definition dense, so a sparse representation would use even more memory. use `x.values().sum()` if you want sparse gradients."}, "49707": {"question": "Inexplicable `test_variant_consistency_{eager/jit}_index_select` failure  for `torch.bfloat16` and `torch.float16`", "answer": "Apparently, index_select is not implemented for scalar half inputs in the cpu (non-scalar inputs are ok):\r\n```\r\nIn [26]: x=torch.tensor(2.2500, dtype=torch.float16)                                                                                                                                                           \r\n\r\nIn [27]: i=torch.tensor([0], dtype=torch.long)                                                                                                                                                                 \r\n\r\nIn [28]: torch.index_select(x,0,i)                                                                                                                                                                             \r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-28-573672115355> in <module>\r\n----> 1 torch.index_select(x,0,i)\r\n\r\nRuntimeError: \"index_select\" not implemented for 'Half'\r\n\r\nIn [29]: torch.index_select(x.cuda(),0,i.cuda())                                                                                                                                                               \r\nOut[29]: tensor(2.2500, device='cuda:0', dtype=torch.float16)\r\n\r\nIn [30]: x=torch.randn(3,4, dtype=torch.float16)                                                                                                                                                               \r\n\r\nIn [31]: i=torch.tensor([0,2])                                                                                                                                                                                 \r\n\r\nIn [32]: torch.index_select(x,0,i)                                                                                                                                                                             \r\nOut[32]: \r\ntensor([[ 0.6143,  0.4473,  0.3953, -0.4465],\r\n        [-0.6519, -1.0537,  0.3137,  1.6221]], dtype=torch.float16)\r\n\r\nIn [33]: torch.index_select(x.cuda(), 0, i.cuda())                                                                                                                                                             \r\nOut[33]: \r\ntensor([[ 0.6143,  0.4473,  0.3953, -0.4465],\r\n        [-0.6519, -1.0537,  0.3137,  1.6221]], device='cuda:0',\r\n       dtype=torch.float16)\r\n\r\n```\r\n"}, "49738": {"question": "amp does not work with LayerNorm gradient checkpointing", "answer": "Can you check if https://github.com/pytorch/pytorch/pull/49757 fixes the issue?  Diffs are 2 lines of python, should be quick to reinstall if you built from source.  If you installed from pip, alter your installed pytorch in place:\r\n```\r\n>>> import sys\r\n>>> import torch\r\n>>> sys.modules[\"torch\"]\r\n```\r\nand edit `torch/utils/checkpoint.py` with the PR's diffs at the path `sys.modules[\"torch\"]` points to."}, "49756": {"question": "Printing should not have (bad) autograd side effects", "answer": "It's unclear to me why `collect_next_edges` returns an empty set when grad mode is disabled. It's likely the unexpected semantics of this function are what caused the bug in the first place. Imo it should be the responsibility of this function to do exactly what it says, and should be a responsibility of the call-site to determine whether the function should be called at all, possibly conditional on whether grad mode is enabled. This make the semantics much more clear.\r\n\r\nIn fact, for the vast majority of `collect_next_edges` call-sites, including all generated ones, there _is_ a separate check to `GradMode::is_enabled`, usually through `compute_requires_grad`, that determines whether `collect_next_edges` needs to be called at all.\r\n\r\nIn the codebase, I found only 3 `collect_next_edges` call-sites that rely on an empty list of edges being returned when grad mode is disabled:\r\n* `unpack_input()` in `torch/csrc/autograd/python_function.cpp`\r\n* `apply()` in `torch/csrc/autograd/custom_function.h`\r\n* `wrap_outputs()` in `torch/csrc/autograd/functions/utils.cpp`\r\n\r\nOf course, the inevitable check for grad mode enabled still happens at some point.\r\n\r\nWith this in mind, I propose that we:\r\n1. Remove the grad enabled check / empty list return logic from `collect_next_edges`\r\n2. Update the 3 call-sites to manually construct an empty list instead of calling `collect_next_edges` when grad mode is disabled\r\n\r\nWhile I understand this proposed fix affects a broader cross-section of the codebase than the originally proposed fixes, I think this fix is better conceptually, and fixing the semantics of `collect_next_edges` makes future maintenance easier.\r\n\r\nIf this seems too dangerous, I have a branch ready to go with the proposed fix of temporarily enabling grad mode during `collect_next_edges` in `grad_fn()`.\r\n\r\nThoughts on this? @albanD @gchanan "}, "49794": {"question": "Different Dice accuracy  using DataParallel ", "answer": "It doesn't matter if it is DP or DDP. You need to use Sync BN if you need global norm."}, "49829": {"question": "rfftn / irfftn is not functioning properly", "answer": "This issue is from a 6 month old nightly release used in a non-standard environment. It wasn't reproducible at the time, and I can't reproduce it now. So, I think it's fair to close this."}, "49830": {"question": "torch.autograd.jacobian returns tensors with all zeros", "answer": "This is not a bug. It happens because in your script the function accepts `x` as an input but computes the sum of `tmp`. Running with `strict=True` would diagnose this error for you."}, "8474": {"question": "[Bug] Sometimes gradient doesn't back-propagate after view", "answer": "I don't think this is a bug. He is checking the gradient of a non-leaf variable.\r\nChange the name of the viewed variable to `x2`, and verify that the gradient of `x1` is good."}, "8480": {"question": "pytorch 0.4.0 always allocates memory on GPU:0 when the model and data are on other GPU.", "answer": "I believe this has been fixed in https://github.com/pytorch/pytorch/pull/7392 , and is available in pytorch master.\r\n\r\nCan you run your python script with\r\n```python\r\nCUDA_VISIBLE_DEVICES=1 python my_script.py\r\n```\r\nwhile you don't update pytorch?"}, "8483": {"question": "roi_crop (from Detectron.pytorch) building consistently fails ", "answer": "@phalexo You could try out to install `pytorch 0.4.0`, and insert `CFLAGS=\"-std=c99` before `sh make.sh`. "}, "8508": {"question": "Crash with SIGFPE due to unhandled cases in distributions.MultivariateNormal", "answer": "There needs to be two changes in the code:\r\n1. `bvec.size(-1)` to `bmat.size(-1)` in\r\nhttps://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L32\r\n\r\n2. `*shape` to `shape` in\r\nhttps://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L173\r\n\r\nThe sample function works fine. The distributions test suite passes as well.\r\n\r\n```python\r\n>>> import torch\r\n>>> m = torch.distributions.MultivariateNormal(torch.tensor(0.1), torch.tensor(0.5) * torch.eye(1))\r\n>>> m.sample()\r\ntensor([-0.2011])\r\n```"}}